# MultiGraphQA

**MultiGraphQA** is a framework designed to enhance question-answering capabilities by integrating two different approaches to process scientific documents from ArXiv: a multimodal approach for question answering (QA) and a graph-based framework for advanced reasoning.

## Usage

- **`multimodalQA.ipynb`**: This notebook explores three strategies for multimodal retrieval:
  1. **Visual Retrieval**: Utilizing visual embeddings to retrieve relevant images or graphs.
  2. **Late interaction Textual Retrieval**: Employing late interaction text embeddings strategyto fetch pertinent textual information.
  3. **Late chunking Textual Retrieval**: Employing late chunking text embeddings strategyto fetch pertinent textual information.

- **`graphRAG.ipynb`**: This notebook showcases the implementation of the GraphRAG approach, which integrates graph-based reasoning into the retrieval-augmented generation process. It demonstrates how to construct and utilize knowledge graphs to enhance answer generation.

Each strategy is demonstrated with examples, highlighting the benefits and challenges associated with multimodal retrieval in question-answering tasks.

## Repository Structure

The repository is organized into several directories and files, each serving a specific purpose:

- **`.byaldi/`**: Contains late interaction visual embeddings, which are used for processing and retrieving visual data relevant to the questions.

- **`.ragatouille/`**: Houses late interaction text embeddings, facilitating the retrieval of pertinent textual information.

- **`data/`**: Includes datasets necessary for training and evaluating the model. This folder contains:
  - **`backup_extraction_nodes/`**: Store the nodes and the entities extracted from each document that have been uploaded in the graph.
  - **`df_generated_*.parquet`**: Contains the generated answer from the model in the Multimodal approach, used for the analysis.
  - **`df_retrieved.parquet`**: Contains the retrieved elements from each model to answer the questions.
  - **`late_chunking.parquet`**: Contains late chunking text embeddings, which are used for retrieving data relevant to the questions.
  - **`processed_images.parquet`**: Contains the descriptions generated by Qwen for each image of the dataset.
  - - **`test-00000-of-000001.parquet`**: The original dataset from Vidore.

- **`helpers_notebooks/`**: Provides Jupyter notebooks with auxiliary scripts and codes that have been helpful to develop.

- **`images/`**: Stores visual assets, such as graphs and plots, utilized within the project.

- **`vector_db_graphRAG/`**: Contains the vector database for the GraphRAG component.

- **`graphRAG.ipynb`**: A Jupyter notebook demonstrating the implementation and usage of the GraphRAG framework.

- **`multimodalQA.ipynb`**: The primary notebook containing Multimodal approach and analysis on late chunking and late interaction.

- **`README.md`**: The main documentation file providing an overview of the project.

- **`.gitignore`**: Specifies files and directories to be ignored by Git.

## Getting Started

To begin using MultiGraphQA, open the notebooks on Google Colab or follow these steps:

1. **Clone the Repository**:
   ```bash
   git clone https://github.com/Robertogiordano/MultiGraphQA.git
   cd MultiGraphQA
   ```

2. **Create Neo4j db**:
   1. Follow the instructions [here](https://neo4j.com/product/auradb/?ref=neo4j-home-hero) to create a local (or remote) db instance.
   2. Create an `.env` file and save the following keys:
    OPENAI_API_KEY=...
    NEO4J_URI = ...
    NEO4J_USERNAME = ...
    NEO4J_PASSWORD = ...


3. **Explore the Notebooks**:
   Open the Jupyter notebooks (`graphRAG.ipynb` and `multimodalQA.ipynb`) to understand the implementation details and see examples of how to utilize the framework.


## Contributions

Contributions to MultiGraphQA are welcome. Feel free to fork the repository, make improvements, and submit pull requests. For major changes, please open an issue first to discuss the proposed modifications.

## Bibliography

[From Local to Global: A Graph RAG Approach to Query-Focused Summarization](https://arxiv.org/abs/2404.16130) 

[Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models](https://arxiv.org/abs/2403.00231)

[ViDoRe Dataset](https://huggingface.co/datasets/vidore/arxivqa_test_subsampled)

---

For any questions or further assistance, please contact the repository owner or open an issue on GitHub. 