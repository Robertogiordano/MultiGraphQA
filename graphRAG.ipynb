{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install neo4j\n",
    "# !pip install langchain\n",
    "# !pip install PyPDF2\n",
    "# !pip install tiktoken\n",
    "# !pip install openai  # Only if you want to use the OpenAI API\n",
    "# !pip install transformers  # For open (HF) models\n",
    "# !pip install sentence_transformers\n",
    "# !pip install -U langchain-community\n",
    "# !pip install graphdatascience\n",
    "# For advanced community detection with Leiden, you might need external libraries (e.g., igraph, networkx, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama pull llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertogiordano/Desktop/MultiGraphQA/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "# -----------------------\n",
    "# Neo4j Database imports\n",
    "# -----------------------\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# -----------------------\n",
    "# LLM / Embeddings imports\n",
    "# -----------------------\n",
    "# If using HuggingFace transformers:\n",
    "from transformers import pipeline\n",
    "\n",
    "# If using LangChain for retrieval + QA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# LangChain GraphRag Setup\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "# from langchain_core.documents import Document\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "from langchain.vectorstores import Milvus\n",
    "from langchain.schema import Document\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# If you want to use OpenAI, uncomment:\n",
    "import openai\n",
    "openai_model=\"gpt-4o-mini\"\n",
    "# -----------------------\n",
    "# Load environment variables\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# -----------------------\n",
    "# ArXiv API\n",
    "# -----------------------\n",
    "import arxiv\n",
    "\n",
    "# -----------------------\n",
    "# PDF Parsing library\n",
    "# -----------------------\n",
    "import PyPDF2  # or \"pypdf\" if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 1) CONFIGURATION: toggle open vs. OpenAI\n",
    "#############################################\n",
    "\n",
    "USE_OPENAI = True  # Set to True if you want to switch to OpenAI’s ChatGPT\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# For Neo4j:\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2) NEO4J CONNECTION AND GRAPH FUNCTIONS\n",
    "##################################################\n",
    "\n",
    "def add_chunk_node(tx, chunk_text: str, chunk_id: str, embedding: List[float]):\n",
    "    \"\"\"\n",
    "    Create or merge a chunk node in Neo4j to represent a piece of text.\n",
    "    Store its embedding as well.\n",
    "    \"\"\"\n",
    "    embedding_str = \",\".join([str(x) for x in embedding])\n",
    "\n",
    "    query = \"\"\"\n",
    "    MERGE (c:Chunk {chunk_id: $chunk_id})\n",
    "    ON CREATE SET c.text = $chunk_text,\n",
    "                  c.embedding = $embedding_str\n",
    "    \"\"\"\n",
    "    tx.run(query, chunk_id=chunk_id, chunk_text=chunk_text, embedding_str=embedding_str)\n",
    "\n",
    "\n",
    "def add_document_relationship(tx, doc_id: str, chunk_id: str):\n",
    "    \"\"\"\n",
    "    Link each chunk node to a parent Document node in Neo4j.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MERGE (d:Document {doc_id: $doc_id})\n",
    "    MERGE (c:Chunk {chunk_id: $chunk_id})\n",
    "    MERGE (d)-[:HAS_CHUNK]->(c)\n",
    "    \"\"\"\n",
    "    tx.run(query, doc_id=doc_id, chunk_id=chunk_id)\n",
    "\n",
    "\n",
    "def add_element_instance(tx, element_data: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    (Step 2.2) Insert extracted graph node/edge relationships from text chunk.\n",
    "    Example structure of element_data might be:\n",
    "    {\n",
    "        \"entity_name\": \"...\",\n",
    "        \"entity_type\": \"...\",\n",
    "        \"entity_description\": \"...\",\n",
    "        \"relationship\": {\n",
    "            \"source_entity\": \"...\",\n",
    "            \"target_entity\": \"...\",\n",
    "            \"description\": \"...\",\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "\n",
    "    In a real pipeline, you might store these as separate nodes/edges. \n",
    "    For demonstration, we store them in a single node with a property that can be parsed.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    CREATE (e:ElementInstance {data: $element_data})\n",
    "    \"\"\"\n",
    "    tx.run(query, element_data=str(element_data))\n",
    "\n",
    "\n",
    "def retrieve_relevant_chunks(tx, user_query: str, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    A simplistic retrieval function that returns chunk nodes based on naive text search.\n",
    "    In a real scenario, you'd have a vector similarity search using embeddings.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MATCH (c:Chunk)\n",
    "    WHERE c.text CONTAINS $user_query\n",
    "    RETURN c.chunk_id AS chunk_id, c.text AS text\n",
    "    LIMIT $limit\n",
    "    \"\"\"\n",
    "    result = tx.run(query, user_query=user_query, limit=limit)\n",
    "    return [record.data() for record in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.1 SOURCE DOCUMENTS → TEXT CHUNKS\n",
    "##################################################\n",
    "def find_metadata(doc_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    (Step 2.1) Retrieve metadata for a document from a database or API.\n",
    "    \"\"\"\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        id_list=[doc_id]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        result = next(client.results(search))\n",
    "        return \\\n",
    "            {\"title\": result.title, \n",
    "            \"summary\": result.summary, \n",
    "            \"url\": result.entry_id,\n",
    "            \"authors\": ', '.join([a.name for a in result.authors]),\n",
    "            \"categories\": ', '.join(result.categories)\n",
    "            }\n",
    "    except StopIteration:\n",
    "        return {}\n",
    "\n",
    "def parse_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract raw text from a PDF file using PyPDF2.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 600, chunk_overlap: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    (Step 2.1) Split text into chunks. \n",
    "    Following the guidance in 2.1, we use a smaller chunk size (e.g., ~600 tokens).\n",
    "    This can improve entity recall at the cost of more LLM calls.\n",
    "    \"\"\"\n",
    "    # Note: the chunk_size is in characters by default using this splitter;\n",
    "    # you may want to adapt to token-based splitting.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 4) EMBEDDING UTILITIES\n",
    "##################################################\n",
    "\n",
    "def get_hf_embedding_function(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\", device: str = \"mps\"):\n",
    "    \"\"\"\n",
    "    Returns a function that can generate embeddings using a HuggingFace model.\n",
    "    \"\"\"\n",
    "    hf_embed = HuggingFaceEmbeddings(model_name=model_name, device=device)\n",
    "    return hf_embed.embed_documents\n",
    "\n",
    "\n",
    "# If using OpenAI embeddings, uncomment and implement:\n",
    "# def get_openai_embedding_function(model_name: str = \"text-embedding-ada-002\"):\n",
    "#     def _embeddings(texts: List[str]) -> List[List[float]]:\n",
    "#         response = openai.Embedding.create(\n",
    "#             input=texts,\n",
    "#             model=model_name\n",
    "#         )\n",
    "#         embeddings = [item[\"embedding\"] for item in response[\"data\"]]\n",
    "#         return embeddings\n",
    "#     return _embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check and initialize Milvus database\n",
    "def init_milvus_db(collection_name: str, uri: str, embedding_function):\n",
    "    \"\"\"\n",
    "    Initialize the Milvus database if it does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(uri.replace(\"sqlite://\", \"\")):\n",
    "        print(f\"Creating database at {uri}\")\n",
    "    vectorstore = Milvus(\n",
    "        collection_name=collection_name,\n",
    "        embedding=embedding_function,\n",
    "        connection_args={\"uri\": uri},\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "# Function to add multiple documents to Milvus\n",
    "def add_documents_to_milvus(docs: list, embedding_function, collection_name: str = \"rag_milvus\", uri: str = \"sqlite://./vector_db_graphRAG/milvus_ingest.db\"):\n",
    "    \"\"\"\n",
    "    Add multiple documents to the Milvus vector store.\n",
    "\n",
    "    Args:\n",
    "        docs (list): List of tuples containing text and metadata.\n",
    "        collection_name (str): Name of the Milvus collection.\n",
    "        uri (str): URI for the Milvus database.\n",
    "    \n",
    "    # Example usage\n",
    "    docs = [\n",
    "        {\"text\": \"Chunk 1 of the document\", \"metadata\": {\"doc_id\": \"doc_1\", \"chunk\": 1}},\n",
    "        {\"text\": \"Chunk 2 of the document\", \"metadata\": {\"doc_id\": \"doc_1\", \"chunk\": 2}}\n",
    "    ]\n",
    "\n",
    "    add_documents_to_milvus(docs)\n",
    "    \"\"\"\n",
    "    # Initialize the database\n",
    "    vectorstore = init_milvus_db(collection_name, uri, embedding_function)\n",
    "\n",
    "    # Prepare documents\n",
    "    document_list = []\n",
    "    for doc in docs:\n",
    "        text = doc.get(\"text\", \"\")\n",
    "        metadata = doc.get(\"metadata\", {})\n",
    "        document_list.append(Document(page_content=text, metadata=metadata))\n",
    "\n",
    "    # Add documents to the vector store\n",
    "    vectorstore.add_documents(document_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_element_instances_from_chunk(\n",
    "    chunk_text: str,\n",
    "    gleaning_rounds: int = 1,\n",
    "    USE_OPENAI: bool = True,\n",
    "    local_llm: str = \"llama3.1\"\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    (Step 2.2) Use an LLM prompt to identify entity references, relationships, and covariates.\n",
    "    - Identifies entities (name, type, description) and relationships.\n",
    "    - Supports multiple rounds of \"gleanings\" to find any missed entities.\n",
    "    \"\"\"\n",
    "    extracted_elements = []\n",
    "    not_parsed = []\n",
    "\n",
    "    # Select the LLM to use\n",
    "    if USE_OPENAI:\n",
    "        llm = ChatOpenAI(model=openai_model, temperature=0.0)\n",
    "    else:\n",
    "        llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    # Base prompt for extracting entities and relationships\n",
    "    base_prompt = (\n",
    "        \"Extract entities and relationships from the following text. \"\n",
    "        \"For each entity, provide its name, type, and description. \"\n",
    "        \"For each relationship, provide the source entity, target entity, and description. \"\n",
    "        \"Text: \\n{chunk_text}\\n\"\n",
    "        \"Output format: List of dictionaries with keys 'entity_name', 'entity_type', 'entity_description', 'relationship'. \"\n",
    "        \"Follow this format in the example: [{{\\\"entity_name\\\": \\\"Alice\\\", \\\"entity_type\\\": \\\"Person\\\", \\\"entity_description\\\": \\\"A person of interest.\\\", \\\"relationship\\\": {{\\\"source_entity\\\": \\\"Alice\\\", \\\"target_entity\\\": \\\"Bob\\\", \\\"description\\\": \\\"Knows\\\"}}}}]\"\n",
    "        \"Return just the list, so that we can parse it.\"\n",
    "        \"No bullet list or asterisks needed.\"\n",
    "        \"It must be a unique list, do NOT separate entities and relationships in different lists.\"\n",
    "    )\n",
    "\n",
    "    # Loop through gleaning rounds\n",
    "    for round_num in range(gleaning_rounds):\n",
    "        prompt = base_prompt.format(chunk_text=chunk_text)\n",
    "\n",
    "        # Send prompt to the selected LLM\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        # Parse the LLM response\n",
    "        new_elements = response.content\n",
    "        new_elements = new_elements.replace('```json','').replace('```','')\n",
    "\n",
    "        # Assume the response is already in JSON format\n",
    "        try:\n",
    "            new_elements = eval(new_elements)  # Convert string to list of dicts\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing LLM output: {e}\")\n",
    "            not_parsed.extend(new_elements)\n",
    "            new_elements = []\n",
    "\n",
    "        # Add new elements to the result\n",
    "        extracted_elements.extend(new_elements)\n",
    "\n",
    "        # Check if gleaning is needed (e.g., ask LLM if entities were missed)\n",
    "        if round_num < gleaning_rounds - 1:\n",
    "            print(\"Asking for validation...\")\n",
    "            validation_prompt = (\n",
    "                \"Were any entities or relationships missed in the previous extraction? \"\n",
    "                \"Answer 'Yes' or 'No'.\"\n",
    "            )\n",
    "            validation_response = llm.invoke(\n",
    "                [ HumanMessage(content=validation_prompt)], \n",
    "                chat_history=\n",
    "                [HumanMessage(content=prompt), SystemMessage(content=new_elements)]\n",
    "            )\n",
    "\n",
    "            # If LLM says 'No', break early\n",
    "            if 'No' in validation_response.content:\n",
    "                break\n",
    "\n",
    "    # Return all extracted elements\n",
    "    return extracted_elements, not_parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_prompt = \"\"\"\n",
    "You are an expert in text summarization and knowledge graph construction. I will provide you with:\n",
    "\n",
    "1. A list of initial nodes, each containing:\n",
    "   - A unique identifier\n",
    "   - A textual description extracted from the source\n",
    "   - Additional properties (optional)\n",
    "\n",
    "2. A list of relationships between these nodes (e.g., an entity \"Einstein\" related to another entity \"Relativity\").\n",
    "\n",
    "Your task is to:\n",
    "- Identify when multiple nodes actually refer to the same entity or concept.\n",
    "- Generate *summarized nodes* by consolidating their textual descriptions and removing duplicates or near-duplicates.\n",
    "- Maintain references to each node's original ID within your summarized node.\n",
    "- Create new relationships among these summarized nodes that reflect the original relationships, but merged and simplified where appropriate.\n",
    "\n",
    "**Important requirements and format details:**\n",
    "1. Each summarized node should have:\n",
    "   - A `summary` field with the merged description.\n",
    "   - A list of `original_ids` that were merged into this new summary node.\n",
    "   - Any relevant `type` or `label` (e.g., Person, Theory, Location) if it can be inferred from the text.\n",
    "   - (Optional) A short list of `keywords` extracted from the descriptions.\n",
    "\n",
    "2. Each relationship should:\n",
    "   - Include `source` and `target` references to the new summarized nodes.\n",
    "   - Provide a `relation_type` (e.g., \"INVENTED\", \"WORKS_ON\", \"LOCATED_IN\", etc.).\n",
    "   - Have a `weight` or `relevance_score` if it can be inferred (e.g., frequency or importance).\n",
    "   - (Optional) Include an `original_relationships` list indicating which original relationships were merged.\n",
    "\n",
    "3. Return the final data in **JSON** format, containing two top-level keys: `summarized_nodes` and `summarized_relationships`.\n",
    "\n",
    "4. Be concise but ensure the summaries and relationships accurately capture the original meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Here is the initial Nodes and Relationships**:\n",
    "\n",
    "{initial_data}\n",
    "\n",
    "---\n",
    "\n",
    "### **Instructions to the LLM**:\n",
    "1. **Identify duplicates or near-duplicates** (e.g., \"Albert Einstein\" and \"Einstein\" might refer to the same entity).\n",
    "2. **Create a new summarized node** that merges the descriptions of \"N1\" and \"N2\" if they represent the same entity (in this case, Albert Einstein).\n",
    "3. **Consolidate relationships** so that if multiple original relationships lead to the same concept, you unify them into a single relationship with an updated weight (e.g., sum or average of the original).\n",
    "4. Provide your final answer in the following **JSON** structure:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"summarized_nodes\": [\n",
    "    {{\n",
    "      \"title\": \"NewTitle1\",\n",
    "      \"summary\": \"Your merged summary text here...\",\n",
    "      \"original_ids\": [\"ExampleID1\", \"ExampleID2\", ...],\n",
    "      \"type\": \"Person\",\n",
    "      \"keywords\": [\"Einstein\", \"relativity\", \"physics\"]\n",
    "    }},\n",
    "    {{\n",
    "      \"title\": \"NewTitle2\",\n",
    "      \"summary\": \"Your summary text here...\",\n",
    "      \"original_ids\": [\"ExampleID3\", \"ExampleID4\"],\n",
    "      \"type\": \"Theory\",\n",
    "      \"keywords\": [\"relativity\", \"physics\"]\n",
    "    }}\n",
    "  ],\n",
    "  \"summarized_relationships\": [\n",
    "    {{\n",
    "      \"source\": \"NewTitle1\",\n",
    "      \"target\": \"NewTitle2\",\n",
    "      \"relation_type\": \"DEVELOPED_OR_ASSOCIATED_WITH\",\n",
    "      \"weight\": 5,\n",
    "      \"original_relationships\": [\"N1->N3(DEVELOPED)\", \"N2->N3(ASSOCIATED_WITH)\"]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "\n",
    "Please **only** output valid JSON in the format described above, without additional commentary so that we can parse it correctly. \n",
    "Make sure to capture the essence of each original node and relationship in your summarized version.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.3 ELEMENT INSTANCES → ELEMENT SUMMARIES\n",
    "##################################################\n",
    "\n",
    "def summarize_element_instances(\n",
    "    element_instances: List[Dict[str, Any]], \n",
    "    USE_OPENAI: bool = True,\n",
    "    local_llm: str = \"llama3.1\") -> str:\n",
    "    \"\"\"\n",
    "    (Step 2.3) Summarize extracted nodes/relationships into a single descriptive block of text\n",
    "    for each chunk. This is an additional LLM-based summarization step, forming \"element summaries.\"\n",
    "    \"\"\"\n",
    "    # Select the LLM to use\n",
    "    if USE_OPENAI:\n",
    "        llm = ChatOpenAI(model=openai_model, temperature=0.0)\n",
    "    else:\n",
    "        llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    prompt = summarization_prompt.format(initial_data=element_instances)\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    response = response.content\n",
    "\n",
    "    try:\n",
    "        response = response.replace('```json','').replace('```','')\n",
    "        response = eval(response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing LLM output: {e}\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_element_summary_in_graph(tx, data: Dict[str, Any], doc_id: str):\n",
    "    \"\"\"\n",
    "    Load the summarized graph data into Neo4j.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creazione dei nodi\n",
    "    for node in data[\"summarized_nodes\"]:\n",
    "        query_create_node = \"\"\"\n",
    "        CREATE (n:SummarizedNode {\n",
    "            title: $title,\n",
    "            summary: $summary,\n",
    "            original_ids: $original_ids,\n",
    "            type: $type,\n",
    "            keywords: $keywords,\n",
    "            doc_id : $doc_id\n",
    "        })\n",
    "        \"\"\"\n",
    "        tx.run(\n",
    "            query_create_node,\n",
    "            title=node.get(\"title\"),\n",
    "            summary=node.get(\"summary\"),\n",
    "            original_ids=node.get(\"original_ids\"),\n",
    "            type=node.get(\"type\"),\n",
    "            keywords=node.get(\"keywords\"),\n",
    "            doc_id=doc_id\n",
    "        )\n",
    "\n",
    "    # Creazione delle relazioni\n",
    "    for rel in data[\"summarized_relationships\"]:\n",
    "        query_create_rel = f\"\"\"\n",
    "        MATCH (source:SummarizedNode {{title: $source_id}})\n",
    "        MATCH (target:SummarizedNode {{title: $target_id}})\n",
    "        CREATE (source)-[:RELATIONSHIP_TYPE {{\n",
    "            type : $relation_type,\n",
    "            weight: $weight,\n",
    "            original_relationships: $original_rels\n",
    "        }}]->(target)\n",
    "        \"\"\"\n",
    "        tx.run(\n",
    "            query_create_rel,\n",
    "            source_id=rel[\"source\"],\n",
    "            target_id=rel[\"target\"],\n",
    "            relation_type=rel[\"relation_type\"],\n",
    "            weight=rel.get(\"weight\", 1),  # default=1 if not provided\n",
    "            original_rels=rel.get(\"original_relationships\", [])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.4 ELEMENT SUMMARIES → GRAPH COMMUNITIES\n",
    "##################################################\n",
    "\n",
    "class CommunityDetection:\n",
    "    def __init__(self, driver: Any = None, uri: str = '', user: str = '', password: str = ''):\n",
    "        \"\"\"\n",
    "        Initialize the CommunityDetection class with a Neo4j driver or connection details.\n",
    "\n",
    "        Args:\n",
    "            driver (Any): An existing Neo4j driver instance. If provided, `uri`, `user`, and `password` are ignored.\n",
    "            uri (str): The URI for the Neo4j database.\n",
    "            user (str): The username for the Neo4j database.\n",
    "            password (str): The password for the Neo4j database.\n",
    "        \"\"\"\n",
    "        # If a driver is provided, use it; otherwise, create a new driver instance\n",
    "        self.driver = driver or GraphDatabase.driver(uri, auth=(user, password))\n",
    "        self.graph_name = 'summarizedGraph'\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def project_graph(self, relationship_weight_property: str = \"weight\") -> None:\n",
    "        \"\"\"\n",
    "        Projects the graph into memory for analysis.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(\n",
    "                f\"\"\"\n",
    "                CALL gds.graph.project(\n",
    "                    '{self.graph_name}',\n",
    "                    'SummarizedNode',\n",
    "                    {{ RELATIONSHIP_TYPE: {{ orientation: 'UNDIRECTED', properties: ['{relationship_weight_property}'] }} }}\n",
    "                )\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "\n",
    "    def set_communities(self, relationship_weight_property: str = \"weight\") -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Sets the community IDs directly into the graph database.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\n",
    "                f\"\"\"\n",
    "                CALL gds.leiden.stream('{self.graph_name}', {{ relationshipWeightProperty: '{relationship_weight_property}' }})\n",
    "                YIELD nodeId, communityId\n",
    "                SET gds.util.asNode(nodeId).communityId = communityId\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    def retrieve_communities(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve the community assignments from the graph.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\n",
    "                f\"\"\"\n",
    "                MATCH (n:SummarizedNode)\n",
    "                RETURN\n",
    "                    n.communityId AS communityId,\n",
    "                    n.title AS nodeTitle,      \n",
    "                    n.summary AS nodeSummary,\n",
    "                    n.keywords AS nodeKeywords \n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "            communities = {}\n",
    "            for record in result:\n",
    "                community_id = record[\"communityId\"]\n",
    "                node_title = record[\"nodeTitle\"]\n",
    "                node_summary = record[\"nodeSummary\"]\n",
    "                node_keywords = record[\"nodeKeywords\"]\n",
    "                \n",
    "                communities[community_id] = communities.get(community_id, []) + [{\n",
    "                    \"title\": node_title,\n",
    "                    \"summary\": node_summary,\n",
    "                    \"keywords\": node_keywords\n",
    "                }]\n",
    "\n",
    "\n",
    "            return communities\n",
    "\n",
    "    def drop_graph(self) -> None:\n",
    "        \"\"\"\n",
    "        Drops the graph from memory.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(f\"CALL gds.graph.drop('{self.graph_name}') YIELD graphName\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.5 GRAPH COMMUNITIES → COMMUNITY SUMMARIES\n",
    "##################################################\n",
    "\n",
    "def summarize_communities():\n",
    "    \"\"\"\n",
    "    (Step 2.5) Summarize each community (or sub-community in a hierarchical approach).\n",
    "    - Gather all element summaries (nodes, edges, covariates) in that community.\n",
    "    - Summarize them, potentially chunking if they don't fit in an LLM context window.\n",
    "    \"\"\"\n",
    "    # Placeholder logic\n",
    "    print(\"[Community Summaries] Placeholder: gather summaries and do hierarchical summarization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Function to generate a summary for each community\n",
    "def generate_community_summary(community_nodes, api_key, model=\"gpt-4\"):\n",
    "    \"\"\"\n",
    "    Generates summaries for communities using OpenAI's GPT model.\n",
    "\n",
    "    Args:\n",
    "        community_nodes (dict): A dictionary where keys are community IDs and values are lists of node titles.\n",
    "        api_key (str): OpenAI API key.\n",
    "        model (str): Model name to use (default is 'gpt-4').\n",
    "\n",
    "    Returns:\n",
    "        dict: Summaries for each community.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the OpenAI API\n",
    "    openai.api_key = api_key\n",
    "\n",
    "    summaries = {}\n",
    "\n",
    "    for community_id, nodes in community_nodes.items():\n",
    "        # Create a prompt with node information\n",
    "        prompt = f\"Summarize the main topics and themes for the following nodes in community {community_id}:\\n\"\n",
    "        prompt += \"\\n\".join(nodes)\n",
    "\n",
    "        try:\n",
    "            # Call the OpenAI API\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert data analyst.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=200\n",
    "            )\n",
    "\n",
    "            # Extract and store the summary\n",
    "            summaries[community_id] = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            summaries[community_id] = f\"Error generating summary: {str(e)}\"\n",
    "\n",
    "    return summaries\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your OpenAI API key\n",
    "    api_key = \"your-openai-api-key\"\n",
    "\n",
    "    # Example community nodes\n",
    "    community_nodes = {\n",
    "        1: [\"Node A\", \"Node B\", \"Node C\"],\n",
    "        2: [\"Node D\", \"Node E\"],\n",
    "        3: [\"Node F\", \"Node G\", \"Node H\"]\n",
    "    }\n",
    "\n",
    "    summaries = generate_community_summary(community_nodes, api_key)\n",
    "\n",
    "    for community_id, summary in summaries.items():\n",
    "        print(f\"Community {community_id}: {summary}\")\n",
    "\n",
    "\n",
    "// Step 1: Create the Graph in GDS\n",
    "CALL gds.graph.project(\n",
    "    'summarizedGraph',\n",
    "    'SummarizedNode',\n",
    "    {\n",
    "        RELATIONSHIP_TYPE: {\n",
    "            orientation: 'UNDIRECTED',\n",
    "            properties: 'weight'\n",
    "        }\n",
    "    }\n",
    ");\n",
    "\n",
    "// Step 2: Run the Leiden algorithm\n",
    "CALL gds.leiden.stream('summarizedGraph', { relationshipWeightProperty: 'weight' })\n",
    "YIELD nodeId, communityId\n",
    "RETURN gds.util.asNode(nodeId).title AS nodeTitle, communityId;\n",
    "\n",
    "// Step 3: Drop the graph from memory\n",
    "CALL gds.graph.drop('summarizedGraph');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.6 COMMUNITY SUMMARIES → COMMUNITY ANSWERS → GLOBAL ANSWER\n",
    "##################################################\n",
    "\n",
    "def answer_query_from_communities(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    (Step 2.6) Use the hierarchical community summaries to answer user queries globally.\n",
    "    - In an actual implementation, you'd fetch the relevant community summaries, chunk them,\n",
    "      run partial QA on each chunk, rank answers by helpfulness, and then produce a final answer.\n",
    "    - Below is a simplified approach that just returns a single, direct LLM-based QA.\n",
    "    \"\"\"\n",
    "    # Placeholder logic\n",
    "    # If you have multiple community summaries, you'd do partial QA in parallel, rank by\n",
    "    # self-reported \"helpfulness\" (0-100), then combine or reduce them into a global answer.\n",
    "\n",
    "    return f\"Global answer to '{user_query}' (placeholder).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 5) INGEST PDF -> STORE IN GRAPH (Putting Steps 2.1 and 2.2+ in context)\n",
    "##################################################\n",
    "\n",
    "# Define parallelized function for processing a single chunk\n",
    "def process_chunk_wrapper(i, chunk_text_str):\n",
    "    # Step 5: Extract element instances\n",
    "    #print(f\"Extracting element instances from chunk {i}...\")\n",
    "    elements_instance, not_parsed_instance = extract_element_instances_from_chunk(chunk_text_str, USE_OPENAI=USE_OPENAI)\n",
    "    #print(f\"Extracted {len(elements_instance)} elements from chunk {i}\\n\\n\")\n",
    "    return elements_instance, not_parsed_instance\n",
    "\n",
    "\n",
    "def ingest_pdf_into_graph(pdf_path: str, doc_id: str, embed_opt: bool = False, BATCH_SIZE: int = 25):\n",
    "    \"\"\"\n",
    "    1) Parse PDF into raw text.\n",
    "    2) Chunk it (Step 2.1).\n",
    "    3) Generate embeddings for each chunk.\n",
    "    4) Store chunk nodes in Neo4j.\n",
    "    5) For each chunk, call LLM to extract element instances (Step 2.2).\n",
    "    6) Summarize them into a single descriptive block (Step 2.3).\n",
    "    7) Optionally store the block in Neo4j for further community detection.\n",
    "    \"\"\"\n",
    "    # Step 1: Parse PDF\n",
    "    print(f\"Parsing PDF at {pdf_path}...\")\n",
    "    raw_text = parse_pdf(pdf_path)\n",
    "    print(f\"Extracted {len(raw_text)} characters from {pdf_path} \\n\\n\")\n",
    "\n",
    "    # Step 1.1: Retrieve metadata\n",
    "    print(f\"Retrieving metadata for {doc_id}...\")\n",
    "    metadata = find_metadata(doc_id)\n",
    "    print(f\"Metadata: {metadata}\\n\\n\")\n",
    "\n",
    "    # Step 2: Chunk the text (default chunk_size=600 for improved recall)\n",
    "    chunks = chunk_text(raw_text)\n",
    "    print(f\"Chunked {len(chunks)} segments from {pdf_path} \\n\\n\")\n",
    "\n",
    "    # Step 3: Embeddings\n",
    "    if embed_opt:\n",
    "        print(\"Generating embeddings for each chunk...\")\n",
    "        # if USE_OPENAI:\n",
    "        #     # Implement an OpenAI embedding function if desired\n",
    "        #     raise NotImplementedError(\"OpenAI embeddings not implemented here.\")\n",
    "        # else:\n",
    "        embed_fn = get_hf_embedding_function()\n",
    "\n",
    "        add_documents_to_milvus([\n",
    "            {\n",
    "                \"text\": chunk, \n",
    "                \"metadata\": {\n",
    "                    \"doc_id\": doc_id, \n",
    "                    \"chunk\": i, \n",
    "                    **metadata\n",
    "                    }\n",
    "            } for i, chunk in enumerate(chunks)], embed_fn)\n",
    "        print(f\"Stored {len(chunks)} chunks in Milvus under Document {doc_id} \\n\\n\")\n",
    "\n",
    "    # Step 4: Store chunk nodes in Neo4j\n",
    "    # Process chunks in parallel with batch size of 25\n",
    "    for lim in tqdm.tqdm(range(0, len(chunks), BATCH_SIZE), desc=\"Processing chunks in parallel\"):\n",
    "        extracted_elements = []\n",
    "        not_parsed_elements = []\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            # Submit tasks\n",
    "            futures = [\n",
    "                executor.submit(process_chunk_wrapper, i, chunk_text_str) \n",
    "                for i, chunk_text_str in enumerate(chunks[lim:lim+BATCH_SIZE])\n",
    "            ]\n",
    "            \n",
    "            # Process results as they complete\n",
    "            for future in tqdm.tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing chunks\"):\n",
    "                elements_instance, not_parsed_instance = future.result()\n",
    "                extracted_elements.extend(elements_instance)\n",
    "                not_parsed_elements.extend(not_parsed_instance)\n",
    "\n",
    "        print(\"Parallel processing completed.\")\n",
    "            \n",
    "        # Step 6: Summarize them (element-level)\n",
    "        print(\"Summarizing element instances...\")\n",
    "        element_summary = summarize_element_instances(extracted_elements, USE_OPENAI=USE_OPENAI)\n",
    "\n",
    "        print(f\"Summarized {len(element_summary['summarized_nodes'])} nodes and {len(element_summary['summarized_relationships'])} relationships\\n\\n\")\n",
    "\n",
    "        print(\"Storing backup files...\")\n",
    "        # Backup element_summary, extracted_elements and not_parsed_elements\n",
    "        with open(f'backup_extraction_nodes/element_summary_{doc_id}_{lim}.json', 'w') as f:\n",
    "            f.write(str(element_summary))\n",
    "        \n",
    "        with open(f'backup_extraction_nodes/extracted_elements_{doc_id}_{lim}.json', 'w') as f:\n",
    "            f.write(str(extracted_elements))\n",
    "        \n",
    "        with open(f'backup_extraction_nodes/not_parsed_elements_{doc_id}_{lim}.json', 'w') as f:\n",
    "            f.write(str(not_parsed_elements))\n",
    "        \n",
    "        print(\"Backup files stored.\\n\\n\")\n",
    "\n",
    "        # Step 7: Store the summary\n",
    "        print(\"Storing element summary in Neo4j...\")\n",
    "        with driver.session() as session:\n",
    "            # Step 7: Store the summary\n",
    "            session.execute_write(store_element_summary_in_graph, element_summary, doc_id)\n",
    "    \n",
    "    print(f\"Ingested {len(chunks)} chunks from {pdf_path} into Neo4j under Document {doc_id}\")\n",
    "    return element_summary, extracted_elements, not_parsed_elements\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# QA / RAG PIPELINE (Simplified)\n",
    "##################################################\n",
    "\n",
    "def perform_qa_with_graph(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    A simplified RAG approach:\n",
    "    1) Retrieve relevant chunks from the Neo4j graph (naive text search).\n",
    "    2) Build a context from those chunks.\n",
    "    3) Use either an open model or an OpenAI model for generative answer.\n",
    "\n",
    "    NOTE: This doesn't incorporate full community-based summarization from 2.6.\n",
    "    For a more complete approach, see `answer_query_from_communities()`.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        candidate_chunks = session.read_transaction(retrieve_relevant_chunks, user_query)\n",
    "\n",
    "    # Build the context\n",
    "    context = \"\\n\\n\".join([c[\"text\"] for c in candidate_chunks])\n",
    "\n",
    "    # Use a HuggingFace or OpenAI model for generation\n",
    "    if USE_OPENAI:\n",
    "        # If using OpenAI ChatCompletion:\n",
    "        # openai.api_key = OPENAI_API_KEY\n",
    "        # response = openai.ChatCompletion.create(\n",
    "        #     model=\"gpt-3.5-turbo\",\n",
    "        #     messages=[\n",
    "        #         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        #         {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {user_query}\"}\n",
    "        #     ]\n",
    "        # )\n",
    "        # answer = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        raise NotImplementedError(\"OpenAI ChatCompletion usage not fully implemented here.\")\n",
    "    else:\n",
    "        # Example with a local HF pipeline\n",
    "        qa_pipeline = pipeline(\"text-generation\", model=\"bigscience/bloom-560m\")\n",
    "        prompt = f\"Context: {context}\\nQuestion: {user_query}\\nAnswer:\"\n",
    "        answer_list = qa_pipeline(prompt, max_new_tokens=100, do_sample=True)\n",
    "        if answer_list:\n",
    "            answer = answer_list[0][\"generated_text\"].split(\"Answer:\")[-1].strip()\n",
    "        else:\n",
    "            answer = \"No answer generated.\"\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# MAIN EXECUTION EXAMPLE\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing PDF at data/docs/0704.2547.pdf...\n",
      "Extracted 148193 characters from data/docs/0704.2547.pdf \n",
      "\n",
      "\n",
      "Retrieving metadata for 0704.2547...\n",
      "Metadata: {'title': 'Inferring DNA sequences from mechanical unzipping data: the large-bandwidth case', 'summary': 'The complementary strands of DNA molecules can be separated when stretched\\napart by a force; the unzipping signal is correlated to the base content of the\\nsequence but is affected by thermal and instrumental noise. We consider here\\nthe ideal case where opening events are known to a very good time resolution\\n(very large bandwidth), and study how the sequence can be reconstructed from\\nthe unzipping data. Our approach relies on the use of statistical Bayesian\\ninference and of Viterbi decoding algorithm. Performances are studied\\nnumerically on Monte Carlo generated data, and analytically. We show how\\nmultiple unzippings of the same molecule may be exploited to improve the\\nquality of the prediction, and calculate analytically the number of required\\nunzippings as a function of the bandwidth, the sequence content, the elasticity\\nparameters of the unzipped strands.', 'url': 'http://arxiv.org/abs/0704.2547v1', 'authors': 'Valentina Baldazzi, Serena Bradde, Simona Cocco, Enzo Marinari, Remi Monasson', 'categories': 'q-bio.BM, cond-mat.stat-mech'}\n",
      "\n",
      "\n",
      "Chunked 284 segments from data/docs/0704.2547.pdf \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:43<00:00,  1.76s/it]s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n",
      "Summarized 11 nodes and 7 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:25<00:00,  1.04s/it]67.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n",
      "Summarized 15 nodes and 13 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:42<00:00,  1.70s/it]72.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n",
      "Summarized 12 nodes and 6 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:44<00:00,  1.77s/it]70.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n",
      "Summarized 11 nodes and 7 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:29<00:00,  1.18s/it]68.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n",
      "Summarized 8 nodes and 5 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 9/9 [00:20<00:00,  2.24s/it], 60.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n",
      "Summarized 11 nodes and 9 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel: 100%|██████████| 6/6 [06:13<00:00, 62.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 284 chunks from data/docs/0704.2547.pdf into Neo4j under Document 0704.2547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) Ingest an arXiv PDF (Steps 2.1–2.3)\n",
    "pdf_path = \"data/docs/0704.2547.pdf\"  # Replace with the path to your local arXiv PDF\n",
    "doc_id = \"0704.2547\"           # Arbitrary doc ID for grouping in Neo4j\n",
    "element_summary, extracted_elements, not_parsed_elements=ingest_pdf_into_graph(pdf_path, doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "The following steps are not yet implemented.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following steps are not yet implemented.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The following steps are not yet implemented."
     ]
    }
   ],
   "source": [
    "raise NotImplementedError(\"The following steps are not yet implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Community detection & summarization (Steps 2.4–2.5)\n",
    "\n",
    "# Initialize the community detection class\n",
    "detector = CommunityDetection(driver)\n",
    "\n",
    "# Project the graph for community detection\n",
    "detector.project_graph()\n",
    "\n",
    "# Set community IDs in the graph\n",
    "detector.set_communities()\n",
    "\n",
    "# Retrieve the communities\n",
    "communities = detector.retrieve_communities()\n",
    "\n",
    "# Drop the graph from memory\n",
    "detector.drop_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Community Summaries] Placeholder: gather summaries and do hierarchical summarization.\n"
     ]
    }
   ],
   "source": [
    "summarize_communities(communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/k27_kqbn1yvdcwxq44w79hw80000gn/T/ipykernel_28803/2195154659.py:16: DeprecationWarning: read_transaction has been renamed to execute_read\n",
      "  candidate_chunks = session.read_transaction(retrieve_relevant_chunks, user_query)\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Chunk)} {position: line: 2, column: 14, offset: 14} for query: '\\n    MATCH (c:Chunk)\\n    WHERE c.text CONTAINS $user_query\\n    RETURN c.chunk_id AS chunk_id, c.text AS text\\n    LIMIT $limit\\n    '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: text)} {position: line: 3, column: 13, offset: 33} for query: '\\n    MATCH (c:Chunk)\\n    WHERE c.text CONTAINS $user_query\\n    RETURN c.chunk_id AS chunk_id, c.text AS text\\n    LIMIT $limit\\n    '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: chunk_id)} {position: line: 4, column: 14, offset: 72} for query: '\\n    MATCH (c:Chunk)\\n    WHERE c.text CONTAINS $user_query\\n    RETURN c.chunk_id AS chunk_id, c.text AS text\\n    LIMIT $limit\\n    '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: text)} {position: line: 4, column: 38, offset: 96} for query: '\\n    MATCH (c:Chunk)\\n    WHERE c.text CONTAINS $user_query\\n    RETURN c.chunk_id AS chunk_id, c.text AS text\\n    LIMIT $limit\\n    '\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "OpenAI ChatCompletion usage not fully implemented here.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m user_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the main contributions of the paper?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Simple QA (naive RAG):\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m answer \u001b[38;5;241m=\u001b[39m perform_qa_with_graph(user_query)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 33\u001b[0m, in \u001b[0;36mperform_qa_with_graph\u001b[0;34m(user_query)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Use a HuggingFace or OpenAI model for generation\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m USE_OPENAI:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# If using OpenAI ChatCompletion:\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# openai.api_key = OPENAI_API_KEY\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# answer = response[\"choices\"][0][\"message\"][\"content\"]\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI ChatCompletion usage not fully implemented here.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Example with a local HF pipeline\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     qa_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigscience/bloom-560m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: OpenAI ChatCompletion usage not fully implemented here."
     ]
    }
   ],
   "source": [
    "# 3) Ask a question (Step 2.6 simplified vs. full approach)\n",
    "user_query = \"What are the main contributions of the paper?\"\n",
    "# Simple QA (naive RAG):\n",
    "answer = perform_qa_with_graph(user_query)\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, full approach using community-based QA:\n",
    "# global_answer = answer_query_from_communities(user_query)\n",
    "# print(f\"Global Answer: {global_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
