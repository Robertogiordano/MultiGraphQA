{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install neo4j\n",
    "# !pip install langchain\n",
    "# !pip install PyPDF2\n",
    "# !pip install tiktoken\n",
    "# !pip install openai  # Only if you want to use the OpenAI API\n",
    "# !pip install transformers  # For open (HF) models\n",
    "# !pip install sentence_transformers\n",
    "# !pip install -U langchain-community\n",
    "# For advanced community detection with Leiden, you might need external libraries (e.g., igraph, networkx, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertogiordano/Desktop/MultiGraphQA/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# -----------------------\n",
    "# Neo4j Database imports\n",
    "# -----------------------\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# -----------------------\n",
    "# LLM / Embeddings imports\n",
    "# -----------------------\n",
    "# If using HuggingFace transformers:\n",
    "from transformers import pipeline\n",
    "\n",
    "# If using LangChain for retrieval + QA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# If you want to use OpenAI, uncomment:\n",
    "import openai\n",
    "\n",
    "# -----------------------\n",
    "# Load environment variables\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# -----------------------\n",
    "# PDF Parsing library\n",
    "# -----------------------\n",
    "import PyPDF2  # or \"pypdf\" if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 1) CONFIGURATION: toggle open vs. OpenAI\n",
    "#############################################\n",
    "\n",
    "USE_OPENAI = False  # Set to True if you want to switch to OpenAI’s ChatGPT\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# For Neo4j:\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2) NEO4J CONNECTION AND GRAPH FUNCTIONS\n",
    "##################################################\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "\n",
    "def add_chunk_node(tx, chunk_text: str, chunk_id: str, embedding: List[float]):\n",
    "    \"\"\"\n",
    "    Create or merge a chunk node in Neo4j to represent a piece of text.\n",
    "    Store its embedding as well.\n",
    "    \"\"\"\n",
    "    embedding_str = \",\".join([str(x) for x in embedding])\n",
    "\n",
    "    query = \"\"\"\n",
    "    MERGE (c:Chunk {chunk_id: $chunk_id})\n",
    "    ON CREATE SET c.text = $chunk_text,\n",
    "                  c.embedding = $embedding_str\n",
    "    \"\"\"\n",
    "    tx.run(query, chunk_id=chunk_id, chunk_text=chunk_text, embedding_str=embedding_str)\n",
    "\n",
    "\n",
    "def add_document_relationship(tx, doc_id: str, chunk_id: str):\n",
    "    \"\"\"\n",
    "    Link each chunk node to a parent Document node in Neo4j.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MERGE (d:Document {doc_id: $doc_id})\n",
    "    MERGE (c:Chunk {chunk_id: $chunk_id})\n",
    "    MERGE (d)-[:HAS_CHUNK]->(c)\n",
    "    \"\"\"\n",
    "    tx.run(query, doc_id=doc_id, chunk_id=chunk_id)\n",
    "\n",
    "\n",
    "def add_element_instance(tx, element_data: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    (Step 2.2) Insert extracted graph node/edge relationships from text chunk.\n",
    "    Example structure of element_data might be:\n",
    "    {\n",
    "        \"entity_name\": \"...\",\n",
    "        \"entity_type\": \"...\",\n",
    "        \"entity_description\": \"...\",\n",
    "        \"relationship\": {\n",
    "            \"source_entity\": \"...\",\n",
    "            \"target_entity\": \"...\",\n",
    "            \"description\": \"...\",\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "\n",
    "    In a real pipeline, you might store these as separate nodes/edges. \n",
    "    For demonstration, we store them in a single node with a property that can be parsed.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    CREATE (e:ElementInstance {data: $element_data})\n",
    "    \"\"\"\n",
    "    tx.run(query, element_data=str(element_data))\n",
    "\n",
    "\n",
    "def retrieve_relevant_chunks(tx, user_query: str, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    A simplistic retrieval function that returns chunk nodes based on naive text search.\n",
    "    In a real scenario, you'd have a vector similarity search using embeddings.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MATCH (c:Chunk)\n",
    "    WHERE c.text CONTAINS $user_query\n",
    "    RETURN c.chunk_id AS chunk_id, c.text AS text\n",
    "    LIMIT $limit\n",
    "    \"\"\"\n",
    "    result = tx.run(query, user_query=user_query, limit=limit)\n",
    "    return [record.data() for record in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.1 SOURCE DOCUMENTS → TEXT CHUNKS\n",
    "##################################################\n",
    "\n",
    "def parse_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract raw text from a PDF file using PyPDF2.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 600, chunk_overlap: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    (Step 2.1) Split text into chunks. \n",
    "    Following the guidance in 2.1, we use a smaller chunk size (e.g., ~600 tokens).\n",
    "    This can improve entity recall at the cost of more LLM calls.\n",
    "    \"\"\"\n",
    "    # Note: the chunk_size is in characters by default using this splitter;\n",
    "    # you may want to adapt to token-based splitting.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PATH_DOCS= \"data/docs\"\n",
    "\n",
    "# Retrieve all docs from path\n",
    "docs_ids = sorted([f.replace('.pdf','') for f in os.listdir(PATH_DOCS) if f.endswith(\".pdf\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "# Fetch papers from arXiv\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    id_list=docs_ids[:10]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entry_id': 'http://arxiv.org/abs/0704.2547v1',\n",
       " 'updated': datetime.datetime(2007, 4, 19, 14, 45, 29, tzinfo=datetime.timezone.utc),\n",
       " 'published': datetime.datetime(2007, 4, 19, 14, 45, 29, tzinfo=datetime.timezone.utc),\n",
       " 'title': 'Inferring DNA sequences from mechanical unzipping data: the large-bandwidth case',\n",
       " 'authors': [arxiv.Result.Author('Valentina Baldazzi'),\n",
       "  arxiv.Result.Author('Serena Bradde'),\n",
       "  arxiv.Result.Author('Simona Cocco'),\n",
       "  arxiv.Result.Author('Enzo Marinari'),\n",
       "  arxiv.Result.Author('Remi Monasson')],\n",
       " 'summary': 'The complementary strands of DNA molecules can be separated when stretched\\napart by a force; the unzipping signal is correlated to the base content of the\\nsequence but is affected by thermal and instrumental noise. We consider here\\nthe ideal case where opening events are known to a very good time resolution\\n(very large bandwidth), and study how the sequence can be reconstructed from\\nthe unzipping data. Our approach relies on the use of statistical Bayesian\\ninference and of Viterbi decoding algorithm. Performances are studied\\nnumerically on Monte Carlo generated data, and analytically. We show how\\nmultiple unzippings of the same molecule may be exploited to improve the\\nquality of the prediction, and calculate analytically the number of required\\nunzippings as a function of the bandwidth, the sequence content, the elasticity\\nparameters of the unzipped strands.',\n",
       " 'comment': None,\n",
       " 'journal_ref': 'Phys. Rev. E 75 (2007) 011904',\n",
       " 'doi': '10.1103/PhysRevE.75.011904',\n",
       " 'primary_category': 'q-bio.BM',\n",
       " 'categories': ['q-bio.BM', 'cond-mat.stat-mech'],\n",
       " 'links': [arxiv.Result.Link('http://dx.doi.org/10.1103/PhysRevE.75.011904', title='doi', rel='related', content_type=None),\n",
       "  arxiv.Result.Link('http://arxiv.org/abs/0704.2547v1', title=None, rel='alternate', content_type=None),\n",
       "  arxiv.Result.Link('http://arxiv.org/pdf/0704.2547v1', title='pdf', rel='related', content_type=None)],\n",
       " 'pdf_url': 'http://arxiv.org/pdf/0704.2547v1',\n",
       " '_raw': {'id': 'http://arxiv.org/abs/0704.2547v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/0704.2547v1',\n",
       "  'updated': '2007-04-19T14:45:29Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2007, tm_mon=4, tm_mday=19, tm_hour=14, tm_min=45, tm_sec=29, tm_wday=3, tm_yday=109, tm_isdst=0),\n",
       "  'published': '2007-04-19T14:45:29Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2007, tm_mon=4, tm_mday=19, tm_hour=14, tm_min=45, tm_sec=29, tm_wday=3, tm_yday=109, tm_isdst=0),\n",
       "  'title': 'Inferring DNA sequences from mechanical unzipping data: the\\n  large-bandwidth case',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Inferring DNA sequences from mechanical unzipping data: the\\n  large-bandwidth case'},\n",
       "  'summary': 'The complementary strands of DNA molecules can be separated when stretched\\napart by a force; the unzipping signal is correlated to the base content of the\\nsequence but is affected by thermal and instrumental noise. We consider here\\nthe ideal case where opening events are known to a very good time resolution\\n(very large bandwidth), and study how the sequence can be reconstructed from\\nthe unzipping data. Our approach relies on the use of statistical Bayesian\\ninference and of Viterbi decoding algorithm. Performances are studied\\nnumerically on Monte Carlo generated data, and analytically. We show how\\nmultiple unzippings of the same molecule may be exploited to improve the\\nquality of the prediction, and calculate analytically the number of required\\nunzippings as a function of the bandwidth, the sequence content, the elasticity\\nparameters of the unzipped strands.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'The complementary strands of DNA molecules can be separated when stretched\\napart by a force; the unzipping signal is correlated to the base content of the\\nsequence but is affected by thermal and instrumental noise. We consider here\\nthe ideal case where opening events are known to a very good time resolution\\n(very large bandwidth), and study how the sequence can be reconstructed from\\nthe unzipping data. Our approach relies on the use of statistical Bayesian\\ninference and of Viterbi decoding algorithm. Performances are studied\\nnumerically on Monte Carlo generated data, and analytically. We show how\\nmultiple unzippings of the same molecule may be exploited to improve the\\nquality of the prediction, and calculate analytically the number of required\\nunzippings as a function of the bandwidth, the sequence content, the elasticity\\nparameters of the unzipped strands.'},\n",
       "  'authors': [{'name': 'Valentina Baldazzi'},\n",
       "   {'name': 'Serena Bradde'},\n",
       "   {'name': 'Simona Cocco'},\n",
       "   {'name': 'Enzo Marinari'},\n",
       "   {'name': 'Remi Monasson'}],\n",
       "  'author_detail': {'name': 'Remi Monasson'},\n",
       "  'arxiv_affiliation': 'LPTENS',\n",
       "  'author': 'Remi Monasson',\n",
       "  'arxiv_doi': '10.1103/PhysRevE.75.011904',\n",
       "  'links': [{'title': 'doi',\n",
       "    'href': 'http://dx.doi.org/10.1103/PhysRevE.75.011904',\n",
       "    'rel': 'related',\n",
       "    'type': 'text/html'},\n",
       "   {'href': 'http://arxiv.org/abs/0704.2547v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/0704.2547v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_journal_ref': 'Phys. Rev. E 75 (2007) 011904',\n",
       "  'arxiv_primary_category': {'term': 'q-bio.BM',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'q-bio.BM',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None},\n",
       "   {'term': 'cond-mat.stat-mech',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for result in client.results(search):\n",
    "    display(result.__dict__)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 10\n",
      "Number of chunks: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/k27_kqbn1yvdcwxq44w79hw80000gn/T/ipykernel_36577/1993685674.py:36: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding=HuggingFaceEmbeddings(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1735680055.393127  306142 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "2024-12-31 13:20:55,896 [ERROR][handler]: RPC error: [create_index], <MilvusException: (code=65535, message=invalid index type: HNSW, local mode only support FLAT IVF_FLAT AUTOINDEX: )>, <Time:{'RPC start': '2024-12-31 13:20:55.896040', 'RPC error': '2024-12-31 13:20:55.896607'}> (decorators.py:140)\n"
     ]
    }
   ],
   "source": [
    "### Milvus Lite Vectorstore\n",
    "\n",
    "import time\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "docs = []\n",
    "for result in client.results(search):\n",
    "    docs.append(\n",
    "        {\"title\": result.title, \n",
    "        \"summary\": result.summary, \n",
    "        \"url\": result.entry_id,\n",
    "        \"authors\": ', '.join([a.name for a in result.authors]),\n",
    "        \"categories\": ', '.join(result.categories)\n",
    "        }\n",
    "    )\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.create_documents(\n",
    "    [doc[\"summary\"] for doc in docs], metadatas=docs\n",
    ")\n",
    "\n",
    "print(f\"Number of papers: {len(docs)}\")\n",
    "print(f\"Number of chunks: {len(doc_splits)}\")\n",
    "\n",
    "\n",
    "# Add to Milvus\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag_milvus\",\n",
    "    embedding=HuggingFaceEmbeddings(\n",
    "        model_name=\"models/all-mpnet-base-v2\", \n",
    "        model_kwargs={\n",
    "            'device':\"mps\"\n",
    "        }\n",
    "    ),\n",
    "    connection_args={\"uri\": \"./vector_db_graphRAG/milvus_ingest.db\"},\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 4) EMBEDDING UTILITIES\n",
    "##################################################\n",
    "\n",
    "def get_hf_embedding_function(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Returns a function that can generate embeddings using a HuggingFace model.\n",
    "    \"\"\"\n",
    "    hf_embed = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    return hf_embed.embed_documents\n",
    "\n",
    "\n",
    "# If using OpenAI embeddings, uncomment and implement:\n",
    "# def get_openai_embedding_function(model_name: str = \"text-embedding-ada-002\"):\n",
    "#     def _embeddings(texts: List[str]) -> List[List[float]]:\n",
    "#         response = openai.Embedding.create(\n",
    "#             input=texts,\n",
    "#             model=model_name\n",
    "#         )\n",
    "#         embeddings = [item[\"embedding\"] for item in response[\"data\"]]\n",
    "#         return embeddings\n",
    "#     return _embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph documents: 10\n",
      "Nodes from 1st graph doc:[Node(id='Dna Molecules', type='Topic', properties={}), Node(id='Unzipping Signal', type='Topic', properties={}), Node(id='Base Content', type='Topic', properties={}), Node(id='Thermal Noise', type='Topic', properties={}), Node(id='Instrumental Noise', type='Topic', properties={}), Node(id='Bayesian Inference', type='Topic', properties={}), Node(id='Viterbi Decoding Algorithm', type='Topic', properties={}), Node(id='Monte Carlo Generated Data', type='Topic', properties={}), Node(id='Unzippings', type='Topic', properties={}), Node(id='Elasticity Parameters', type='Topic', properties={})]\n",
      "Relationships from 1st graph doc:[Relationship(source=Node(id='Dna Molecules', type='Topic', properties={}), target=Node(id='Unzipping Signal', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Unzipping Signal', type='Topic', properties={}), target=Node(id='Base Content', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Unzipping Signal', type='Topic', properties={}), target=Node(id='Thermal Noise', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Unzipping Signal', type='Topic', properties={}), target=Node(id='Instrumental Noise', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Unzipping Signal', type='Topic', properties={}), target=Node(id='Bayesian Inference', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Bayesian Inference', type='Topic', properties={}), target=Node(id='Viterbi Decoding Algorithm', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Viterbi Decoding Algorithm', type='Topic', properties={}), target=Node(id='Monte Carlo Generated Data', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Unzippings', type='Topic', properties={}), target=Node(id='Quality Of The Prediction', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Unzippings', type='Topic', properties={}), target=Node(id='Bandwidth', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Unzippings', type='Topic', properties={}), target=Node(id='Sequence Content', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Unzippings', type='Topic', properties={}), target=Node(id='Elasticity Parameters', type='Topic', properties={}), type='RELATED_TO', properties={})]\n"
     ]
    }
   ],
   "source": [
    "# GraphRAG Setup\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "\n",
    "graph_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "graph_transformer = LLMGraphTransformer(\n",
    "    llm=graph_llm,\n",
    "    allowed_nodes=[\"Paper\", \"Author\", \"Topic\"],\n",
    "    node_properties=[\"title\", \"summary\", \"url\"],\n",
    "    allowed_relationships=[\"AUTHORED\", \"DISCUSSES\", \"RELATED_TO\"],\n",
    ")\n",
    "\n",
    "graph_documents = graph_transformer.convert_to_graph_documents(doc_splits)\n",
    "\n",
    "graph.add_graph_documents(graph_documents)\n",
    "\n",
    "print(f\"Graph documents: {len(graph_documents)}\")\n",
    "print(f\"Nodes from 1st graph doc:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships from 1st graph doc:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0:\n",
      "  Nodes: [Node(id='Dna Molecules', type='Topic', properties={}), Node(id='Unzipping Signal', type='Topic', properties={}), Node(id='Base Content', type='Topic', properties={}), Node(id='Thermal Noise', type='Topic', properties={}), Node(id='Instrumental Noise', type='Topic', properties={}), Node(id='Bayesian Inference', type='Topic', properties={}), Node(id='Viterbi Decoding Algorithm', type='Topic', properties={}), Node(id='Monte Carlo Generated Data', type='Topic', properties={}), Node(id='Unzippings', type='Topic', properties={}), Node(id='Elasticity Parameters', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Dna Molecules', type='Topic', properties={}), target=Node(id='Unzipping Signal', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Unzipping Signal', type='Topic', properties={}), target=Node(id='Base Content', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Unzipping Signal', type='Topic', properties={}), target=Node(id='Thermal Noise', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Unzipping Signal', type='Topic', properties={}), target=Node(id='Instrumental Noise', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Unzipping Signal', type='Topic', properties={}), target=Node(id='Bayesian Inference', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Bayesian Inference', type='Topic', properties={}), target=Node(id='Viterbi Decoding Algorithm', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Viterbi Decoding Algorithm', type='Topic', properties={}), target=Node(id='Monte Carlo Generated Data', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Unzippings', type='Topic', properties={}), target=Node(id='Quality Of The Prediction', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Unzippings', type='Topic', properties={}), target=Node(id='Bandwidth', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Unzippings', type='Topic', properties={}), target=Node(id='Sequence Content', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Unzippings', type='Topic', properties={}), target=Node(id='Elasticity Parameters', type='Topic', properties={}), type='RELATED_TO', properties={})]\n",
      "---\n",
      "Document 1:\n",
      "  Nodes: [Node(id='Shock Probes', type='Topic', properties={}), Node(id='One-Dimensional Driven Diffusive Medium', type='Topic', properties={}), Node(id='Nearest Neighbor Ising Interaction', type='Topic', properties={}), Node(id='Kls Model', type='Topic', properties={}), Node(id='Static And Dynamical Correlation Functions', type='Topic', properties={}), Node(id='Numerical Simulations', type='Topic', properties={}), Node(id='Asep', type='Topic', properties={}), Node(id='Short-Ranged Correlations', type='Topic', properties={}), Node(id='Large Time And Large Distance Properties', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Shock Probes', type='Topic', properties={}), target=Node(id='One-Dimensional Driven Diffusive Medium', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='One-Dimensional Driven Diffusive Medium', type='Topic', properties={}), target=Node(id='Nearest Neighbor Ising Interaction', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='One-Dimensional Driven Diffusive Medium', type='Topic', properties={}), target=Node(id='Kls Model', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Kls Model', type='Topic', properties={}), target=Node(id='Static And Dynamical Correlation Functions', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Numerical Simulations', type='Topic', properties={}), target=Node(id='Static And Dynamical Correlation Functions', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Numerical Simulations', type='Topic', properties={}), target=Node(id='Asep', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Nearest Neighbor Ising Interaction', type='Topic', properties={}), target=Node(id='Short-Ranged Correlations', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Short-Ranged Correlations', type='Topic', properties={}), target=Node(id='Large Time And Large Distance Properties', type='Topic', properties={}), type='RELATED_TO', properties={})]\n",
      "---\n",
      "Document 2:\n",
      "  Nodes: [Node(id='Nested Chinese Restaurant Process', type='Topic', properties={'title': 'nested Chinese restaurant process'}), Node(id='Bayesian Nonparametric Model', type='Topic', properties={'title': 'Bayesian nonparametric model'}), Node(id='Information Retrieval', type='Topic', properties={'title': 'information retrieval'}), Node(id='Scientific Abstracts', type='Topic', properties={'title': 'scientific abstracts'}), Node(id='Stochastic Process', type='Topic', properties={'title': 'stochastic process'})]\n",
      "  Relationships: [Relationship(source=Node(id='Nested Chinese Restaurant Process', type='Topic', properties={}), target=Node(id='Bayesian Nonparametric Model', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Bayesian Nonparametric Model', type='Topic', properties={}), target=Node(id='Information Retrieval', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Information Retrieval', type='Topic', properties={}), target=Node(id='Scientific Abstracts', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Stochastic Process', type='Topic', properties={}), target=Node(id='Nested Chinese Restaurant Process', type='Topic', properties={}), type='RELATED_TO', properties={})]\n",
      "---\n",
      "Document 3:\n",
      "  Nodes: [Node(id='Ch4', type='Topic', properties={}), Node(id='Spitzer Space Telescope', type='Topic', properties={}), Node(id='Low Mass Young Stellar Objects', type='Topic', properties={}), Node(id='High Mass Star Forming Regions', type='Topic', properties={}), Node(id='C2D', type='Topic', properties={}), Node(id='Solid Ch4 Abundance', type='Topic', properties={}), Node(id='Solid H2O', type='Topic', properties={}), Node(id='Ch3Oh', type='Topic', properties={}), Node(id='Co2', type='Topic', properties={}), Node(id='Co', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Ch4', type='Topic', properties={}), target=Node(id='Solid Ch4 Abundance', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Spitzer Space Telescope', type='Topic', properties={}), target=Node(id='Low Mass Young Stellar Objects', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='C2D', type='Topic', properties={}), target=Node(id='Solid Ch4 Abundance', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Solid H2O', type='Topic', properties={}), target=Node(id='Ch4', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Ch3Oh', type='Topic', properties={}), target=Node(id='Ch4', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Co2', type='Topic', properties={}), target=Node(id='Ch4', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Co', type='Topic', properties={}), target=Node(id='Ch4', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Low Mass Young Stellar Objects', type='Topic', properties={}), target=Node(id='High Mass Star Forming Regions', type='Topic', properties={}), type='RELATED_TO', properties={})]\n",
      "---\n",
      "Document 4:\n",
      "  Nodes: [Node(id='Neutron Stars', type='Topic', properties={}), Node(id='Thermal Conductivity', type='Topic', properties={}), Node(id='Molecular Dynamics Simulations', type='Topic', properties={}), Node(id='Crust', type='Topic', properties={}), Node(id='Impurities', type='Topic', properties={}), Node(id='Proton Capture Nucleosynthesis', type='Topic', properties={}), Node(id='Electron Captures', type='Topic', properties={}), Node(id='Phase Separation', type='Topic', properties={}), Node(id='Asymmetric Star', type='Topic', properties={}), Node(id='Gravitational Waves', type='Topic', properties={}), Node(id='Crust Cooling', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Neutron Stars', type='Topic', properties={}), target=Node(id='Crust Cooling', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Crust Cooling', type='Topic', properties={}), target=Node(id='Thermal Conductivity', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Molecular Dynamics Simulations', type='Topic', properties={}), target=Node(id='Crust', type='Topic', properties={}), type='AUTHORED', properties={}), Relationship(source=Node(id='Crust', type='Topic', properties={}), target=Node(id='Impurities', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Proton Capture Nucleosynthesis', type='Topic', properties={}), target=Node(id='Impurities', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Electron Captures', type='Topic', properties={}), target=Node(id='Impurities', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Crust', type='Topic', properties={}), target=Node(id='Phase Separation', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Phase Separation', type='Topic', properties={}), target=Node(id='Asymmetric Star', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Asymmetric Star', type='Topic', properties={}), target=Node(id='Gravitational Waves', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Crust Cooling', type='Topic', properties={}), target=Node(id='Impurities', type='Topic', properties={}), type='DISCUSSES', properties={})]\n",
      "---\n",
      "Document 5:\n",
      "  Nodes: [Node(id='Collective Rearrangements In Solution', type='Topic', properties={}), Node(id='Protein Folding', type='Topic', properties={}), Node(id='Nanocrystal Phase Transitions', type='Topic', properties={}), Node(id='Transition Path Sampling', type='Topic', properties={}), Node(id='New Generation Algorithm', type='Topic', properties={}), Node(id='Precision Shooting Technique', type='Topic', properties={}), Node(id='Isomerization Process', type='Topic', properties={}), Node(id='Dense Liquid Of Soft Spheres', type='Topic', properties={}), Node(id='Barrier Crossing Events', type='Topic', properties={}), Node(id='Metastable Intermediate States', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Collective Rearrangements In Solution', type='Topic', properties={}), target=Node(id='Protein Folding', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Collective Rearrangements In Solution', type='Topic', properties={}), target=Node(id='Nanocrystal Phase Transitions', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Transition Path Sampling', type='Topic', properties={}), target=Node(id='New Generation Algorithm', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='New Generation Algorithm', type='Topic', properties={}), target=Node(id='Precision Shooting Technique', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Precision Shooting Technique', type='Topic', properties={}), target=Node(id='Isomerization Process', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Isomerization Process', type='Topic', properties={}), target=Node(id='Dense Liquid Of Soft Spheres', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='New Generation Algorithm', type='Topic', properties={}), target=Node(id='Barrier Crossing Events', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Barrier Crossing Events', type='Topic', properties={}), target=Node(id='Metastable Intermediate States', type='Topic', properties={}), type='RELATED_TO', properties={})]\n",
      "---\n",
      "Document 6:\n",
      "  Nodes: [Node(id='Interlayer Electrical And Thermal Resistivity', type='Topic', properties={}), Node(id='Fermi Liquid Quasiparticles', type='Topic', properties={}), Node(id='Two-Dimensional Antiferromagnetic Spin Fluctuations', type='Topic', properties={}), Node(id='Wiedemann-Franz Law', type='Topic', properties={}), Node(id='Cecoin5', type='Topic', properties={}), Node(id='Magnetic Field-Induced Quantum Phase Transition', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Interlayer Electrical And Thermal Resistivity', type='Topic', properties={}), target=Node(id='Fermi Liquid Quasiparticles', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Fermi Liquid Quasiparticles', type='Topic', properties={}), target=Node(id='Two-Dimensional Antiferromagnetic Spin Fluctuations', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Interlayer Electrical And Thermal Resistivity', type='Topic', properties={}), target=Node(id='Wiedemann-Franz Law', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Cecoin5', type='Topic', properties={}), target=Node(id='Magnetic Field-Induced Quantum Phase Transition', type='Topic', properties={}), type='DISCUSSES', properties={})]\n",
      "---\n",
      "Document 7:\n",
      "  Nodes: [Node(id='Short-Lived Radionuclides', type='Topic', properties={}), Node(id=\"Astrophysical Context Of Sun'S Birth Place\", type='Topic', properties={}), Node(id='Solar Accretion Disk', type='Topic', properties={}), Node(id='60Fe', type='Topic', properties={}), Node(id='Supernovae', type='Topic', properties={}), Node(id='Star Formation', type='Topic', properties={}), Node(id='4.57 Ga Ago', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Short-Lived Radionuclides', type='Topic', properties={}), target=Node(id=\"Astrophysical Context Of Sun'S Birth Place\", type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Short-Lived Radionuclides', type='Topic', properties={}), target=Node(id='Solar Accretion Disk', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='60Fe', type='Topic', properties={}), target=Node(id='Supernovae', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Supernovae', type='Topic', properties={}), target=Node(id='Star Formation', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Solar Accretion Disk', type='Topic', properties={}), target=Node(id='4.57 Ga Ago', type='Topic', properties={}), type='RELATED_TO', properties={})]\n",
      "---\n",
      "Document 8:\n",
      "  Nodes: [Node(id='Active Nonlinear Microrheology', type='Topic', properties={}), Node(id='Colloidal Model Systems', type='Topic', properties={}), Node(id='Friction', type='Topic', properties={}), Node(id='Mode-Coupling Theory', type='Topic', properties={}), Node(id='Microrheology Data', type='Topic', properties={}), Node(id='Simulations', type='Topic', properties={}), Node(id='Simplified Model', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Active Nonlinear Microrheology', type='Topic', properties={}), target=Node(id='Colloidal Model Systems', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Active Nonlinear Microrheology', type='Topic', properties={}), target=Node(id='Friction', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Mode-Coupling Theory', type='Topic', properties={}), target=Node(id='Friction', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Microrheology Data', type='Topic', properties={}), target=Node(id='Simulations', type='Topic', properties={}), type='AUTHORED', properties={}), Relationship(source=Node(id='Simulations', type='Topic', properties={}), target=Node(id='Simplified Model', type='Topic', properties={}), type='DISCUSSES', properties={})]\n",
      "---\n",
      "Document 9:\n",
      "  Nodes: [Node(id='Optical Meta-Materials', type='Topic', properties={}), Node(id='Matter Waves', type='Topic', properties={}), Node(id='Pulsed Comoving Magnetic Fields', type='Topic', properties={}), Node(id='Atomic Phase Shift', type='Topic', properties={}), Node(id='Transient Negative Group Velocity', type='Topic', properties={}), Node(id='Negative Refraction', type='Topic', properties={}), Node(id='Slow Metastable Argon Atoms Ar*(3P2)', type='Topic', properties={}), Node(id='Beam Splitter', type='Topic', properties={}), Node(id='Atomic Meta-Lens', type='Topic', properties={}), Node(id='Meta-Media', type='Topic', properties={}), Node(id='Atom Optics', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Optical Meta-Materials', type='Topic', properties={}), target=Node(id='Matter Waves', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Pulsed Comoving Magnetic Fields', type='Topic', properties={}), target=Node(id='Atomic Phase Shift', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Atomic Phase Shift', type='Topic', properties={}), target=Node(id='Transient Negative Group Velocity', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Transient Negative Group Velocity', type='Topic', properties={}), target=Node(id='Negative Refraction', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Slow Metastable Argon Atoms Ar*(3P2)', type='Topic', properties={}), target=Node(id='Beam Splitter', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Slow Metastable Argon Atoms Ar*(3P2)', type='Topic', properties={}), target=Node(id='Atomic Meta-Lens', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Meta-Media', type='Topic', properties={}), target=Node(id='Atom Optics', type='Topic', properties={}), type='RELATED_TO', properties={})]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# After converting to graph documents\n",
    "for i, doc in enumerate(graph_documents):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(f\"  Nodes: {doc.nodes}\")\n",
    "    print(f\"  Relationships: {doc.relationships}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.2 TEXT CHUNKS → ELEMENT INSTANCES\n",
    "##################################################\n",
    "\n",
    "def extract_element_instances_from_chunk(\n",
    "    chunk_text: str,\n",
    "    gleaning_rounds: int = 1\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    (Step 2.2) Use an LLM prompt to identify entity references, relationships, and covariates.\n",
    "    - Our default prompt identifies entities (name, type, description) and relationships.\n",
    "    - We support multiple rounds of \"gleanings\" to find any missed entities (multi-stage approach).\n",
    "    For demonstration, this is a stub that returns dummy data.\n",
    "    \"\"\"\n",
    "\n",
    "    # -- Basic LLM extraction prompt (conceptual) --\n",
    "    # e.g., \"Identify all entities and relationships in the following text ...\"\n",
    "\n",
    "    # If gleaning rounds > 1, you’d incorporate prompts that ask:\n",
    "    #  - \"Were any entities missed? (Yes/No) [Logit bias forcing yes/no]\"\n",
    "    #  - \"Please glean the missing entities.\" \n",
    "    # This is a simplified representation:\n",
    "\n",
    "    dummy_data = [\n",
    "        {\n",
    "            \"entity_name\": \"ExampleEntity\",\n",
    "            \"entity_type\": \"Person\",\n",
    "            \"entity_description\": \"A person of interest in the text.\",\n",
    "            \"relationship\": {\n",
    "                \"source_entity\": \"ExampleEntity\",\n",
    "                \"target_entity\": \"AnotherEntity\",\n",
    "                \"description\": \"A potential relationship described here.\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # In reality, you'd call your LLM with few-shot examples specialized to your domain.\n",
    "    # Each gleaning round might add newly discovered entities/relationships.\n",
    "    # Here, just return dummy data for demonstration.\n",
    "    return dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.3 ELEMENT INSTANCES → ELEMENT SUMMARIES\n",
    "##################################################\n",
    "\n",
    "def summarize_element_instances(element_instances: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    (Step 2.3) Summarize extracted nodes/relationships into a single descriptive block of text\n",
    "    for each chunk. This is an additional LLM-based summarization step, forming \"element summaries.\"\n",
    "    For demonstration, we simply concatenate them into a single string.\n",
    "    \"\"\"\n",
    "    # In practice, you might use an LLM to produce an abstractive summary that merges\n",
    "    # repeated references, standardizes entity mention formats, etc.\n",
    "    summary_parts = []\n",
    "    for e in element_instances:\n",
    "        entity_str = (f\"Entity: {e.get('entity_name')} ({e.get('entity_type')}). \"\n",
    "                      f\"Description: {e.get('entity_description')}\")\n",
    "        rel = e.get(\"relationship\", {})\n",
    "        relationship_str = (f\"Relationship: {rel.get('source_entity')} -> {rel.get('target_entity')}. \"\n",
    "                            f\"Desc: {rel.get('description')}\")\n",
    "        summary_parts.append(f\"{entity_str}\\n{relationship_str}\\n\")\n",
    "\n",
    "    return \"\\n\".join(summary_parts)\n",
    "\n",
    "\n",
    "def store_element_summary_in_graph(tx, summary_text: str):\n",
    "    \"\"\"\n",
    "    For demonstration, store the summary as a node in Neo4j. \n",
    "    In a real solution, you might merge it back into existing entity/relationship nodes\n",
    "    or create a dedicated \"Summary\" node referencing them.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    CREATE (s:ElementSummary {summary_text: $summary_text})\n",
    "    \"\"\"\n",
    "    tx.run(query, summary_text=summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.4 ELEMENT SUMMARIES → GRAPH COMMUNITIES\n",
    "##################################################\n",
    "\n",
    "def detect_communities():\n",
    "    \"\"\"\n",
    "    (Step 2.4) Perform community detection on the stored nodes/edges in the graph.\n",
    "    For demonstration, we omit the full code for Leiden or other algorithms.\n",
    "    In a real system, you’d gather the graph elements from Neo4j, run community detection,\n",
    "    and store the results (community IDs, hierarchical structure, etc.) back into Neo4j.\n",
    "    \"\"\"\n",
    "    # Placeholder function\n",
    "    print(\"[Community Detection] Placeholder: run Leiden or other community detection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.5 GRAPH COMMUNITIES → COMMUNITY SUMMARIES\n",
    "##################################################\n",
    "\n",
    "def summarize_communities():\n",
    "    \"\"\"\n",
    "    (Step 2.5) Summarize each community (or sub-community in a hierarchical approach).\n",
    "    - Gather all element summaries (nodes, edges, covariates) in that community.\n",
    "    - Summarize them, potentially chunking if they don't fit in an LLM context window.\n",
    "    \"\"\"\n",
    "    # Placeholder logic\n",
    "    print(\"[Community Summaries] Placeholder: gather summaries and do hierarchical summarization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.6 COMMUNITY SUMMARIES → COMMUNITY ANSWERS → GLOBAL ANSWER\n",
    "##################################################\n",
    "\n",
    "def answer_query_from_communities(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    (Step 2.6) Use the hierarchical community summaries to answer user queries globally.\n",
    "    - In an actual implementation, you'd fetch the relevant community summaries, chunk them,\n",
    "      run partial QA on each chunk, rank answers by helpfulness, and then produce a final answer.\n",
    "    - Below is a simplified approach that just returns a single, direct LLM-based QA.\n",
    "    \"\"\"\n",
    "    # Placeholder logic\n",
    "    # If you have multiple community summaries, you'd do partial QA in parallel, rank by\n",
    "    # self-reported \"helpfulness\" (0-100), then combine or reduce them into a global answer.\n",
    "\n",
    "    return f\"Global answer to '{user_query}' (placeholder).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 5) INGEST PDF -> STORE IN GRAPH (Putting Steps 2.1 and 2.2+ in context)\n",
    "##################################################\n",
    "\n",
    "def ingest_pdf_into_graph(pdf_path: str, doc_id: str):\n",
    "    \"\"\"\n",
    "    1) Parse PDF into raw text.\n",
    "    2) Chunk it (Step 2.1).\n",
    "    3) Generate embeddings for each chunk.\n",
    "    4) Store chunk nodes in Neo4j.\n",
    "    5) For each chunk, call LLM to extract element instances (Step 2.2).\n",
    "    6) Summarize them into a single descriptive block (Step 2.3).\n",
    "    7) Optionally store the block in Neo4j for further community detection.\n",
    "    \"\"\"\n",
    "    # Step 1: Parse PDF\n",
    "    raw_text = parse_pdf(pdf_path)\n",
    "\n",
    "    # Step 2: Chunk the text (default chunk_size=600 for improved recall)\n",
    "    chunks = chunk_text(raw_text)\n",
    "\n",
    "    # Step 3: Embeddings\n",
    "    if USE_OPENAI:\n",
    "        # Implement an OpenAI embedding function if desired\n",
    "        raise NotImplementedError(\"OpenAI embeddings not implemented here.\")\n",
    "    else:\n",
    "        embed_fn = get_hf_embedding_function()\n",
    "\n",
    "    embeddings = embed_fn(chunks)\n",
    "\n",
    "    # Step 4: Store chunk nodes in Neo4j\n",
    "    with driver.session() as session:\n",
    "        for i, chunk_text_str in enumerate(chunks):\n",
    "            chunk_id = f\"{doc_id}_chunk_{i}\"\n",
    "            embedding = embeddings[i]\n",
    "            session.execute_write(add_chunk_node, chunk_text_str, chunk_id, embedding)\n",
    "            session.execute_write(add_document_relationship, doc_id, chunk_id)\n",
    "\n",
    "            # Step 5: Extract element instances\n",
    "            element_instances = extract_element_instances_from_chunk(chunk_text_str)\n",
    "\n",
    "            # Step 6: Summarize them (element-level)\n",
    "            element_summary = summarize_element_instances(element_instances)\n",
    "\n",
    "            # Step 7: Store the summary\n",
    "            session.execute_write(store_element_summary_in_graph, element_summary)\n",
    "\n",
    "    print(f\"Ingested {len(chunks)} chunks from {pdf_path} into Neo4j under Document {doc_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# QA / RAG PIPELINE (Simplified)\n",
    "##################################################\n",
    "\n",
    "def perform_qa_with_graph(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    A simplified RAG approach:\n",
    "    1) Retrieve relevant chunks from the Neo4j graph (naive text search).\n",
    "    2) Build a context from those chunks.\n",
    "    3) Use either an open model or an OpenAI model for generative answer.\n",
    "\n",
    "    NOTE: This doesn't incorporate full community-based summarization from 2.6.\n",
    "    For a more complete approach, see `answer_query_from_communities()`.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        candidate_chunks = session.read_transaction(retrieve_relevant_chunks, user_query)\n",
    "\n",
    "    # Build the context\n",
    "    context = \"\\n\\n\".join([c[\"text\"] for c in candidate_chunks])\n",
    "\n",
    "    # Use a HuggingFace or OpenAI model for generation\n",
    "    if USE_OPENAI:\n",
    "        # If using OpenAI ChatCompletion:\n",
    "        # openai.api_key = OPENAI_API_KEY\n",
    "        # response = openai.ChatCompletion.create(\n",
    "        #     model=\"gpt-3.5-turbo\",\n",
    "        #     messages=[\n",
    "        #         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        #         {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {user_query}\"}\n",
    "        #     ]\n",
    "        # )\n",
    "        # answer = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        raise NotImplementedError(\"OpenAI ChatCompletion usage not fully implemented here.\")\n",
    "    else:\n",
    "        # Example with a local HF pipeline\n",
    "        qa_pipeline = pipeline(\"text-generation\", model=\"bigscience/bloom-560m\")\n",
    "        prompt = f\"Context: {context}\\nQuestion: {user_query}\\nAnswer:\"\n",
    "        answer_list = qa_pipeline(prompt, max_new_tokens=100, do_sample=True)\n",
    "        if answer_list:\n",
    "            answer = answer_list[0][\"generated_text\"].split(\"Answer:\")[-1].strip()\n",
    "        else:\n",
    "            answer = \"No answer generated.\"\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# MAIN EXECUTION EXAMPLE\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Ingest an arXiv PDF (Steps 2.1–2.3)\n",
    "pdf_path = \"data/docs/0704.2547.pdf\"  # Replace with the path to your local arXiv PDF\n",
    "doc_id = \"0704.2547\"           # Arbitrary doc ID for grouping in Neo4j\n",
    "ingest_pdf_into_graph(pdf_path, doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Community detection & summarization (Steps 2.4–2.5)\n",
    "detect_communities()\n",
    "summarize_communities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Ask a question (Step 2.6 simplified vs. full approach)\n",
    "user_query = \"What are the main contributions of the paper?\"\n",
    "# Simple QA (naive RAG):\n",
    "answer = perform_qa_with_graph(user_query)\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, full approach using community-based QA:\n",
    "# global_answer = answer_query_from_communities(user_query)\n",
    "# print(f\"Global Answer: {global_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
