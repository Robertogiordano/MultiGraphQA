{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG pipeline\n",
    "\n",
    "Developed by Roberto Giordano\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project is based on the research paper titled *\"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"* created by Microsoft. The primary objective of this work is to explore and implement the methodologies described in the paper, focusing on leveraging Graph Retrieval-Augmented Generation (Graph RAG) techniques which are able to capture relationships and context across multiple sources, enabling richer and more insightful responses.\n",
    "\n",
    "### Problem Statement\n",
    "Modern summarization and QA systems struggle with repository-level questions that require the synthesis of information distributed across multiple documents. The Graph RAG approach aims to overcome this limitation by modeling entities and their relationships through graph structures, facilitating complex information retrieval and generation tasks.\n",
    "\n",
    "\n",
    "### Objectives\n",
    "1. Implement a Graph RAG framework for repository-level questions.\n",
    "2. Use datasets derived from arXiv papers, ensuring coherence with the multimodal module.\n",
    "3. Develope all the tool with the possibility to switch between closed-sources (openai) and open-sources models (ollama).\n",
    "4. Demonstrate the framework through some example questions.\n",
    "\n",
    "### Budget and Tools\n",
    "The framework is built using state-of-the-art technologies such as Neo4j, Milvus, and LangGraph for robust and scalable performance.\n",
    "\n",
    "Each document extraction and integration costs ~0.06$-0.08$ using gpt-4o-mini.\n",
    "\n",
    "This version indeed, differs from the paper since uses also a vector database to enhance retrieval performances and set the bases for futre developments and improvements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install neo4j\n",
    "# !pip install plotly\n",
    "# !pip install langchain\n",
    "# !pip install PyPDF2\n",
    "# !pip install tiktoken\n",
    "# !pip install openai  # Only if you want to use the OpenAI API\n",
    "# !pip install transformers  # For open (HF) models\n",
    "# !pip install sentence_transformers\n",
    "# !pip install -U langchain-community\n",
    "# !pip install -qU  langchain_milvus\n",
    "# !pip install -U langchain-ollama\n",
    "# !pip install graphdatascience\n",
    "# For advanced community detection with Leiden, you might need external libraries (e.g., igraph, networkx, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama pull llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "# -----------------------\n",
    "# Neo4j Database imports\n",
    "# -----------------------\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# -----------------------\n",
    "# LLM / Embeddings imports\n",
    "# -----------------------\n",
    "\n",
    "# LangChain for retrieval + QA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "from langchain_milvus import Milvus\n",
    "from uuid import uuid4\n",
    "from pymilvus import MilvusClient\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# to use OpenAI:\n",
    "import openai\n",
    "openai_model=\"gpt-4o-mini\"\n",
    "\n",
    "# -----------------------\n",
    "# Load environment variables\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# -----------------------\n",
    "# ArXiv API\n",
    "# -----------------------\n",
    "import arxiv\n",
    "\n",
    "# -----------------------\n",
    "# PDF Parsing library\n",
    "# -----------------------\n",
    "import PyPDF2  # or \"pypdf\" if needed\n",
    "\n",
    "# -----------------------\n",
    "# Plotly for visualization\n",
    "# -----------------------\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 1) CONFIGURATION: toggle open vs. OpenAI\n",
    "#############################################\n",
    "\n",
    "USE_OPENAI = True  # Set to True if you want to switch to OpenAI’s ChatGPT\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# For Neo4j:\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Source Documents → Text Chunks\n",
    "\n",
    "**Objective**  \n",
    "The goal is to divide the original documents (referred to as \"source documents\") into shorter text chunks before proceeding with the extraction of entities and relationships from these smaller text segments.\n",
    "\n",
    "If the chunks are too long, less splitting work is required (resulting in fewer calls to the LLM); however, the LLM struggles to maintain high levels of recall and precision. In other words, larger context windows tend to \"lose\" more references (the text cites studies by Kuratov et al., 2024 and Liu et al., 2023 in support of this observation).\n",
    "\n",
    "Conversely, if the chunks are too short, more calls to the LLM will be necessary, but the likelihood of accurately extracting a greater number of entities improves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.1 SOURCE DOCUMENTS → TEXT CHUNKS\n",
    "##################################################\n",
    "\n",
    "def parse_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract raw text from a PDF file using PyPDF2.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def find_metadata(doc_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    (Step 2.1) Retrieve metadata for a document from a database or API.\n",
    "    \"\"\"\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        id_list=[doc_id]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        result = next(client.results(search))\n",
    "        return \\\n",
    "            {\"title\": result.title, \n",
    "            \"summary\": result.summary, \n",
    "            \"url\": result.entry_id,\n",
    "            \"authors\": ', '.join([a.name for a in result.authors]),\n",
    "            \"categories\": ', '.join(result.categories)\n",
    "            }\n",
    "    except StopIteration:\n",
    "        return {}\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 600, chunk_overlap: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    (Step 2.1) Split text into chunks. \n",
    "    Following the guidance in 2.1, we use a smaller chunk size (e.g., ~600 tokens).\n",
    "    This can improve entity recall at the cost of more LLM calls.\n",
    "    \"\"\"\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an embedder model to populate in a second step a vector database to enhance retrieving and lower the costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# EMBEDDING UTILITIES\n",
    "##################################################\n",
    "\n",
    "def get_hf_embedding_function(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\", device: str = \"mps\", USE_OPENAI: bool = False):\n",
    "    \"\"\"\n",
    "    Returns a function that can generate embeddings using a HuggingFace model.\n",
    "    \"\"\"\n",
    "    if not USE_OPENAI:\n",
    "        hf_embed = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={'device': device})\n",
    "        return hf_embed\n",
    "    else:\n",
    "        def _embeddings(texts: List[str], model_name: str = \"text-embedding-ada-002\") -> List[List[float]]:\n",
    "            response = openai.Embedding.create(\n",
    "                input=texts,\n",
    "                model=model_name\n",
    "            )\n",
    "            embeddings = [item[\"embedding\"] for item in response[\"data\"]]\n",
    "            return embeddings\n",
    "        return _embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# VECTORSTORE UTILITIES\n",
    "##################################################\n",
    "\n",
    "def init_milvus_db(collection_name: str, uri: str, embedding_function):\n",
    "    \"\"\"\n",
    "    Initialize the Milvus database if it does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(uri):\n",
    "        print(f\"Creating database at {uri}\")\n",
    "\n",
    "    vectorstore = Milvus(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=embedding_function,\n",
    "        connection_args={\"uri\": uri},\n",
    "    )\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "# Function to add multiple documents to Milvus\n",
    "def add_documents_to_milvus(docs: list, embedding_function, collection_name: str = \"rag_milvus\", uri: str = \"./vector_db_graphRAG/milvus_ingest.db\"):\n",
    "    \"\"\"\n",
    "    Add multiple documents to the Milvus vector store.\n",
    "\n",
    "    Args:\n",
    "        docs (list): List of tuples containing text and metadata.\n",
    "        collection_name (str): Name of the Milvus collection.\n",
    "        uri (str): URI for the Milvus database.\n",
    "    \n",
    "    # Example usage\n",
    "    docs = [\n",
    "        {\"text\": \"Chunk 1 of the document\", \"metadata\": {\"doc_id\": \"doc_1\", \"chunk\": 1}},\n",
    "        {\"text\": \"Chunk 2 of the document\", \"metadata\": {\"doc_id\": \"doc_1\", \"chunk\": 2}}\n",
    "    ]\n",
    "\n",
    "    add_documents_to_milvus(docs)\n",
    "    \"\"\"\n",
    "    # Initialize the database\n",
    "    vectorstore = init_milvus_db(collection_name, uri, embedding_function)\n",
    "\n",
    "    # Prepare documents\n",
    "    document_list = []\n",
    "    for doc in docs:\n",
    "        text = doc.get(\"text\", \"\")\n",
    "        metadata = doc.get(\"metadata\", {})\n",
    "        document_list.append(Document(page_content=text, metadata=metadata))\n",
    "    \n",
    "    uuids = [str(uuid4()) for _ in range(len(document_list))]\n",
    "\n",
    "    # Add documents to the vector store\n",
    "    vectorstore.add_documents(document_list, ids=uuids)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "# Function to search for similar documents in Milvus\n",
    "def search_milvus(query: str, vectorstore, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Search for similar documents in the Milvus vector store.\n",
    "\n",
    "    Args:\n",
    "        query (str): Text to search for.\n",
    "        vectorstore: Milvus vector store.\n",
    "        top_k (int): Number of similar documents to return.\n",
    "    \"\"\"\n",
    "    return vectorstore.similarity_search_with_score_by_vector(query, k=top_k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Text Chunks → Element Instances\n",
    "**Objective**  \n",
    "Given the chunk, the goal is to extract:  \n",
    "- **Nodes**: Entities, including name, type, description, etc.  \n",
    "- **Edges (Relationships)**: Connections between two or more mentioned entities, along with a description of the relationship type.\n",
    "\n",
    "**Technique**  \n",
    "A \"multipart\" prompt (structured in multiple parts) is provided to the LLM to extract both entities (node instances) and relationships (edge instances) in a single response. The prompt produces a list of delimited tuples, enumerating the detected entities and relationships.\n",
    "\n",
    "Few-shot examples specific to the target domain (e.g., medical, legal, scientific) can be supplied to the LLM to guide its focus and improve extraction quality.\n",
    "\n",
    "**Covariates**  \n",
    "If additional information needs to be associated with an entity (e.g., start/end dates, claims, source text), a secondary extraction prompt is used. This approach enriches each node not only with \"name and description\" but also with supplemental metadata (claims, time periods, etc.).\n",
    "\n",
    "**Gleanings (Iterative Extractions)**  \n",
    "To address potential omissions during a single extraction pass, multiple rounds of extraction can be performed (up to a specified limit).  \n",
    "- In each round, the LLM checks for any missing entities.  \n",
    "- If entities are found to be missing, an \"additional\" extraction is triggered, using a logit bias that forces the model to respond with a yes/no prompt.  \n",
    "- If the response indicates missing entities, a prompt is issued to re-extract them.\n",
    "\n",
    "This iterative approach allows the use of larger chunks without sacrificing quality, as successive passes help \"recover\" any entities initially skipped.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.2. Text Chunks → Element Instances\n",
    "##################################################\n",
    "\n",
    "def extract_element_instances_from_chunk(\n",
    "    chunk_text: str,\n",
    "    gleaning_rounds: int = 1,\n",
    "    USE_OPENAI: bool = True,\n",
    "    local_llm: str = \"llama3.1\"\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    (Step 2.2) Use an LLM prompt to identify entity references, relationships, and covariates.\n",
    "    - Identifies entities (name, type, description) and relationships.\n",
    "    - Supports multiple rounds of \"gleanings\" to find any missed entities.\n",
    "    \"\"\"\n",
    "    extracted_elements = []\n",
    "    not_parsed = []\n",
    "\n",
    "    # Select the LLM to use\n",
    "    if USE_OPENAI:\n",
    "        llm = ChatOpenAI(model=openai_model, temperature=0.0)\n",
    "    else:\n",
    "        llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    # Base prompt for extracting entities and relationships\n",
    "    base_prompt = (\n",
    "        \"Extract entities and relationships from the following text. \"\n",
    "        \"For each entity, provide its name, type, and description. \"\n",
    "        \"For each relationship, provide the source entity, target entity, and description. \"\n",
    "        \"Text: \\n{chunk_text}\\n\"\n",
    "        \"Output format: List of dictionaries with keys 'entity_name', 'entity_type', 'entity_description', 'relationship'. \"\n",
    "        \"Follow this format in the example: [{{\\\"entity_name\\\": \\\"Alice\\\", \\\"entity_type\\\": \\\"Person\\\", \\\"entity_description\\\": \\\"A person of interest.\\\", \\\"relationship\\\": {{\\\"source_entity\\\": \\\"Alice\\\", \\\"target_entity\\\": \\\"Bob\\\", \\\"description\\\": \\\"Knows\\\"}}}}]\"\n",
    "        \"Return just the list, so that we can parse it.\"\n",
    "        \"No bullet list or asterisks needed.\"\n",
    "        \"It must be a unique list, do NOT separate entities and relationships in different lists.\"\n",
    "    )\n",
    "\n",
    "    # Loop through gleaning rounds\n",
    "    for round_num in range(gleaning_rounds):\n",
    "        prompt = base_prompt.format(chunk_text=chunk_text)\n",
    "\n",
    "        # Send prompt to the selected LLM\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        # Parse the LLM response\n",
    "        new_elements = response.content\n",
    "        new_elements = new_elements.replace('```json','').replace('```','')\n",
    "\n",
    "        # Assume the response is already in JSON format\n",
    "        try:\n",
    "            new_elements = eval(new_elements)  # Convert string to list of dicts\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing LLM output: {e}\")\n",
    "            not_parsed.extend(new_elements)\n",
    "            new_elements = []\n",
    "\n",
    "        # Add new elements to the result\n",
    "        extracted_elements.extend(new_elements)\n",
    "\n",
    "        # Check if gleaning is needed (e.g., ask LLM if entities were missed)\n",
    "        if round_num < gleaning_rounds - 1:\n",
    "            print(\"Asking for validation...\")\n",
    "            validation_prompt = (\n",
    "                \"Were any entities or relationships missed in the previous extraction? \"\n",
    "                \"Answer 'Yes' or 'No'.\"\n",
    "            )\n",
    "            validation_response = llm.invoke(\n",
    "                [ HumanMessage(content=validation_prompt)], \n",
    "                chat_history=\n",
    "                [HumanMessage(content=prompt), SystemMessage(content=new_elements)]\n",
    "            )\n",
    "\n",
    "            # If LLM says 'No', break early\n",
    "            if 'No' in validation_response.content:\n",
    "                break\n",
    "\n",
    "    # Return all extracted elements\n",
    "    return extracted_elements, not_parsed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Element Instances → Element Summaries\n",
    "\n",
    "**Objective**  \n",
    "The aim is to transition from the level of \"entity and relationship instances\" to more compact summaries describing each element. Essentially, this involves a second \"re-synthesis\":  \n",
    "- For each entity, all descriptions from the original chunks are merged.  \n",
    "- For each relationship, the same applies: if multiple chunks or extraction passes describe the same relationship, the information is aggregated into a single summary.\n",
    "\n",
    "**Key Aspect: The LLM as Summarizer**  \n",
    "The LLM not only performs mechanical extraction but can also abstract information and capture implicit relationships. This step effectively represents a form of \"abstractive summarization.\"\n",
    "\n",
    "**Potential Duplication**  \n",
    "If the LLM extracts the same entity with different names or formats across chunks, there is a potential risk of duplicates. However, the following section (community detection) can merge and consolidate highly similar entities if they share relationships with a \"core\" set of other entities.\n",
    "\n",
    "**Advantage**  \n",
    "Having rich textual descriptions in the nodes (instead of just RDF triples in subject-predicate-object format) facilitates a \"global query\" approach using the LLM, as the text is not constrained to rigid formats.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.3 ELEMENT INSTANCES → ELEMENT SUMMARIES\n",
    "##################################################\n",
    "\n",
    "summarization_instances_prompt = \"\"\"\n",
    "You are an expert in text summarization and knowledge graph construction. I will provide you with:\n",
    "\n",
    "1. A list of initial nodes, each containing:\n",
    "   - A unique identifier\n",
    "   - A textual description extracted from the source\n",
    "   - Additional properties (optional)\n",
    "\n",
    "2. A list of relationships between these nodes (e.g., an entity \"Einstein\" related to another entity \"Relativity\").\n",
    "\n",
    "Your task is to:\n",
    "- Identify when multiple nodes actually refer to the same entity or concept.\n",
    "- Generate *summarized nodes* by consolidating their textual descriptions and removing duplicates or near-duplicates.\n",
    "- Maintain references to each node's original ID within your summarized node.\n",
    "- Create new relationships among these summarized nodes that reflect the original relationships, but merged and simplified where appropriate.\n",
    "\n",
    "**Important requirements and format details:**\n",
    "1. Each summarized node should have:\n",
    "   - A `summary` field with the merged description.\n",
    "   - A list of `original_ids` that were merged into this new summary node.\n",
    "   - Any relevant `type` or `label` (e.g., Person, Theory, Location) if it can be inferred from the text.\n",
    "   - (Optional) A short list of `keywords` extracted from the descriptions.\n",
    "\n",
    "2. Each relationship should:\n",
    "   - Include `source` and `target` references to the new summarized nodes.\n",
    "   - Provide a `relation_type` (e.g., \"INVENTED\", \"WORKS_ON\", \"LOCATED_IN\", etc.).\n",
    "   - Have a `weight` or `relevance_score` if it can be inferred (e.g., frequency or importance).\n",
    "   - (Optional) Include an `original_relationships` list indicating which original relationships were merged.\n",
    "\n",
    "3. Return the final data in **JSON** format, containing two top-level keys: `summarized_nodes` and `summarized_relationships`.\n",
    "\n",
    "4. Be concise but ensure the summaries and relationships accurately capture the original meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Here is the initial Nodes and Relationships**:\n",
    "\n",
    "{initial_data}\n",
    "\n",
    "---\n",
    "\n",
    "### **Instructions to the LLM**:\n",
    "1. **Identify duplicates or near-duplicates** (e.g., \"Albert Einstein\" and \"Einstein\" might refer to the same entity).\n",
    "2. **Create a new summarized node** that merges the descriptions of \"N1\" and \"N2\" if they represent the same entity (in this case, Albert Einstein).\n",
    "3. **Consolidate relationships** so that if multiple original relationships lead to the same concept, you unify them into a single relationship with an updated weight (e.g., sum or average of the original).\n",
    "4. Provide your final answer in the following **JSON** structure:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"summarized_nodes\": [\n",
    "    {{\n",
    "      \"title\": \"NewTitle1\",\n",
    "      \"summary\": \"Your merged summary text here...\",\n",
    "      \"original_ids\": [\"ExampleID1\", \"ExampleID2\", ...],\n",
    "      \"type\": \"Person\",\n",
    "      \"keywords\": [\"Einstein\", \"relativity\", \"physics\"]\n",
    "    }},\n",
    "    {{\n",
    "      \"title\": \"NewTitle2\",\n",
    "      \"summary\": \"Your summary text here...\",\n",
    "      \"original_ids\": [\"ExampleID3\", \"ExampleID4\"],\n",
    "      \"type\": \"Theory\",\n",
    "      \"keywords\": [\"relativity\", \"physics\"]\n",
    "    }}\n",
    "  ],\n",
    "  \"summarized_relationships\": [\n",
    "    {{\n",
    "      \"source\": \"NewTitle1\",\n",
    "      \"target\": \"NewTitle2\",\n",
    "      \"relation_type\": \"DEVELOPED_OR_ASSOCIATED_WITH\",\n",
    "      \"weight\": 5,\n",
    "      \"original_relationships\": [\"N1->N3(DEVELOPED)\", \"N2->N3(ASSOCIATED_WITH)\"]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "\n",
    "Please **only** output valid JSON in the format described above, without additional commentary so that we can parse it correctly. \n",
    "Make sure to capture the essence of each original node and relationship in your summarized version.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_element_instances(\n",
    "    element_instances: List[Dict[str, Any]], \n",
    "    USE_OPENAI: bool = True,\n",
    "    local_llm: str = \"llama3.1\") -> str:\n",
    "    \"\"\"\n",
    "    (Step 2.3) Summarize extracted nodes/relationships into a single descriptive block of text\n",
    "    for each chunk. This is an additional LLM-based summarization step, forming \"element summaries.\"\n",
    "    \"\"\"\n",
    "    # Select the LLM to use\n",
    "    if USE_OPENAI:\n",
    "        llm = ChatOpenAI(model=openai_model, temperature=0.0)\n",
    "    else:\n",
    "        llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    prompt = summarization_instances_prompt.format(initial_data=element_instances)\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    response = response.content\n",
    "\n",
    "    try:\n",
    "        response = response.replace('```json','').replace('```','')\n",
    "        response = eval(response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing LLM output: {e}\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_element_summary_in_graph(tx, data: Dict[str, Any], doc_id: str, chunks_bounds: tuple):\n",
    "    \"\"\"\n",
    "    Load the summarized graph data into Neo4j.\n",
    "\n",
    "    Args:\n",
    "        tx: Neo4j transaction object.\n",
    "        data (Dict[str, Any]): Summarized nodes and relationships.\n",
    "        doc_id (str): Document ID.\n",
    "        chunks_bounds (tuple): Tuple containing the start and end positions\n",
    "            of the text chunk in the original document.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creazione dei nodi\n",
    "    for node in data[\"summarized_nodes\"]:\n",
    "        query_create_node = \"\"\"\n",
    "        CREATE (n:SummarizedNode {\n",
    "            title: $title,\n",
    "            summary: $summary,\n",
    "            original_ids: $original_ids,\n",
    "            type: $type,\n",
    "            keywords: $keywords,\n",
    "            doc_id : $doc_id,\n",
    "            chunks_lower_bound: $chunks_lower_bound,\n",
    "            chunks_upper_bound: $chunks_upper_bound\n",
    "        })\n",
    "        \"\"\"\n",
    "        tx.run(\n",
    "            query_create_node,\n",
    "            title=node.get(\"title\"),\n",
    "            summary=node.get(\"summary\"),\n",
    "            original_ids=node.get(\"original_ids\"),\n",
    "            type=node.get(\"type\"),\n",
    "            keywords=node.get(\"keywords\"),\n",
    "            doc_id=doc_id,\n",
    "            chunks_lower_bound=chunks_bounds[0],\n",
    "            chunks_upper_bound=chunks_bounds[1]\n",
    "        )\n",
    "\n",
    "    # Creazione delle relazioni\n",
    "    for rel in data[\"summarized_relationships\"]:\n",
    "        query_create_rel = f\"\"\"\n",
    "        MATCH (source:SummarizedNode {{title: $source_id}})\n",
    "        MATCH (target:SummarizedNode {{title: $target_id}})\n",
    "        CREATE (source)-[:RELATIONSHIP_TYPE {{\n",
    "            type : $relation_type,\n",
    "            weight: $weight,\n",
    "            original_relationships: $original_rels\n",
    "        }}]->(target)\n",
    "        \"\"\"\n",
    "        tx.run(\n",
    "            query_create_rel,\n",
    "            source_id=rel[\"source\"],\n",
    "            target_id=rel[\"target\"],\n",
    "            relation_type=rel[\"relation_type\"],\n",
    "            weight=rel.get(\"weight\", 1),  # default=1 if not provided\n",
    "            original_rels=rel.get(\"original_relationships\", [])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Element Summaries → Graph Communities\n",
    "\n",
    "**Graph Construction**  \n",
    "At this stage, we build a homogeneous, undirected graph with:  \n",
    "- **Nodes** = Entities (with associated claims, if available)  \n",
    "- **Edges** = Relationships (with weights representing, for instance, how often a specific relationship is observed across different texts)  \n",
    "\n",
    "**Community Detection**  \n",
    "A community detection algorithm (specifically Leiden, Traag et al., 2019) is applied to partition the nodes into hierarchical groups (“communities”). A community groups nodes that are more strongly connected to each other than to the rest of the graph.  \n",
    "\n",
    "**Key Feature**  \n",
    "Leiden is highly efficient for large-scale graphs and supports hierarchical partitioning, allowing multiple levels of clustering.  \n",
    "\n",
    "At each “level” of this hierarchy, different granularities of grouping are produced—from broad macro-communities to more specific micro-communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.4 ELEMENT SUMMARIES → GRAPH COMMUNITIES\n",
    "##################################################\n",
    "\n",
    "class CommunityDetection:\n",
    "    def __init__(self, driver: Any = None, uri: str = '', user: str = '', password: str = ''):\n",
    "        \"\"\"\n",
    "        Initialize the CommunityDetection class with a Neo4j driver or connection details.\n",
    "\n",
    "        Args:\n",
    "            driver (Any): An existing Neo4j driver instance. If provided, `uri`, `user`, and `password` are ignored.\n",
    "            uri (str): The URI for the Neo4j database.\n",
    "            user (str): The username for the Neo4j database.\n",
    "            password (str): The password for the Neo4j database.\n",
    "        \"\"\"\n",
    "        # If a driver is provided, use it; otherwise, create a new driver instance\n",
    "        self.driver = driver or GraphDatabase.driver(uri, auth=(user, password))\n",
    "        self.graph_name = 'summarizedGraph'\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def project_graph(self, relationship_weight_property: str = \"weight\") -> None:\n",
    "        \"\"\"\n",
    "        Projects the graph into memory for analysis.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(\n",
    "                f\"\"\"\n",
    "                CALL gds.graph.project(\n",
    "                    '{self.graph_name}',\n",
    "                    'SummarizedNode',\n",
    "                    {{ RELATIONSHIP_TYPE: {{ orientation: 'UNDIRECTED', properties: ['{relationship_weight_property}'] }} }}\n",
    "                )\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "\n",
    "    def set_communities(self, relationship_weight_property: str = \"weight\") -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Sets the community IDs directly into the graph database.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\n",
    "                f\"\"\"\n",
    "                CALL gds.leiden.stream(\n",
    "                    '{self.graph_name}', \n",
    "                    {{ relationshipWeightProperty: '{relationship_weight_property}' }})\n",
    "                YIELD nodeId, communityId\n",
    "                SET gds.util.asNode(nodeId).communityId = communityId\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    def retrieve_communities(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve the community assignments from the graph.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\n",
    "                f\"\"\"\n",
    "                MATCH (n:SummarizedNode)-[r:RELATIONSHIP_TYPE]->(m:SummarizedNode)\n",
    "                RETURN\n",
    "                    n.communityId AS communityId,\n",
    "                    n.title AS nodeTitle,      \n",
    "                    n.summary AS nodeSummary,\n",
    "                    n.keywords AS nodeKeywords,\n",
    "                    r.type AS relationshipType,\n",
    "                    r.weight AS relationshipWeight,\n",
    "                    m.title AS targetNodeTitle\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "            communities = {}\n",
    "            for record in result:\n",
    "                community_id = record[\"communityId\"]\n",
    "                node_title = record[\"nodeTitle\"]\n",
    "                node_summary = record[\"nodeSummary\"]\n",
    "                node_keywords = record[\"nodeKeywords\"]\n",
    "                relationship_type = record[\"relationshipType\"]\n",
    "                relationship_weight = record[\"relationshipWeight\"]\n",
    "                target_node_title = record[\"targetNodeTitle\"]\n",
    "                \n",
    "                if community_id not in communities:\n",
    "                    communities[community_id] = {\"nodes\": [], \"relationships\": []}\n",
    "                \n",
    "                communities[community_id][\"nodes\"].append({\n",
    "                    \"title\": node_title,\n",
    "                    \"summary\": node_summary,\n",
    "                    \"keywords\": node_keywords\n",
    "                })\n",
    "                \n",
    "                communities[community_id][\"relationships\"].append({\n",
    "                    \"source\": node_title,\n",
    "                    \"target\": target_node_title,\n",
    "                    \"type\": relationship_type,\n",
    "                    \"weight\": relationship_weight\n",
    "                })\n",
    "\n",
    "            return communities\n",
    "\n",
    "    def drop_graph(self) -> None:\n",
    "        \"\"\"\n",
    "        Drops the graph from memory.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(f\"CALL gds.graph.drop('{self.graph_name}') YIELD graphName\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Graph Communities → Community Summaries\n",
    "\n",
    "**Objective**  \n",
    "The goal is to create textual reports or summaries for each identified community. This approach allows users (or systems) to navigate the overall structure of the graph:  \n",
    "- At a high level, users can view macro-themes (top-level communities).  \n",
    "- If something is of interest, they can drill down to more granular “child” communities.  \n",
    "\n",
    "**Utility**  \n",
    "Even in the absence of a specific query, community summaries help to \"understand\" the overall content of a textual corpus, functioning like a conceptual map.  \n",
    "When a global query is posed, these summaries serve as an index for extracting the final answer.  \n",
    "\n",
    "**Scalability**  \n",
    "For very large datasets, multi-level hierarchical summaries provide a scalable method for sensemaking.\n",
    "\n",
    "**Vectorial Search**    \n",
    "This step moreover gives the possibility to embed this new summaries and perform a custom retrieval like in a normal RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.5 GRAPH COMMUNITIES → COMMUNITY SUMMARIES\n",
    "##################################################\n",
    "\n",
    "summary_community_prompt = \"\"\"\n",
    "You are an expert summarizer helping to create a concise “community report” from a list of related nodes. Each node has a title, a summary, and keywords. All these nodes belong to the same community, meaning they share a common theme, topic, or set of closely related ideas. Moreover, you are provided with a list of relationships between these nodes, indicating how they are connected or interact with each other.\n",
    "\n",
    "Here is the list of nodes for this community:\n",
    "\n",
    "{nodes}\n",
    "\n",
    "And here are the relationships between these nodes:\n",
    "\n",
    "{relationships}\n",
    "\n",
    "Please read through the nodes and relationships and then produce a coherent summary describing:\n",
    "1. The main topics, themes, or domains covered by the nodes in this community.\n",
    "2. Any notable or central nodes and why they are important.\n",
    "3. How the nodes interrelate: highlight significant relationships and mention if there are strong connections (high weight).\n",
    "4. Overall, what makes this community distinct or interesting?\n",
    "\n",
    "- Aim for a concise yet informative text, written in a clear paragraph style.\n",
    "- You may group related nodes together and mention prominent links or patterns.\n",
    "- Use plain English. Avoid overly technical language unless it is necessary to describe the domain.\n",
    "\n",
    "4. Provide your final answer in the following **JSON** structure:\n",
    "\n",
    "```json\n",
    "{{\n",
    "    \"title\": a title capable to summarize the community,\n",
    "    \"community_summary\": a single, cohesive summary that helps a reader quickly understand the core content of this community. You do NOT need to repeat every node’s individual summary verbatim. Instead, synthesize the most relevant information into a unified overview.\n",
    "    \"keywords\": a list of keywords that capture the main topics or themes of this community.\n",
    "}}\n",
    "```\n",
    "\n",
    "Please **only** output valid JSON in the format described above, without additional commentary so that we can parse it correctly. \n",
    "\n",
    "Begin now.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_communities(\n",
    "    communities: Dict[int, List[Dict[str, Any]]],\n",
    "    USE_OPENAI: bool = True,\n",
    "    local_llm: str = \"llama3.1\",\n",
    ") -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    (Step 2.5) Summarize each community (or sub-community in a hierarchical approach).\n",
    "    - Gather all element summaries (nodes, edges, covariates) in that community.\n",
    "    - Summarize them, potentially chunking if they don't fit in an LLM context window.\n",
    "\n",
    "    Args:\n",
    "        communities (dict): A dictionary where keys are community IDs and values are lists of nodes with 'title', 'summary', and 'keywords' and a list of relationships with 'source', 'target', 'type', and 'weight'.\n",
    "        USE_OPENAI (bool): Whether to use OpenAI or a local LLM.\n",
    "        local_llm (str): The name of the local LLM model.\n",
    "\n",
    "    Returns:\n",
    "        dict: Summaries for each community.\n",
    "    \"\"\"\n",
    "    # Select the LLM to use\n",
    "    if USE_OPENAI:\n",
    "        llm = ChatOpenAI(model=openai_model, temperature=0.0)\n",
    "    else:\n",
    "        llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    def process_community(community_id, data):\n",
    "        try:\n",
    "            # Extract nodes and relationships\n",
    "            nodes = data[\"nodes\"]\n",
    "            relationships = data[\"relationships\"]\n",
    "\n",
    "            # Prepare prompt content\n",
    "            node_descriptions = \"\\n\".join([f\"Title: {node['title']}, Summary: {node['summary']}, Keywords: {', '.join(node['keywords'])}\" for node in nodes])\n",
    "            relationships = \"\\n\".join([f\"Source: {rel['source']}, Target: {rel['target']}, Type: {rel['type']}, Weight: {rel['weight']}\" for rel in relationships])\n",
    "            prompt = summary_community_prompt.format(nodes=node_descriptions, relationships=relationships)\n",
    "\n",
    "            # Generate summary\n",
    "            response = llm.invoke([HumanMessage(content=prompt)])\n",
    "            response = response.content.replace('```json','').replace('```','')\n",
    "\n",
    "            return community_id, eval(response)\n",
    "        except Exception as e:\n",
    "            return community_id, [f\"Error generating summary: {str(e)}\", response]\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    summaries = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Prepare data for processing\n",
    "        futures = {executor.submit(process_community, community_id, data): community_id for community_id, data in communities.items()}\n",
    "\n",
    "        # Process results as they complete\n",
    "        for future in tqdm.tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            community_id, result = future.result()\n",
    "            summaries[community_id] = result\n",
    "\n",
    "\n",
    "    return summaries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Community Summaries → Community Answers → Global Answer\n",
    "\n",
    "**Scenario**  \n",
    "When a user poses a broad question, it is necessary to treat the community summaries as a knowledge base.  \n",
    "\n",
    "**Multi-Stage Procedure**  \n",
    "\n",
    "1. **Preparation**  \n",
    "   - Community summaries (potentially at a specific hierarchical level) are segmented into predefined-sized chunks to avoid exceeding the LLM’s input context limits.  \n",
    "   - Chunks are shuffled randomly to prevent relevant information from being concentrated in a single block, minimizing the risk of being “lost.”  \n",
    "\n",
    "2. **Generation of Intermediate Answers**  \n",
    "   - For each chunk, the LLM generates both a local response to the query and a utility score (from 0 to 100) indicating how useful that response is for the question.  \n",
    "   - Responses with a score of 0 are discarded.  \n",
    "\n",
    "3. **Reduction to a Global Answer**  \n",
    "   - Responses are ranked by utility score, from highest to lowest.  \n",
    "   - The content of the top responses is concatenated iteratively into a final context until the token limit is reached.  \n",
    "   - The LLM is then tasked with generating the global answer using this consolidated context.  \n",
    "\n",
    "**Advantage**  \n",
    "The global answer does not rely on a single prompt with limited context. Instead, it integrates information from multiple chunks (across communities or sub-communities). This method also incorporates a filtering mechanism based on the perceived utility of information, enabling iterative refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# MAIN FUNCTION: 2.6 COMMUNITY SUMMARIES → COMMUNITY ANSWERS → GLOBAL ANSWER\n",
    "###################################################\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# UTILITY: Prompt to get partial answer + helpfulness score\n",
    "# -------------------------------------------------------------------------\n",
    "PARTIAL_ANSWER_PROMPT = \"\"\"\\\n",
    "You have a user query:\n",
    "\\\"\\\"\\\"{user_query}\\\"\\\"\\\"\n",
    "\n",
    "Below is a chunk of text from a community summary that may or may not be relevant to the query:\n",
    "\\\"\\\"\\\"{chunk_text}\\\"\\\"\\\"\n",
    "\n",
    "1) Please provide a concise partial answer (if any) relevant to the query, based on the chunk above.\n",
    "   If the chunk is irrelevant, you can say \"No relevant info here.\"\n",
    "2) Provide a helpfulness score from 0 to 100 (integer), indicating how much this chunk helps answer the query. \n",
    "   0 = not relevant at all, 100 = extremely helpful.\n",
    "\n",
    "Output your response **only** in valid JSON format like:\n",
    "{{\n",
    "  \"partial_answer\": \"...\",\n",
    "  \"helpfulness_score\": ...\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_from_communities(\n",
    "    user_query: str,\n",
    "    community_summaries: Dict[int, str],\n",
    "    USE_OPENAI: bool = True,\n",
    "    local_llm: str = \"llama3.1\",\n",
    "    max_context_tokens: int = 1000\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    (Step 2.6) Use the hierarchical community summaries to answer a user query globally.\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): The question asked by the user.\n",
    "        community_summaries (Dict[int, str]): A dict of {community_id: summary_text}.\n",
    "        USE_OPENAI (bool): Whether to use OpenAI or a local LLM.\n",
    "        local_llm (str): The local LLM name if not using OpenAI.\n",
    "        max_context_tokens (int): Approx token limit for each chunk.\n",
    "\n",
    "    Returns:\n",
    "        str: The final global answer.\n",
    "    \n",
    "    High-level process:\n",
    "      1) For each community summary:\n",
    "         - Chunk the text (so we don't exceed context window).\n",
    "      2) For each chunk:\n",
    "         - Ask the LLM for a partial answer + helpfulness score.\n",
    "      3) Sort partial answers by score.\n",
    "      4) Combine top partial answers into a final context.\n",
    "      5) Ask the LLM for a final answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # 0) Select which LLM to use\n",
    "    # -------------------------\n",
    "    if USE_OPENAI:\n",
    "        llm = ChatOpenAI(model=openai_model, temperature=0.0)\n",
    "    else:\n",
    "        llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # 1) Chunk each community summary\n",
    "    # ----------------------------------------------\n",
    "    chunked_texts = []\n",
    "    for comm_id, summary_text in community_summaries.items():\n",
    "        # Break down the summary into smaller pieces\n",
    "        chunks = chunk_text(summary_text, chunk_size=max_context_tokens)\n",
    "        for c in chunks:\n",
    "            chunked_texts.append((comm_id, c))\n",
    "\n",
    "   \n",
    "    # ----------------------------------------------\n",
    "    # 2a) For each chunk, get partial answer + score\n",
    "    # ----------------------------------------------\n",
    "    # Function to process a single chunk\n",
    "    def process_chunk(comm_id, text_chunk, user_query):\n",
    "        try:\n",
    "            # Build the prompt\n",
    "            prompt = PARTIAL_ANSWER_PROMPT.format(\n",
    "                user_query=user_query,\n",
    "                chunk_text=text_chunk\n",
    "            )\n",
    "\n",
    "            # LLM call\n",
    "            response = llm.invoke([HumanMessage(content=prompt)])\n",
    "            raw_content = response.content.strip()\n",
    "\n",
    "            # Attempt to parse JSON response\n",
    "            first_brace = raw_content.find('{')\n",
    "            last_brace = raw_content.rfind('}')\n",
    "            json_str = raw_content[first_brace:last_brace+1]\n",
    "\n",
    "            parsed = eval(json_str)  # or use json.loads if strictly valid\n",
    "            partial_answer = parsed.get(\"partial_answer\", \"No relevant info here.\")\n",
    "            score = parsed.get(\"helpfulness_score\", 0)\n",
    "\n",
    "        except Exception as e:\n",
    "            partial_answer = \"Parsing error or no relevant info.\"\n",
    "            score = 0\n",
    "\n",
    "        # Return the result for this chunk\n",
    "        return (comm_id, partial_answer, score)\n",
    "\n",
    "    # Parallel processing\n",
    "    partial_answers = []\n",
    "    marks = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submit tasks for each chunk\n",
    "        futures = [executor.submit(process_chunk, comm_id, text_chunk, user_query) for comm_id, text_chunk in chunked_texts]\n",
    "\n",
    "        # Collect results as they complete\n",
    "        print(\"Processing partial answers...\")\n",
    "        for future in tqdm.tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            comm_id, partial_answer, score = future.result()\n",
    "            partial_answers.append((partial_answer, score))\n",
    "            marks.append((comm_id, score))\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # 3) Sort partial answers by score (descending)\n",
    "    # ----------------------------------------------\n",
    "    partial_answers.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # ----------------------------------------------\n",
    "    # 4) Combine top partial answers into a final context\n",
    "    #    We'll do a simple cutoff if we have too many\n",
    "    # ----------------------------------------------\n",
    "    final_context = []\n",
    "    used_chars = 0\n",
    "\n",
    "    for ans, sc in partial_answers:\n",
    "        # Add some label or delimiter if needed\n",
    "        if sc==0:\n",
    "            continue\n",
    "        \n",
    "        snippet = f\"PartialAnswer(Score={sc}): {ans}\\n\"\n",
    "        if used_chars + len(snippet) <= max_context_tokens * 4:\n",
    "            final_context.append(snippet)\n",
    "            used_chars += len(snippet)\n",
    "        else:\n",
    "            break  # no more space in final context\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # 5) Produce a final answer from the LLM\n",
    "    # ----------------------------------------------\n",
    "    final_prompt = f\"\"\"\\\n",
    "We have these partial answers and their helpfulness scores:\n",
    "\n",
    "{''.join(final_context)}\n",
    "\n",
    "User Query: {user_query}\n",
    "\n",
    "Based on these partial answers, please produce a single, coherent, and well-structured final answer.\n",
    "Feel free to synthesize or refine the information. If there's conflicting info, do your best to clarify.\n",
    "\n",
    "Respond in plain text.\n",
    "\"\"\"\n",
    "\n",
    "    # Final LLM call\n",
    "    print(\"Processing final answer...\")\n",
    "    final_response = llm.invoke([HumanMessage(content=final_prompt)])\n",
    "    global_answer = final_response.content.strip()\n",
    "\n",
    "    return global_answer, partial_answers, marks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 5) INGEST PDF -> STORE IN GRAPH (Putting Steps 2.1 and 2.2+ in context)\n",
    "##################################################\n",
    "\n",
    "def ingest_pdf_into_graph(pdf_path: str, doc_id: str, embed_opt: bool = True, BATCH_SIZE: int = 25):\n",
    "    \"\"\"\n",
    "    1) Parse PDF into raw text.\n",
    "    2) Chunk it (Step 2.1).\n",
    "    3) Generate embeddings for each chunk.\n",
    "    4) Store chunk nodes in Neo4j.\n",
    "    5) For each chunk, call LLM to extract element instances (Step 2.2).\n",
    "    6) Summarize them into a single descriptive block (Step 2.3).\n",
    "    7) Store the block in Neo4j for further community detection.\n",
    "    \"\"\"\n",
    "    # Step 1: Parse PDF\n",
    "    print(f\"Parsing PDF at {pdf_path}...\")\n",
    "    raw_text = parse_pdf(pdf_path)\n",
    "    print(f\"Extracted {len(raw_text)} characters from {pdf_path} \\n\\n\")\n",
    "\n",
    "    # Step 1.1: Retrieve metadata\n",
    "    print(f\"Retrieving metadata for {doc_id}...\")\n",
    "    metadata = find_metadata(doc_id)\n",
    "    print(f\"Metadata: {metadata}\\n\\n\")\n",
    "\n",
    "    # Step 2: Chunk the text (default chunk_size=600 for improved recall)\n",
    "    chunks = chunk_text(raw_text)\n",
    "    print(f\"Chunked {len(chunks)} segments from {pdf_path} \\n\\n\")\n",
    "\n",
    "    # Step 3: Embeddings\n",
    "    if embed_opt:\n",
    "        print(\"Generating embeddings for each chunk...\")\n",
    "        embed_fn = get_hf_embedding_function(USE_OPENAI=False)\n",
    "\n",
    "        print(\"Storing chunks in Milvus...\")\n",
    "        vectorstore = add_documents_to_milvus([\n",
    "            {\n",
    "                \"text\": chunk, \n",
    "                \"metadata\": {\n",
    "                    \"doc_id\": doc_id, \n",
    "                    \"chunk\": i, \n",
    "                    **metadata\n",
    "                    }\n",
    "            } for i, chunk in enumerate(chunks)], embed_fn)\n",
    "        print(f\"Stored {len(chunks)} chunks in Milvus under Document {doc_id} \\n\\n\")\n",
    "\n",
    "\n",
    "    # Step 4: Store chunk nodes in Neo4j\n",
    "    # Process chunks in parallel with batch size of 25\n",
    "    for lim in tqdm.tqdm(range(0, len(chunks), BATCH_SIZE), desc=\"Processing chunks in parallel\"):\n",
    "        extracted_elements = []\n",
    "        not_parsed_elements = []\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            # Submit tasks\n",
    "            futures = [\n",
    "                executor.submit(extract_element_instances_from_chunk, chunk_text_str, USE_OPENAI=USE_OPENAI)\n",
    "                for i, chunk_text_str in enumerate(chunks[lim:lim+BATCH_SIZE])\n",
    "            ]\n",
    "            \n",
    "            # Process results as they complete\n",
    "            for future in tqdm.tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing chunks\"):\n",
    "                elements_instance, not_parsed_instance = future.result()\n",
    "                extracted_elements.extend(elements_instance)\n",
    "                not_parsed_elements.extend(not_parsed_instance)\n",
    "\n",
    "        print(\"Parallel processing completed.\")\n",
    "            \n",
    "        # Step 6: Summarize them (element-level)\n",
    "        print(\"Summarizing element instances...\")\n",
    "        element_summary = summarize_element_instances(extracted_elements, USE_OPENAI=USE_OPENAI)\n",
    "\n",
    "        print(f\"Summarized {len(element_summary['summarized_nodes'])} nodes and {len(element_summary['summarized_relationships'])} relationships\\n\\n\")\n",
    "\n",
    "        print(\"Storing backup files...\")\n",
    "        # Backup element_summary, extracted_elements and not_parsed_elements\n",
    "        os.makedirs('backup_extraction_nodes/'+doc_id+'/element_summary', exist_ok=True)\n",
    "        os.makedirs('backup_extraction_nodes/'+doc_id+'/extracted_elements', exist_ok=True)\n",
    "        os.makedirs('backup_extraction_nodes/'+doc_id+'/not_parsed_elements', exist_ok=True)\n",
    "\n",
    "        with open(f'backup_extraction_nodes/{doc_id}/element_summary/{lim}.json', 'w') as f:\n",
    "            f.write(str(element_summary))\n",
    "        \n",
    "        with open(f'backup_extraction_nodes/{doc_id}/extracted_elements/{lim}.json', 'w') as f:\n",
    "            f.write(str(extracted_elements))\n",
    "        \n",
    "        with open(f'backup_extraction_nodes/{doc_id}/not_parsed_elements/{lim}.json', 'w') as f:\n",
    "            f.write(str(not_parsed_elements))\n",
    "        \n",
    "        print(\"Backup files stored.\\n\\n\")\n",
    "\n",
    "        # Step 7: Store the summary\n",
    "        print(\"Storing element summary in Neo4j...\")\n",
    "        with driver.session() as session:\n",
    "            # Step 7: Store the summary\n",
    "            session.execute_write(store_element_summary_in_graph, element_summary, doc_id, (lim, lim+BATCH_SIZE))\n",
    "    \n",
    "    print(f\"Ingested {len(chunks)} chunks from {pdf_path} into Neo4j under Document {doc_id}\")\n",
    "    return True\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# MAIN EXECUTION EXAMPLE\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = 'data/docs/'\n",
    "\n",
    "# Retrieve all the docs in PDF_PATH\n",
    "docs=sorted([f for f in os.listdir(PDF_PATH) if f.endswith('.pdf')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_docs=docs[:61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 1208.1583.pdf...\n",
      "Parsing PDF at data/docs/1208.1583.pdf...\n",
      "Extracted 118707 characters from data/docs/1208.1583.pdf \n",
      "\n",
      "\n",
      "Retrieving metadata for 1208.1583...\n",
      "Metadata: {'title': 'Exotic nuclei far from the stability line', 'summary': 'The recent availability of radioactive beams has opened up a new era in\\nnuclear physics. The interactions and structure of exotic nuclei close to the\\ndrip lines have been studied extensively world wide, and it has been revealed\\nthat unstable nuclei, having weakly bound nucleons, exhibit characteristic\\nfeatures such as a halo structure and a soft dipole excitation. We here review\\nthe developments of the physics of unstable nuclei in the past few decades. The\\ntopics discussed in this Chapter include the halo and skin structures, the\\nCoulomb breakup, the dineutron correlation, the pair transfer reactions, the\\ntwo-nucleon radioactivity, the appearance of new magic numbers, and the pygmy\\ndipole resonances.', 'url': 'http://arxiv.org/abs/1208.1583v2', 'authors': 'K. Hagino, I. Tanihata, H. Sagawa', 'categories': 'nucl-th, nucl-ex'}\n",
      "\n",
      "\n",
      "Chunked 245 segments from data/docs/1208.1583.pdf \n",
      "\n",
      "\n",
      "Generating embeddings for each chunk...\n",
      "Storing chunks in Milvus...\n",
      "Stored 245 chunks in Milvus under Document 1208.1583 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [01:45<00:00,  4.20s/it]s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n",
      "Summarized 33 nodes and 31 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [01:28<00:00,  3.55s/it]174.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel:  67%|██████▋   | 2/3 [04:41<02:14, 134.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 6 nodes and 6 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 20/20 [01:04<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel: 100%|██████████| 3/3 [06:28<00:00, 129.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 2 nodes and 1 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n",
      "Ingested 245 chunks from data/docs/1208.1583.pdf into Neo4j under Document 1208.1583\n",
      "Finished processing document 1208.1583.pdf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in filtered_docs:\n",
    "    print(f\"Processing document {doc}...\")\n",
    "    doc_id = doc.replace('.pdf', '')\n",
    "\n",
    "    if ingest_pdf_into_graph(PDF_PATH+doc, doc_id, embed_opt=True):\n",
    "        print(f\"Finished processing document {doc}.\")\n",
    "        continue\n",
    "    else:\n",
    "        raise Exception(f\"Error processing document {doc}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the community detection class\n",
    "detector = CommunityDetection(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Community detection & summarization (Steps 2.4–2.5)\n",
    "\n",
    "# Project the graph for community detection\n",
    "detector.project_graph()\n",
    "\n",
    "# Set community IDs in the graph\n",
    "detector.set_communities()\n",
    "\n",
    "# Drop the graph from memory\n",
    "detector.drop_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 377/377 [01:32<00:00,  4.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the communities\n",
    "communities = detector.retrieve_communities()\n",
    "community_summaries=summarize_communities(communities, USE_OPENAI=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to create the graph plot in the images folder\n",
    "\n",
    "def fetch_data_from_neo4j():\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    query_nodes = \"MATCH (n) RETURN id(n) as id, labels(n) as labels, properties(n) as properties\"\n",
    "    query_relationships = \"MATCH (n)-[r]->(m) RETURN id(n) as source, id(m) as target, properties(r) as properties\"\n",
    "\n",
    "    nodes = []\n",
    "    relationships = []\n",
    "\n",
    "    with driver.session() as session:\n",
    "        # Fetch nodes\n",
    "        result_nodes = session.run(query_nodes)\n",
    "        for record in result_nodes:\n",
    "            properties = dict(record['properties'])  # Explicitly cast to dictionary\n",
    "       \n",
    "            del properties['summary']\n",
    "            del properties['original_ids']\n",
    "\n",
    "            nodes.append({\n",
    "                'id': record['id'],\n",
    "                'labels': record['labels'],\n",
    "                'properties': properties\n",
    "            })\n",
    "\n",
    "        # Fetch relationships\n",
    "        result_relationships = session.run(query_relationships)\n",
    "        for record in result_relationships:\n",
    "            relationships.append({\n",
    "                'source': record['source'],\n",
    "                'target': record['target'],\n",
    "                'properties': dict(record['properties'])  # Explicitly cast to dictionary\n",
    "            })\n",
    "\n",
    "    driver.close()\n",
    "    return nodes, relationships\n",
    "\n",
    "def plot_graph(nodes, relationships):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes with attributes\n",
    "    for node in nodes:\n",
    "        G.add_node(node['id'], label=node['labels'], properties=node['properties'])\n",
    "\n",
    "    # Add edges with attributes\n",
    "    for rel in relationships:\n",
    "        G.add_edge(rel['source'], rel['target'], properties=rel['properties'])\n",
    "\n",
    "    # Extract node attributes\n",
    "    community_ids = [node['properties']['communityId'] for node in nodes]\n",
    "    unique_communities = list(set(community_ids))\n",
    "    color_list = list(mcolors.TABLEAU_COLORS.values()) + list(mcolors.CSS4_COLORS.values())\n",
    "    colors = color_list[:50]  # Limit to 50 colors\n",
    "    color_map = [colors[unique_communities.index(node['properties']['communityId']) % 50] for node in nodes]\n",
    "\n",
    "    # Get positions for nodes\n",
    "    pos = nx.spring_layout(G)\n",
    "\n",
    "    # Create edges\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_text = []\n",
    "    for edge in G.edges(data=True):\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "        edge_text.append(str(edge[2]['properties']))\n",
    "\n",
    "    edge_trace = go.Scatter(x=edge_x, y=edge_y, mode='lines', line=dict(width=0.5, color='#888'), hoverinfo='text', text=edge_text)\n",
    "\n",
    "    # Create nodes\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    node_color = []\n",
    "    node_text = []\n",
    "    for i, node in enumerate(G.nodes(data=True)):\n",
    "        x, y = pos[node[0]]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        node_color.append(color_map[i])\n",
    "        node_text.append(str(node[1]['properties']))\n",
    "\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y, mode='markers',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=node_color,\n",
    "            line_width=0\n",
    "        ),\n",
    "        hoverinfo='text',\n",
    "        text=node_text\n",
    "    )\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                    layout=go.Layout(\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=0, l=0, r=0, t=0),\n",
    "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
    "                    ))\n",
    "    fig.write_html(\"images/graph_plot.html\")\n",
    "\n",
    "# Create a file with the graph plot\n",
    "nodes, relationships = fetch_data_from_neo4j()\n",
    "plot_graph(nodes, relationships)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the nodes and communities created, a screenshot taken from Neo4j Bloom:\n",
    "![Communities](./images/bloom-visualisation.png)\n",
    "\n",
    "The purple nodes are the communities summary to which are linked all the entities. Moreover the entities are connected between each other. We can see in the bottom a huge network linked. This is related to authors and common topics among the docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this query in neo4j to create communities nodes and visualize the relationships with the entities.\n",
    "\n",
    "``` cypher\n",
    "MATCH (n:SummarizedNode)\n",
    "WITH n.communityId AS communityId, count(n) AS nodeCount, collect(n) AS nodes\n",
    "MERGE (c:Community {communityId: communityId}) \n",
    "SET c.size = nodeCount\n",
    "WITH c, nodes  // Add this WITH clause to pass variables forward\n",
    "UNWIND nodes AS node\n",
    "MERGE (node)-[:BELONGS_TO]->(c)\n",
    "RETURN c\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store json file with community summaries\n",
    "os.makedirs('backup_extraction_nodes/', exist_ok=True)\n",
    "with open(f'backup_extraction_nodes/community_summaries.json', 'w') as f:\n",
    "    f.write(str(community_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for the community summaries and store them in Milvus\n",
    "embed_fn = get_hf_embedding_function(USE_OPENAI=False)\n",
    "vectorstore_community_summaries = add_documents_to_milvus([\n",
    "    {\n",
    "        \"text\": summary[\"community_summary\"], \n",
    "        \"metadata\": {\n",
    "            \"doc_id\": \"community_summaries\",\n",
    "            \"community_id\": id,\n",
    "            \"title\": summary[\"title\"],\n",
    "            \"keywords\": \", \".join(summary[\"keywords\"]),\n",
    "            }\n",
    "    } for id, summary in community_summaries.items()], embed_fn, collection_name=\"community_summaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost examples:**\n",
    "\n",
    "0.05$ for a single document of 21 pages with 89 chunks and 10 nodes and 7 relationships extracted. Morevoer this price include the generation of the community summaries of 2 documents.\n",
    "\n",
    "In average a document takes 4 min to be ingested in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Here starts the generation part",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHere starts the generation part\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Here starts the generation part"
     ]
    }
   ],
   "source": [
    "raise Exception(\"Here starts the generation part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle this variable to filter out the communities that are not relevant to the user query through a vector similarity search\n",
    "RETRIEVE_BY_VECTOR = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "user_query = \"What Charm and B Meson are?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all community summaries\n",
    "\n",
    "if RETRIEVE_BY_VECTOR:\n",
    "    # Retrieve embeddings for the user query if needed.\n",
    "    # This allows to filter out irrelevant communities based on the query in advance.\n",
    "    embed_fn = get_hf_embedding_function(USE_OPENAI=False)\n",
    "    vectorstore_community_summaries = init_milvus_db(\"community_summaries\", \"./vector_db_graphRAG/milvus_ingest.db\", embed_fn)\n",
    "    embed=embed_fn.embed_query(user_query)\n",
    "    search=search_milvus(embed, vectorstore_community_summaries, top_k=10)\n",
    "    community_summaries_ingestion = {el[0].metadata['community_id']:el[0].page_content for el in search}\n",
    "else:\n",
    "    # Load the community summaries from the milvus sb\n",
    "    clientMilvus = MilvusClient(\n",
    "        uri=\"./vector_db_graphRAG/milvus_ingest.db\",\n",
    "    )\n",
    "\n",
    "    community_summaries_retrieved = clientMilvus.query(\n",
    "        collection_name=\"community_summaries\",\n",
    "        output_fields=[\"community_id\", \"text\"],\n",
    "        limit = 1000\n",
    "    )\n",
    "\n",
    "    # Refactor for generation\n",
    "    community_summaries_ingestion = {}\n",
    "    for el in community_summaries_retrieved:\n",
    "        community_summaries_ingestion[el['community_id']]=el['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing partial answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing final answer...\n"
     ]
    }
   ],
   "source": [
    "# Generate answers:\n",
    "# Partial answers are the answers generated from the community summaries\n",
    "# Marks are the helpfulness scores of the partial answers for each community summary\n",
    "global_answer, partial_answers, marks  = answer_query_from_communities(user_query, community_summaries_ingestion, USE_OPENAI=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {global_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Sample questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's question: Which are the main themes of the documents?\n",
      "Answer: The main themes of the documents encompass a diverse range of scientific fields and topics. Key areas of focus include:\n",
      "\n",
      "1. **Nuclear Physics**: Research on charge radii and the structure of isotopes, highlighting collaborative efforts between institutions like Argonne National Laboratory (ANL) and GSI.\n",
      "\n",
      "2. **Atomic Interactions**: The dynamics of ion-atom collisions, their impact on energy transfer, and the heating effects that are crucial for understanding atomic behavior.\n",
      "\n",
      "3. **Astrophysics and Astronomy**: Exploration of exoplanets using advanced instruments such as the Gemini Planet Imager, along with challenges in imaging technology and astrometry. Additionally, the study of solar phenomena, including solar flares and their dynamics, is emphasized.\n",
      "\n",
      "4. **Astrochemistry**: Investigation into the chemical processes in space, particularly the roles of water, methane, and carbon dioxide in the formation of stars and planetary systems.\n",
      "\n",
      "5. **Bayesian Statistics**: The application of Bayesian nonparametric models, including the Dirichlet Process and Beta Distribution, focusing on their significance in probabilistic modeling and inference.\n",
      "\n",
      "6. **Machine Learning**: The use of techniques like Latent Dirichlet Allocation (LDA) and its hierarchical variant (hLDA) in various scientific contexts, including the study of neutron star crusts and their thermal properties.\n",
      "\n",
      "7. **Galactic Studies**: Analysis of galaxies, particularly the characteristics and evolution of quiescent versus star-forming galaxies, and the mechanisms that lead to quiescence.\n",
      "\n",
      "8. **Molecular Biology**: Examination of motor proteins involved in cellular transport, the dynamics of muscle contraction, and the interaction between actin filaments and myosin motor proteins.\n",
      "\n",
      "9. **Astrophysical Observations**: Measurement and analysis of beam sizes in observations, with a focus on Full Width at Half Maximum (FWHM) and the significance of molecular transitions in achieving effective angular resolution.\n",
      "\n",
      "10. **Charm Physics**: The study of charm quarks, their mesons, decay processes, and the implications for new physics theories.\n",
      "\n",
      "11. **Biological Dynamics**: The interaction of forces in biological processes, particularly in relation to molecular interactions and the dynamics of DNA replication forks.\n",
      "\n",
      "12. **Computational Systems**: Simulation of two-processor systems, including interactions and decision-making processes within computational frameworks.\n",
      "\n",
      "These themes reflect a rich tapestry of interdisciplinary research, showcasing advancements in understanding both fundamental and applied scientific questions across various domains.\n"
     ]
    }
   ],
   "source": [
    "# generation through gpt-4o-mini \n",
    "# time 31.5 sec\n",
    "# cost 0.03$\n",
    "# partial answers generated on all the communities\n",
    "\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {global_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's question: What Charm and B Meson are?\n",
      "Answer: Charm and B mesons are types of mesons, which are subatomic particles made up of quarks. Specifically, charm mesons contain a charm quark, while B mesons contain a bottom quark. \n",
      "\n",
      "Charm mesons include particles such as D mesons (D0 and D+) and D+ s mesons, which are composed of a charm quark paired with either an up quark or a strange antiquark. The J/ψ meson is another important charm meson, consisting of a charm quark and a charm antiquark. These particles are significant in the study of particle physics, particularly in understanding the interactions and decay processes involving quarks.\n",
      "\n",
      "B mesons, on the other hand, are primarily studied in high-energy physics experiments, such as those conducted at the BABAR, Belle, and CLEO-c collaborations. These experiments focus on the decay processes of B mesons, which can provide insights into the fundamental forces and symmetries in particle physics.\n",
      "\n",
      "Overall, the study of charm and B mesons is crucial for advancing our understanding of the Standard Model of particle physics and the behavior of fundamental particles.\n"
     ]
    }
   ],
   "source": [
    "# generation through gpt-4o-mini \n",
    "# time 5.4 sec\n",
    "# cost <0.01$\n",
    "# partial answers generated on retrived top 10 communities by vector similarity\n",
    "\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {global_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vidore dataset questions sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### document 0707.1659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's question: What does the graph indicate about the relationship between \\( \\langle r(t) \\rangle \\) and time (t) during steady-state conditions?['A. \\\\( \\\\langle r(t) \\\\rangle \\\\) decreases as time increases.', 'B. \\\\( \\\\langle r(t) \\\\rangle \\\\) increases as time increases.', 'C. \\\\( \\\\langle r(t) \\\\rangle \\\\) remains constant as time increases.', 'D. The relationship between \\\\( \\\\langle r(t) \\\\rangle \\\\) and time cannot be determined from the graph.']\n",
      "Answer: The relationship between \\( \\langle r(t) \\rangle \\) and time (t) during steady-state conditions can be interpreted based on the information provided. The options suggest different behaviors of \\( \\langle r(t) \\rangle \\) as time progresses:\n",
      "\n",
      "- Option A states that \\( \\langle r(t) \\rangle \\) decreases as time increases.\n",
      "- Option B indicates that \\( \\langle r(t) \\rangle \\) increases as time increases.\n",
      "- Option C suggests that \\( \\langle r(t) \\rangle \\) remains constant as time increases.\n",
      "- Option D claims that the relationship cannot be determined from the graph.\n",
      "\n",
      "To arrive at a coherent conclusion, we need to consider the context of steady-state conditions. In such conditions, it is common for certain parameters to stabilize over time. Therefore, if the graph shows a consistent value for \\( \\langle r(t) \\rangle \\) as time progresses, it would support Option C, indicating that \\( \\langle r(t) \\rangle \\) remains constant.\n",
      "\n",
      "However, if the graph shows a trend of either increasing or decreasing values, then Options A or B would be more appropriate. If the graph lacks clarity or sufficient data to draw a conclusion, then Option D would be valid.\n",
      "\n",
      "In summary, without specific details from the graph, the most likely scenario during steady-state conditions is that \\( \\langle r(t) \\rangle \\) remains constant as time increases (Option C). However, if the graph indicates a clear trend, then the appropriate option would depend on that trend.\n"
     ]
    }
   ],
   "source": [
    "# generation through gpt-4o-mini \n",
    "# time 30.5 sec\n",
    "# cost 0.04$\n",
    "# partial answers generated on all the communities\n",
    "\n",
    "# The answer is wrong\n",
    "# the marks of the partial answers are all zeros\n",
    "# the real answer is embedded in the picture which is not in the graph\n",
    "\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {global_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's question: What does the graph indicate about the relationship between \\( \\langle r(t) \\rangle \\) and time (t) during steady-state conditions?['A. \\\\( \\\\langle r(t) \\\\rangle \\\\) decreases as time increases.', 'B. \\\\( \\\\langle r(t) \\\\rangle \\\\) increases as time increases.', 'C. \\\\( \\\\langle r(t) \\\\rangle \\\\) remains constant as time increases.', 'D. The relationship between \\\\( \\\\langle r(t) \\\\rangle \\\\) and time cannot be determined from the graph.']\n",
      "Answer: The relationship between \\( \\langle r(t) \\rangle \\) and time (t) during steady-state conditions can be interpreted based on the information provided. The options suggest different behaviors of \\( \\langle r(t) \\rangle \\) as time progresses:\n",
      "\n",
      "- Option A states that \\( \\langle r(t) \\rangle \\) decreases as time increases.\n",
      "- Option B indicates that \\( \\langle r(t) \\rangle \\) increases as time increases.\n",
      "- Option C suggests that \\( \\langle r(t) \\rangle \\) remains constant as time increases.\n",
      "- Option D claims that the relationship cannot be determined from the graph.\n",
      "\n",
      "To arrive at a coherent conclusion, we need to consider the context of steady-state conditions. In such conditions, it is common for certain parameters to stabilize over time. Therefore, if the graph indicates a stable value for \\( \\langle r(t) \\rangle \\) as time progresses, the most appropriate interpretation would be that \\( \\langle r(t) \\rangle \\) remains constant as time increases (Option C).\n",
      "\n",
      "However, if the graph shows a clear trend of either increase or decrease, then Options A or B would be applicable. If the graph does not provide enough information to ascertain the relationship, then Option D would be valid.\n",
      "\n",
      "In summary, without specific details from the graph, the most likely scenario during steady-state conditions is that \\( \\langle r(t) \\rangle \\) remains constant as time increases. Therefore, the final answer is:\n",
      "\n",
      "**C. \\( \\langle r(t) \\rangle \\) remains constant as time increases.**\n"
     ]
    }
   ],
   "source": [
    "# generation through gpt-4o-mini \n",
    "# time 6.1 sec\n",
    "# cost <0.01$\n",
    "# partial answers generated on 20 communities retireved by vector similarity on milvus\n",
    "\n",
    "# The answer is wrong\n",
    "# the marks of the partial answers are all zeros\n",
    "# the real answer is embedded in the picture which is not in the graph\n",
    "\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {global_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### document 0704.2547"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's question: What are the main author contributions in the field of quantum computing?\n",
      "Answer: The main author contributions in the field of quantum computing include:\n",
      "\n",
      "* Research on nonlinear resonators in Quantum Electrodynamics (QED)\n",
      "* Collaborative research on quantum computing with a focus on specific areas such as signal integrity, low-noise systems, and spin qubits\n",
      "* Exploration of measurement techniques and their impact on quantum states, particularly in the context of superconducting transmon qubits\n",
      "* Development of multi-qubit architectures and quantum error correction methods\n",
      "* Investigation of energy-level structures and resonant peaks that arise from the coupling of photons with qubits\n",
      "\n",
      "Additionally, notable researchers who have made significant contributions to the field include:\n",
      "\n",
      "* M.A. Nielsen and I.L. Chuang for their foundational text on quantum computation and information theory\n",
      "* F. R. Ong, M. Boissonneault, X.-J. Chen, A. Johansson, L. M. Zhang, and M. Fukuda for their research in various areas of quantum computing\n",
      "\n",
      "These contributions are crucial to advancing our understanding of quantum computing principles and applications.\n"
     ]
    }
   ],
   "source": [
    "# generation through gpt-4o-mini \n",
    "# time 23.1 sec\n",
    "# cost 0.03$\n",
    "# partial answers generated on all the communities\n",
    "\n",
    "# the answer is intrinsically correct\n",
    "\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {global_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison same question on different retrieval methods and different model generation\n",
    "The question is: What are the main author contributions in the field of quantum computing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's question: What are the main author contributions in the field of quantum computing?\n",
      "Answer: In the field of quantum computing, several key contributions have emerged from influential works and notable figures. One of the foundational texts is \"Quantum Computation and Quantum Information,\" co-authored by M.A. Nielsen and I.L. Chuang, which has significantly shaped the understanding of quantum computing and information theory. This work emphasizes the importance of multi-qubit architectures, which are essential for enhancing computational power, and highlights the critical role of quantum error correction techniques to ensure the reliable operation of quantum systems.\n",
      "\n",
      "Additionally, advancements in cryogenic technologies have been pivotal, particularly in maintaining signal integrity and developing low-noise systems that utilize spin qubits. These technologies are crucial for high-frequency measurements in quantum operations, further advancing the field.\n",
      "\n",
      "Numerical techniques such as the Density Matrix Renormalization Group (DMRG) have also played a significant role in understanding quantum many-body systems, particularly in calculating ground state energy. Contributions from researchers like A.N. Nicholson, M.G. Endres, D.B. Kaplan, and J.-W. Lee illustrate the collaborative nature of research in quantum computing, where teamwork is vital for progress.\n",
      "\n",
      "Moreover, physicists Gil Refael and David Pekker have made notable contributions through their collaborative work at the California Institute of Technology, further enriching the field. In quantum electrodynamics (QED), figures like F.R. Ong and M. Boissonneault are advancing the understanding and applications relevant to quantum computing.\n",
      "\n",
      "Overall, the contributions in quantum computing are characterized by foundational texts, innovative technologies, and collaborative efforts among researchers, all of which are essential for the ongoing development and understanding of this rapidly evolving field.\n"
     ]
    }
   ],
   "source": [
    "# generation through gpt-4o-mini \n",
    "# time 9.1 sec\n",
    "# cost <0.01$\n",
    "# partial answers generated on 20 embeddings retireved by vector similarity on milvus\n",
    "\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {global_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"The influential work 'Quantum Computation and Quantum Information' co-authored by M.A. Nielsen and I.L. Chuang is a foundational text in the field of quantum computing and information, highlighting significant author contributions.\",\n",
       "  75),\n",
       " ('The text discusses the development of multi-qubit architectures in quantum computing, which are essential for enhancing computational power, and highlights the importance of quantum error correction techniques to ensure reliable operation of these systems.',\n",
       "  70),\n",
       " ('The text highlights contributions related to cryogenic technologies and their importance in quantum computing, particularly in areas like signal integrity, low-noise systems, and the use of spin qubits, which are significant for high-frequency measurements in quantum operations.',\n",
       "  70),\n",
       " ('The Density Matrix Renormalization Group (DMRG) is a crucial numerical technique in quantum physics that contributes to the understanding of quantum many-body systems, particularly in calculating ground state energy.',\n",
       "  60),\n",
       " ('The text mentions significant contributions in areas like ion-atom interactions, spin liquid states, and advanced quantum techniques such as DMRG, as well as theorems on percolation and the exploration of quantum dynamics.',\n",
       "  60),\n",
       " ('The text mentions key figures in quantum physics research, including A.N. Nicholson, M.G. Endres, D.B. Kaplan, and J.-W. Lee, who have collaborated on a paper in the field. Their equal contributions and strong collaborative network highlight the importance of teamwork in advancing research in quantum computing.',\n",
       "  60),\n",
       " ('The text mentions the collaborative research efforts of physicists Gil Refael and David Pekker at the California Institute of Technology, indicating their contributions to the field of quantum computing through co-authored work.',\n",
       "  60),\n",
       " ('The text mentions notable figures in the field of quantum electrodynamics (QED), such as F. R. Ong and M. Boissonneault, who are contributing to the understanding and applications of QED, which is relevant to quantum computing.',\n",
       "  60),\n",
       " ('Paul Hopkins is a notable figure in the field of physics research, which includes contributions to quantum computing, as indicated by his collaboration with Matthias Schmidt.',\n",
       "  40),\n",
       " ('Notable figures in the field of quantum mechanics include Marek M. Rams and Bogdan Damski, who are engaged in advancing knowledge and research in this domain.',\n",
       "  40),\n",
       " (\"The text highlights the collaborative efforts of authors in scientific research, particularly mentioning X.-J. Chen's significant partnerships with F. Fu and L Wang, which indicates the importance of teamwork in advancing scientific knowledge. However, it does not provide specific contributions in the field of quantum computing.\",\n",
       "  30),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0)]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's question: What are the main author contributions in the field of quantum computing?\n",
      "Answer: In the field of quantum computing, several key contributions from notable authors and researchers have significantly advanced the discipline. One of the foundational texts is \"Quantum Computation and Quantum Information,\" co-authored by M.A. Nielsen and I.L. Chuang, which has been instrumental in shaping the theoretical framework of quantum computing and information.\n",
      "\n",
      "Research by A. Johansson and colleagues, particularly their studies published in reputable journals like Physical Review Letters, has also made substantial contributions, focusing on innovative approaches and experimental techniques in quantum systems.\n",
      "\n",
      "The development of cryogenic technologies has been highlighted as crucial for maintaining signal integrity and low-noise conditions in quantum computing, especially in the context of spin qubits, which are vital for high-frequency measurements. Additionally, advancements in superconducting transmon qubits and the understanding of measurement-induced dephasing have provided insights into the performance and coherence of quantum states.\n",
      "\n",
      "Multi-qubit architectures have emerged as essential for enhancing computational power, with significant emphasis on quantum error correction techniques that address challenges related to fidelity and decoherence. Understanding decoherence itself is a critical area of research, as it impacts the behavior of quantum states and their interactions, which is fundamental for developing robust quantum systems.\n",
      "\n",
      "Institutions like the Tata Institute of Fundamental Research (TIFR) play a pivotal role in advancing theoretical physics, which underpins quantum computing. Researchers such as Sakuntala Chatterjee and Mustansir Barma contribute to a collaborative environment that fosters innovation in the field.\n",
      "\n",
      "Moreover, techniques like the Density Matrix Renormalization Group (DMRG) are vital for analyzing quantum many-body systems, while concepts from cavity quantum electrodynamics (Cavity QED) explore the interactions between light and matter, further enriching the understanding of qubit dynamics.\n",
      "\n",
      "Overall, the contributions from these authors and their collaborative efforts are crucial for the ongoing development and refinement of quantum computing technologies.\n"
     ]
    }
   ],
   "source": [
    "# generation through gpt-4o-mini \n",
    "# time 31.6 sec\n",
    "# cost 0.03$\n",
    "# partial answers generated on all the communities\n",
    "\n",
    "# The answer is correct\n",
    "\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {global_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"The influential work 'Quantum Computation and Quantum Information' co-authored by M.A. Nielsen and I.L. Chuang is a foundational text in the field of quantum computing and information, highlighting significant author contributions.\",\n",
       "  75),\n",
       " ('The research conducted by A. Johansson and colleagues, particularly their study published in Physical Review Letters, represents a significant contribution to the field of quantum computing.',\n",
       "  70),\n",
       " ('The text highlights contributions related to cryogenic technologies and their importance in quantum computing, particularly in areas like signal integrity, low-noise systems, and the use of spin qubits, which are significant for high-frequency measurements in quantum operations.',\n",
       "  70),\n",
       " ('The text discusses the superconducting transmon qubit and measurement-induced dephasing, which are significant contributions to the field of quantum computing, particularly in understanding the performance and coherence of quantum states.',\n",
       "  70),\n",
       " ('The text discusses the development of multi-qubit architectures in quantum computing, which are essential for enhancing computational power. It also highlights the importance of quantum error correction techniques in maintaining fidelity and managing decoherence, which are significant challenges in the field.',\n",
       "  70),\n",
       " ('The text mentions the collaborative research efforts of physicists Gil Refael and David Pekker at the California Institute of Technology, indicating their contributions to the field of quantum computing through co-authored work.',\n",
       "  60),\n",
       " ('The Tata Institute of Fundamental Research (TIFR) is a leading research institution that plays a pivotal role in advancing research in theoretical physics, which is foundational to quantum computing. Notable researchers affiliated with TIFR, such as Sakuntala Chatterjee and Mustansir Barma, contribute to the academic partnerships in this field.',\n",
       "  60),\n",
       " ('The chunk discusses the concept of decoherence in quantum mechanics, which is a significant topic in quantum computing as it affects the behavior of quantum states and their interactions. Understanding decoherence is crucial for developing robust quantum systems.',\n",
       "  60),\n",
       " ('The Tata Institute of Fundamental Research (TIFR) is a significant institution in advancing research in theoretical physics, which is foundational to quantum computing. Notable researchers affiliated with TIFR, such as Sakuntala Chatterjee and Mustansir Barma, contribute to the collaborative environment that fosters advancements in this field.',\n",
       "  60),\n",
       " ('The Density Matrix Renormalization Group (DMRG) is a crucial numerical technique in quantum physics that contributes to the field of quantum computing by analyzing quantum many-body systems and calculating ground state energy.',\n",
       "  60),\n",
       " ('The chunk discusses cavity quantum electrodynamics (Cavity QED), which is relevant to quantum computing as it explores the interactions between light and matter, particularly in the context of qubits. Understanding these interactions is important for advancing quantum technologies.',\n",
       "  60),\n",
       " ('The text mentions notable figures in the field of quantum electrodynamics (QED), such as F. R. Ong and M. Boissonneault, who are contributing to the understanding and applications of QED, which is relevant to quantum computing.',\n",
       "  60),\n",
       " ('The text discusses the role of PCB design techniques in enhancing quantum computing technologies, particularly through the use of fencing vias to reduce crosstalk and improve device performance, which are important contributions to the field.',\n",
       "  60),\n",
       " ('The research conducted at the Max-Planck-Institut für Quantenoptik, particularly by authors like B.G.U. Englert, contributes to the understanding of polar molecules in microstructured traps, which is relevant to advancements in quantum optics and quantum computing.',\n",
       "  60),\n",
       " ('The text discusses the theoretical framework of Kerr resonators and their quantum behavior, which may contribute to advancements in quantum computing, particularly in measurement techniques and understanding quantum effects.',\n",
       "  60),\n",
       " ('The text mentions key figures in quantum physics research, including A.N. Nicholson, M.G. Endres, D.B. Kaplan, and J.-W. Lee, who have collaborated on a paper in the field. Their equal contributions and strong collaborative network highlight the importance of teamwork in advancing research in quantum computing.',\n",
       "  60),\n",
       " ('Abu-Ibrahim is a key figure in the research and development of methods for higher order corrections in quantum computing, and his collaboration with Suzuki highlights the importance of teamwork in advancing this field.',\n",
       "  60),\n",
       " ('The text mentions significant contributions in areas like ion-atom interactions, spin liquid states, and advanced quantum techniques such as DMRG, as well as theorems on percolation and the exploration of quantum dynamics, indicating a collaborative effort in the field of quantum systems.',\n",
       "  60),\n",
       " ('Ultracold clouds are utilized in quantum memory applications, highlighting their importance in quantum information storage.',\n",
       "  60),\n",
       " ('The chunk discusses the study of quantum many-body systems and the use of periodic driving, which are important areas in quantum computing research. However, it does not specify individual author contributions.',\n",
       "  40),\n",
       " ('The chunk mentions advancements in quantum technologies and materials science, which may relate to contributions in quantum computing, particularly through the manipulation of atomic waves and engineered materials.',\n",
       "  40),\n",
       " (\"The text highlights the collaborative efforts of authors in scientific research, particularly mentioning X.-J. Chen's significant partnership with F. Fu, which may indicate contributions in the field of quantum computing through their co-authored papers.\",\n",
       "  40),\n",
       " ('The text discusses quantum phase transitions and their relationship with level-spacing distributions, which are important concepts in quantum computing. However, it does not specify individual author contributions in the field.',\n",
       "  40),\n",
       " ('Paul Hopkins is a notable figure in the field of physics research, which includes contributions to quantum computing, and his collaboration with Matthias Schmidt indicates a shared interest in advancing knowledge in this area.',\n",
       "  40),\n",
       " ('The text mentions Robert P. Smith and Zoran Hadzibabic as notable contributors in the field of quantum physics, specifically in Bose-Einstein condensation, indicating their collaborative efforts in advancing research.',\n",
       "  40),\n",
       " ('Notable figures in the field of quantum mechanics include Marek M. Rams and Bogdan Damski, who are affiliated with Los Alamos National Laboratory.',\n",
       "  30),\n",
       " ('The chunk mentions the potential for innovative applications in fields such as quantum computing, indicating a connection between electric traps and quantum computing, but does not specify main author contributions.',\n",
       "  30),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0)]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's question: What are the main author contributions in the field of quantum computing?\n",
      "Answer: The main author contributions in the field of quantum computing include:\n",
      "\n",
      "* Research on nonlinear resonators in Quantum Electrodynamics (QED)\n",
      "* Collaborative research on quantum computing with a focus on specific areas such as signal integrity, low-noise systems, and spin qubits\n",
      "* Exploration of measurement techniques and their impact on quantum states, particularly in the context of superconducting transmon qubits\n",
      "* Development of multi-qubit architectures and quantum error correction methods\n",
      "* Investigation of energy-level structures and resonant peaks that arise from the coupling of photons with qubits\n",
      "\n",
      "Additionally, notable researchers who have made significant contributions to the field include:\n",
      "\n",
      "* M.A. Nielsen and I.L. Chuang for their foundational text on quantum computation and information theory\n",
      "* F. R. Ong, M. Boissonneault, X.-J. Chen, A. Johansson, L. M. Zhang, and M. Fukuda for their research in various areas of quantum computing\n",
      "\n",
      "These contributions are crucial to advancing our understanding of quantum computing principles and applications.\n"
     ]
    }
   ],
   "source": [
    "# generation through llama3.1\n",
    "# time 11.9min on local pc with M2 pro\n",
    "# No cost\n",
    "\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {global_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('F. R. Ong, M. Boissonneault, and others have made notable contributions to the field of quantum computing through their research on nonlinear resonators in QED.',\n",
       "  60),\n",
       " ('X.-J. Chen has made significant contributions to the field of quantum computing through his collaborative research with F. Fu and L Wang.',\n",
       "  60),\n",
       " (\"A. Johansson and colleagues' study published in Phys. Rev. Lett. 95, 116805 (2005) is a significant contribution to the field of quantum computing.\",\n",
       "  60),\n",
       " ('Key contributors to quantum computing mentioned include those working on signal integrity, low-noise systems, and spin qubits.',\n",
       "  60),\n",
       " ('The main author contributions in this field are related to the exploration of measurement techniques and their impact on quantum states, particularly in the context of superconducting transmon qubits.',\n",
       "  60),\n",
       " (\"M.A. Nielsen and I.L. Chuang's book 'Quantum Computation and Quantum Information' is a foundational text in understanding the principles and applications of quantum mechanics in computing and information theory.\",\n",
       "  60),\n",
       " ('Authors contributing to multi-qubit architectures and quantum error correction are likely key figures in the field of quantum computing.',\n",
       "  60),\n",
       " ('Understanding quantum behavior and properties is crucial for the field of quantum computing.',\n",
       "  30),\n",
       " ('quantum technologies', 30),\n",
       " ('Researchers studying complex quantum phenomena use DMRG to calculate ground state energy.',\n",
       "  30),\n",
       " ('The main author contributions are likely related to the investigation of energy-level structures and resonant peaks that arise from the coupling of photons with qubits.',\n",
       "  30),\n",
       " ('No direct mention of quantum computing, but understanding atomic behavior and energy transfer could be related to the fundamental processes governing quantum systems.',\n",
       "  20),\n",
       " ('Gil Refael and David Pekker are notable researchers in the field of physics.',\n",
       "  20),\n",
       " ('Paul M. Goldbart is a prominent researcher affiliated with the University of Illinois at Urbana-Champaign, focusing on condensed matter physics.',\n",
       "  20),\n",
       " ('The study of quantum many-body systems and periodic driving is significant for understanding the dynamics and behaviors of complex systems.',\n",
       "  20),\n",
       " ('Marek M. Rams and Bogdan Damski are notable figures advancing knowledge and research in quantum mechanics.',\n",
       "  20),\n",
       " ('Theoretical models and empirical research are integrated in the field of quantum computing.',\n",
       "  20),\n",
       " (\"The 'Authors' node serves as a hub, linking numerous researchers who contribute to various publications...\",\n",
       "  20),\n",
       " (\"No direct mention of quantum computing, but the community's focus on advanced cooling and manipulation techniques might be related to its applications.\",\n",
       "  20),\n",
       " (\"Understanding quantum behaviors and phenomena is a key aspect of this community's focus.\",\n",
       "  20),\n",
       " ('L. M. Zhang and M. Fukuda are mentioned as authors who have contributed important research.',\n",
       "  20),\n",
       " ('No direct mention of quantum computing, but Markov Chain and MCMC methods are probabilistic processes that might be related to the field.',\n",
       "  20),\n",
       " ('P. L. Frabetti has made significant contributions to the field of physics, particularly in advancing knowledge through publications in Phys. Lett. B.',\n",
       "  20),\n",
       " ('Sakuntala Chatterjee and Mustansir Barma are affiliated with TIFR, indicating a strong collaborative environment.',\n",
       "  20),\n",
       " ('quantum computing', 20),\n",
       " ('Theoretical models, such as the Ising model, are crucial for understanding complex quantum behaviors.',\n",
       "  20),\n",
       " ('Co-authorship and collaboration are emphasized in advancing understanding in this area.',\n",
       "  20),\n",
       " ('Computational methods are used to study magnetic properties.', 20),\n",
       " ('The community emphasizes collective contributions to the topic.', 20),\n",
       " ('No direct mention of quantum computing, but the concept of microscopic interactions leading to macroscopic phenomena might be tangentially related.',\n",
       "  20),\n",
       " ('The main author contributions are related to understanding decoherence and its impact on quantum state behavior.',\n",
       "  20),\n",
       " ('Key contributors include James Aspnes, Jim Lathrop, Jack Lutz, Scott Summers, and Soma Chaudhuri.',\n",
       "  20),\n",
       " ('Sakuntala Chatterjee and Mustansir Barma are affiliated with TIFR, indicating a strong collaborative environment.',\n",
       "  20),\n",
       " (\"Al-Khalili's contributions to Glauber theory include developing a few-body treatment within this theoretical framework.\",\n",
       "  20),\n",
       " ('The community highlights the importance of vibrational dynamics in understanding quantum phenomena.',\n",
       "  20),\n",
       " ('No specific contributions are mentioned, but the community focuses on user-driven execution and problem-solving.',\n",
       "  20),\n",
       " ('No direct mention of quantum computing, but Markov Chain Monte Carlo (MCMC) techniques are mentioned.',\n",
       "  20),\n",
       " ('No direct mention of quantum computing, but the text does discuss quantum phenomena such as vortices and supercurrents.',\n",
       "  20),\n",
       " ('G. Szabó has authored and co-authored several significant scientific papers contributing to the advancement of knowledge in computational biology and physics.',\n",
       "  20),\n",
       " ('Quantum mechanics is central to this discussion.', 20),\n",
       " ('Understanding interactions and behavior of particles is a key area of study in quantum physics.',\n",
       "  20),\n",
       " ('Bayesian inference is used to estimate integrals and optimize sample weights.',\n",
       "  20),\n",
       " ('The main author contributions mentioned are the development of coupled-PCB platforms that support the operation of spin qubit devices.',\n",
       "  20),\n",
       " ('No direct mention of author contributions, but the community focuses on quantum systems and simulations.',\n",
       "  20),\n",
       " ('Bose-Einstein condensation is a phenomenon related to quantum physics.',\n",
       "  20),\n",
       " ('B.G.U. Englert and others have made a key contribution to the understanding of polar molecules in microstructured traps.',\n",
       "  20),\n",
       " ('No direct mention of quantum computing, but the concept of symmetry breaking and phase transitions might be related to some aspects of quantum computing.',\n",
       "  20),\n",
       " ('Notable figures include G. Ewald, W. Nörtershäuser, and J. Dilling, who are interconnected through a series of co-authored publications.',\n",
       "  20),\n",
       " ('Cryogenic environments play a critical role in advancing quantum technologies.',\n",
       "  20),\n",
       " ('No direct mention of quantum computing, but mean-field theory is mentioned as a critical concept.',\n",
       "  20),\n",
       " ('notable physicists', 20),\n",
       " ('The Luttinger Liquid model is central to understanding the dynamics of these one-dimensional systems, as it describes the behavior of interacting particles and their excitations.',\n",
       "  20),\n",
       " (\"R.J. Glauber's work on coherence in quantum systems is a key contribution to the field of quantum computing.\",\n",
       "  20),\n",
       " ('The community focuses on quantum systems and phase transitions.', 20),\n",
       " ('Huanyang Chen is a notable member of the community contributing to advancing knowledge in cold atom physics.',\n",
       "  20),\n",
       " ('Paul Hopkins and Matthias Schmidt are notable figures in this community, highlighting the collaborative nature of their work.',\n",
       "  20),\n",
       " ('Understanding quantum interactions is a key aspect of the field, which may be related to quantum computing.',\n",
       "  20),\n",
       " ('The acknowledgment of funding from grants is crucial for advancing research efforts.',\n",
       "  20),\n",
       " ('Theoretical models and quantum mechanics are crucial for advancing experimental quantum physics.',\n",
       "  20),\n",
       " ('The key figures include M.V. Zhukov, B.V. Danilin, D.V. Fedorov, J.M. Bang, and I.J. Thompson.',\n",
       "  20),\n",
       " (\"Erik Winfree's work established the foundational principles of a mathematical framework for studying self-assembly processes, demonstrating its Turing universality.\",\n",
       "  20),\n",
       " ('Robert P. Smith and Zoran Hadzibabic have made notable contributions to the field of quantum physics.',\n",
       "  20),\n",
       " ('The concept of spinons and topological sectors are crucial to understanding quantum systems.',\n",
       "  20),\n",
       " ('Understanding quantum phase transitions and the behavior of spin systems are related to quantum computing.',\n",
       "  20),\n",
       " ('The community highlights the collective efforts of researchers in advancing knowledge within their scientific domain.',\n",
       "  20),\n",
       " ('The key figures include A.N. Nicholson, M.G. Endres, D.B. Kaplan, and J.-W. Lee.',\n",
       "  20),\n",
       " ('No direct mention of quantum computing or its authors, but the community explores interactions of light at a quantum level.',\n",
       "  20),\n",
       " ('Abu-Ibrahim and Suzuki are key figures in advancing the field of higher order corrections.',\n",
       "  20),\n",
       " ('Authors and their collaborative works are central to this community.', 20),\n",
       " ('No direct mention of quantum computing, but resonators and parametric amplifiers might be related to quantum computing concepts.',\n",
       "  20),\n",
       " ('Renyuan Liao and other researchers have made significant contributions to the field through collaborative efforts.',\n",
       "  20),\n",
       " ('No direct mention of quantum computing, but the concept of phase transitions and statistical mechanics might be tangentially related to some aspects of quantum computing.',\n",
       "  20),\n",
       " ('H. N. and H. J. Carmichael are key individuals in this research landscape.',\n",
       "  20),\n",
       " ('The community focuses on various aspects of quantum mechanics, including the behavior of quantum systems during phase transitions.',\n",
       "  20),\n",
       " ('Understanding how self-assembly can enhance computational reliability and efficiency is a key topic.',\n",
       "  20),\n",
       " ('Quantum mechanics is mentioned as a related field.', 20),\n",
       " ('No direct mention of quantum computing, but the community focuses on superconductivity and its mathematical frameworks.',\n",
       "  20),\n",
       " ('Notable authors such as McKeegan, Chaussidon, and Robert have co-authored publications.',\n",
       "  20),\n",
       " ('The concept of quantum states and the manipulation of particles in certain systems is significant in quantum mechanics.',\n",
       "  20),\n",
       " ('The exploration of connections between visons and bulk singlet excitations highlights the intricate dynamics of quantum systems.',\n",
       "  20),\n",
       " ('Ultracold clouds are utilized in quantum memory applications, highlighting their importance in quantum information storage.',\n",
       "  20),\n",
       " ('The concept of quantum superposition is essential for grasping the complexities of quantum behavior and has significant implications for the process of measurement in quantum systems.',\n",
       "  20),\n",
       " ('The community focuses on NP-completeness and its implications in computer science.',\n",
       "  20),\n",
       " ('No direct mention of quantum computing, but the concept of spin liquids and quantum fluctuations may be related to understanding complex systems that could be relevant to quantum computing.',\n",
       "  20),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('Numerical simulations are used as computational methods to model and analyze systems.',\n",
       "  0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No direct mention of quantum computing or its authors.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No direct mention of quantum computing, but the community focuses on computational challenges and optimization.',\n",
       "  0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No direct mention of quantum computing or its authors.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No direct mention of quantum computing or its authors.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('Eigenvalues are mentioned as a key concept to determine population growth or decline.',\n",
       "  0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No direct relevance to quantum computing is mentioned.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No direct mention of quantum computing or its authors.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('Ross H. McKenzie and M. F. Smith have made key contributions to the study of the Wiedemann-Franz law.',\n",
       "  0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No direct mention of quantum computing or its authors.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('Michael Grunwald and Christoph Dellago contribute to computational materials science.',\n",
       "  0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0),\n",
       " ('No relevant info here.', 0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
