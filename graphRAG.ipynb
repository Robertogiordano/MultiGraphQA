{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install neo4j\n",
    "# !pip install langchain\n",
    "# !pip install PyPDF2\n",
    "# !pip install tiktoken\n",
    "# !pip install openai  # Only if you want to use the OpenAI API\n",
    "# !pip install transformers  # For open (HF) models\n",
    "# !pip install sentence_transformers\n",
    "# !pip install -U langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# -----------------------\n",
    "# Neo4j Database imports\n",
    "# -----------------------\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# -----------------------\n",
    "# LLM / Embeddings imports\n",
    "# -----------------------\n",
    "# If using HuggingFace transformers:\n",
    "from transformers import pipeline\n",
    "\n",
    "# If using LangChain for retrieval + QA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# If you want to use OpenAI, uncomment:\n",
    "# import openai\n",
    "\n",
    "# -----------------------\n",
    "# PDF Parsing library\n",
    "# -----------------------\n",
    "import PyPDF2  # or pypdf if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 1) CONFIGURATION: toggle open vs. OpenAI\n",
    "#############################################\n",
    "\n",
    "USE_OPENAI = False  # Set to True if you want to switch to OpenAIâ€™s ChatGPT\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"YOUR_OPENAI_API_KEY\")\n",
    "\n",
    "# For Neo4j:\n",
    "NEO4J_URI = \"neo4j+s://97d4d6ef.databases.neo4j.io\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"3bhQDb-56wt4bhRkekO83OCj0YXjD9N2lWn7sQpOnkc\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2) NEO4J CONNECTION AND GRAPH FUNCTIONS\n",
    "##################################################\n",
    "\n",
    "# Connect to Neo4j\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "def add_chunk_to_graph(tx, chunk_text: str, chunk_id: str, embedding: List[float]):\n",
    "    \"\"\"\n",
    "    Create a node in Neo4j for each chunk of text from the document,\n",
    "    along with the embedding as properties (for demonstration).\n",
    "    We can store the embedding as a list or a string representation.\n",
    "    \"\"\"\n",
    "    # Convert embedding to a string (for demonstration).\n",
    "    embedding_str = \",\".join([str(x) for x in embedding])\n",
    "\n",
    "    query = \"\"\"\n",
    "    MERGE (c:Chunk {chunk_id: $chunk_id})\n",
    "    ON CREATE SET c.text = $chunk_text,\n",
    "                  c.embedding = $embedding_str\n",
    "    \"\"\"\n",
    "    tx.run(query, chunk_id=chunk_id, chunk_text=chunk_text, embedding_str=embedding_str)\n",
    "\n",
    "\n",
    "def add_document_relationship(tx, doc_id: str, chunk_id: str):\n",
    "    \"\"\"\n",
    "    Link each chunk node to a parent Document node in Neo4j.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MERGE (d:Document {doc_id: $doc_id})\n",
    "    MERGE (c:Chunk {chunk_id: $chunk_id})\n",
    "    MERGE (d)-[:HAS_CHUNK]->(c)\n",
    "    \"\"\"\n",
    "    tx.run(query, doc_id=doc_id, chunk_id=chunk_id)\n",
    "\n",
    "\n",
    "def retrieve_relevant_chunks(tx, user_query: str, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    A simplistic retrieval function that returns chunk nodes.\n",
    "    In a real scenario, you'd have a vector similarity search using\n",
    "    embeddings. Here, for demonstration, we only do a naive text search\n",
    "    in the graph. You can integrate with external vector DB or\n",
    "    vector similarity queries in Neo4j (Graph Data Science).\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MATCH (c:Chunk)\n",
    "    WHERE c.text CONTAINS $user_query\n",
    "    RETURN c.chunk_id AS chunk_id, c.text AS text\n",
    "    LIMIT $limit\n",
    "    \"\"\"\n",
    "    result = tx.run(query, user_query=user_query, limit=limit)\n",
    "    return [record.data() for record in result]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 3) PDF PARSING AND CHUNKING\n",
    "##################################################\n",
    "\n",
    "def parse_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract raw text from a PDF file using PyPDF2.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_text_func(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:\n",
    "    \"\"\"\n",
    "    Use LangChain's RecursiveCharacterTextSplitter to chunk the text\n",
    "    for better embeddings & retrieval.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 4) EMBEDDING UTILITIES\n",
    "##################################################\n",
    "\n",
    "def get_hf_embedding_function(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Returns a function that can generate embeddings using a HuggingFace model.\n",
    "    \"\"\"\n",
    "    hf_embed = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    return hf_embed.embed_documents\n",
    "\n",
    "\n",
    "# If using OpenAI (uncomment if needed):\n",
    "# def get_openai_embedding_function(model_name: str = \"text-embedding-ada-002\"):\n",
    "#     def _embeddings(texts: List[str]) -> List[List[float]]:\n",
    "#         response = openai.Embedding.create(\n",
    "#             input=texts,\n",
    "#             model=model_name\n",
    "#         )\n",
    "#         embeddings = [item[\"embedding\"] for item in response[\"data\"]]\n",
    "#         return embeddings\n",
    "#     return _embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 5) INGEST PDF -> STORE IN GRAPH\n",
    "##################################################\n",
    "\n",
    "def ingest_pdf_into_graph(pdf_path: str, doc_id: str):\n",
    "    \"\"\"\n",
    "    Parse PDF, chunk it, generate embeddings, and store in Neo4j.\n",
    "    \"\"\"\n",
    "    # Step 1: Parse the PDF\n",
    "    raw_text = parse_pdf(pdf_path)\n",
    "\n",
    "    # Step 2: Chunk the text\n",
    "    chunks = chunk_text_func(raw_text)\n",
    "\n",
    "\n",
    "    # Step 3: Embeddings\n",
    "    if USE_OPENAI:\n",
    "        # Example if you have an OpenAI embeddings function\n",
    "        # embed_fn = get_openai_embedding_function()\n",
    "        # ...\n",
    "        raise NotImplementedError(\"OpenAI Embeddings not implemented in this snippet.\")\n",
    "    else:\n",
    "        embed_fn = get_hf_embedding_function()\n",
    "\n",
    "    # Generate embeddings in batch\n",
    "    embeddings = embed_fn(chunks)\n",
    "\n",
    "    # Step 4: Store in Neo4j\n",
    "    with driver.session() as session:\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            chunk_id = f\"{doc_id}_chunk_{i}\"\n",
    "            embedding = embeddings[i]\n",
    "            session.write_transaction(add_chunk_to_graph, chunk_text, chunk_id, embedding)\n",
    "            session.write_transaction(add_document_relationship, doc_id, chunk_id)\n",
    "\n",
    "    print(f\"Ingested {len(chunks)} chunks from {pdf_path} into Neo4j under Document {doc_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 6) QA / RAG PIPELINE\n",
    "##################################################\n",
    "\n",
    "def perform_qa_with_graph(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves relevant chunks from the Neo4j graph, then performs\n",
    "    a simple RAG-based answer. Here we demonstrate a naive approach:\n",
    "    1) Retrieve relevant chunks from the graph\n",
    "    2) Concatenate them as context\n",
    "    3) Use either an open model or an OpenAI model for generative answer.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        # Retrieve relevant chunks (Naive text-based example)\n",
    "        candidate_chunks = session.read_transaction(retrieve_relevant_chunks, user_query)\n",
    "    \n",
    "    print(f\"Found {len(candidate_chunks)} relevant chunks in the graph.\")\n",
    "    print(candidate_chunks)\n",
    "    \n",
    "    # Build the context\n",
    "    context = \"\\n\\n\".join([c[\"text\"] for c in candidate_chunks])\n",
    "\n",
    "    # Use a huggingface or openAI model for generation\n",
    "    if USE_OPENAI:\n",
    "        # Example for OpenAI's chatgpt:\n",
    "        # openai.api_key = OPENAI_API_KEY\n",
    "        # response = openai.ChatCompletion.create(\n",
    "        #     model=\"gpt-3.5-turbo\",\n",
    "        #     messages=[\n",
    "        #         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        #         {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {user_query}\"}\n",
    "        #     ]\n",
    "        # )\n",
    "        # answer = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        raise NotImplementedError(\"OpenAI ChatCompletion usage not fully implemented here.\")\n",
    "    else:\n",
    "        # Example with a local HF pipeline\n",
    "        qa_pipeline = pipeline(\"text-generation\", model=\"bigscience/bloom-560m\")\n",
    "        prompt = f\"Context: {context}\\nQuestion: {user_query}\\nAnswer:\"\n",
    "        answer_list = qa_pipeline(prompt, max_new_tokens=100, do_sample=True)\n",
    "        if answer_list:\n",
    "            answer = answer_list[0][\"generated_text\"].split(\"Answer:\")[-1].strip()\n",
    "        else:\n",
    "            answer = \"No answer generated.\"\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 7) MAIN EXECUTION EXAMPLE\n",
    "#############################################\n",
    "\n",
    "# Example usage:\n",
    "# 1) Ingest an arXiv PDF\n",
    "pdf_path = \"data/docs/0704.2547.pdf\"  # Replace with the path to your local arXiv PDF\n",
    "doc_id = \"0704.2547\"           # Arbitrary doc ID for grouping in Neo4j\n",
    "ingest_pdf_into_graph(pdf_path, doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/k27_kqbn1yvdcwxq44w79hw80000gn/T/ipykernel_10973/2632495188.py:15: DeprecationWarning: read_transaction has been renamed to execute_read\n",
      "  candidate_chunks = session.read_transaction(retrieve_relevant_chunks, user_query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 relevant chunks in the graph.\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's question: What are the main contributions of the paper?\n",
      "Answer: in the paper we show that a class of functions defined by the identity equation in a compact set is a continuous functional from a compact interval into the entire function space. We also show that for this class of functions the function values of (1.7) are the fundamental solutions of the integro-differential equation.\n",
      "Finally we show that for bounded functions (1.8) a necessary condition for solutions for (1.7) and (1.8) is that its values are contained\n"
     ]
    }
   ],
   "source": [
    "# 2) Ask a question\n",
    "user_query = \"What are the main contributions of the paper?\"\n",
    "answer = perform_qa_with_graph(user_query)\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
