{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install neo4j\n",
    "# !pip install langchain\n",
    "# !pip install PyPDF2\n",
    "# !pip install tiktoken\n",
    "# !pip install openai  # Only if you want to use the OpenAI API\n",
    "# !pip install transformers  # For open (HF) models\n",
    "# !pip install sentence_transformers\n",
    "# !pip install -U langchain-community\n",
    "# For advanced community detection with Leiden, you might need external libraries (e.g., igraph, networkx, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama pull llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "# -----------------------\n",
    "# Neo4j Database imports\n",
    "# -----------------------\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# -----------------------\n",
    "# LLM / Embeddings imports\n",
    "# -----------------------\n",
    "# If using HuggingFace transformers:\n",
    "from transformers import pipeline\n",
    "\n",
    "# If using LangChain for retrieval + QA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# LangChain GraphRag Setup\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "# from langchain_core.documents import Document\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "from langchain.vectorstores import Milvus\n",
    "from langchain.schema import Document\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# If you want to use OpenAI, uncomment:\n",
    "import openai\n",
    "openai_model=\"gpt-4o-mini\"\n",
    "# -----------------------\n",
    "# Load environment variables\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# -----------------------\n",
    "# ArXiv API\n",
    "# -----------------------\n",
    "import arxiv\n",
    "\n",
    "# -----------------------\n",
    "# PDF Parsing library\n",
    "# -----------------------\n",
    "import PyPDF2  # or \"pypdf\" if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 1) CONFIGURATION: toggle open vs. OpenAI\n",
    "#############################################\n",
    "\n",
    "USE_OPENAI = True  # Set to True if you want to switch to OpenAI’s ChatGPT\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# For Neo4j:\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2) NEO4J CONNECTION AND GRAPH FUNCTIONS\n",
    "##################################################\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "def add_chunk_node(tx, chunk_text: str, chunk_id: str, embedding: List[float]):\n",
    "    \"\"\"\n",
    "    Create or merge a chunk node in Neo4j to represent a piece of text.\n",
    "    Store its embedding as well.\n",
    "    \"\"\"\n",
    "    embedding_str = \",\".join([str(x) for x in embedding])\n",
    "\n",
    "    query = \"\"\"\n",
    "    MERGE (c:Chunk {chunk_id: $chunk_id})\n",
    "    ON CREATE SET c.text = $chunk_text,\n",
    "                  c.embedding = $embedding_str\n",
    "    \"\"\"\n",
    "    tx.run(query, chunk_id=chunk_id, chunk_text=chunk_text, embedding_str=embedding_str)\n",
    "\n",
    "\n",
    "def add_document_relationship(tx, doc_id: str, chunk_id: str):\n",
    "    \"\"\"\n",
    "    Link each chunk node to a parent Document node in Neo4j.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MERGE (d:Document {doc_id: $doc_id})\n",
    "    MERGE (c:Chunk {chunk_id: $chunk_id})\n",
    "    MERGE (d)-[:HAS_CHUNK]->(c)\n",
    "    \"\"\"\n",
    "    tx.run(query, doc_id=doc_id, chunk_id=chunk_id)\n",
    "\n",
    "\n",
    "def add_element_instance(tx, element_data: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    (Step 2.2) Insert extracted graph node/edge relationships from text chunk.\n",
    "    Example structure of element_data might be:\n",
    "    {\n",
    "        \"entity_name\": \"...\",\n",
    "        \"entity_type\": \"...\",\n",
    "        \"entity_description\": \"...\",\n",
    "        \"relationship\": {\n",
    "            \"source_entity\": \"...\",\n",
    "            \"target_entity\": \"...\",\n",
    "            \"description\": \"...\",\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "\n",
    "    In a real pipeline, you might store these as separate nodes/edges. \n",
    "    For demonstration, we store them in a single node with a property that can be parsed.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    CREATE (e:ElementInstance {data: $element_data})\n",
    "    \"\"\"\n",
    "    tx.run(query, element_data=str(element_data))\n",
    "\n",
    "\n",
    "def retrieve_relevant_chunks(tx, user_query: str, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    A simplistic retrieval function that returns chunk nodes based on naive text search.\n",
    "    In a real scenario, you'd have a vector similarity search using embeddings.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MATCH (c:Chunk)\n",
    "    WHERE c.text CONTAINS $user_query\n",
    "    RETURN c.chunk_id AS chunk_id, c.text AS text\n",
    "    LIMIT $limit\n",
    "    \"\"\"\n",
    "    result = tx.run(query, user_query=user_query, limit=limit)\n",
    "    return [record.data() for record in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.1 SOURCE DOCUMENTS → TEXT CHUNKS\n",
    "##################################################\n",
    "def find_metadata(doc_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    (Step 2.1) Retrieve metadata for a document from a database or API.\n",
    "    \"\"\"\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        id_list=[doc_id]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        result = next(client.results(search))\n",
    "        return \\\n",
    "            {\"title\": result.title, \n",
    "            \"summary\": result.summary, \n",
    "            \"url\": result.entry_id,\n",
    "            \"authors\": ', '.join([a.name for a in result.authors]),\n",
    "            \"categories\": ', '.join(result.categories)\n",
    "            }\n",
    "    except StopIteration:\n",
    "        return {}\n",
    "\n",
    "def parse_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract raw text from a PDF file using PyPDF2.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 600, chunk_overlap: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    (Step 2.1) Split text into chunks. \n",
    "    Following the guidance in 2.1, we use a smaller chunk size (e.g., ~600 tokens).\n",
    "    This can improve entity recall at the cost of more LLM calls.\n",
    "    \"\"\"\n",
    "    # Note: the chunk_size is in characters by default using this splitter;\n",
    "    # you may want to adapt to token-based splitting.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 4) EMBEDDING UTILITIES\n",
    "##################################################\n",
    "\n",
    "def get_hf_embedding_function(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\", device: str = \"mps\"):\n",
    "    \"\"\"\n",
    "    Returns a function that can generate embeddings using a HuggingFace model.\n",
    "    \"\"\"\n",
    "    hf_embed = HuggingFaceEmbeddings(model_name=model_name, device=device)\n",
    "    return hf_embed.embed_documents\n",
    "\n",
    "\n",
    "# If using OpenAI embeddings, uncomment and implement:\n",
    "# def get_openai_embedding_function(model_name: str = \"text-embedding-ada-002\"):\n",
    "#     def _embeddings(texts: List[str]) -> List[List[float]]:\n",
    "#         response = openai.Embedding.create(\n",
    "#             input=texts,\n",
    "#             model=model_name\n",
    "#         )\n",
    "#         embeddings = [item[\"embedding\"] for item in response[\"data\"]]\n",
    "#         return embeddings\n",
    "#     return _embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check and initialize Milvus database\n",
    "def init_milvus_db(collection_name: str, uri: str, embedding_function):\n",
    "    \"\"\"\n",
    "    Initialize the Milvus database if it does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(uri.replace(\"sqlite://\", \"\")):\n",
    "        print(f\"Creating database at {uri}\")\n",
    "    vectorstore = Milvus(\n",
    "        collection_name=collection_name,\n",
    "        embedding=embedding_function,\n",
    "        connection_args={\"uri\": uri},\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "# Function to add multiple documents to Milvus\n",
    "def add_documents_to_milvus(docs: list, embedding_function, collection_name: str = \"rag_milvus\", uri: str = \"sqlite://./vector_db_graphRAG/milvus_ingest.db\"):\n",
    "    \"\"\"\n",
    "    Add multiple documents to the Milvus vector store.\n",
    "\n",
    "    Args:\n",
    "        docs (list): List of tuples containing text and metadata.\n",
    "        collection_name (str): Name of the Milvus collection.\n",
    "        uri (str): URI for the Milvus database.\n",
    "    \n",
    "    # Example usage\n",
    "    docs = [\n",
    "        {\"text\": \"Chunk 1 of the document\", \"metadata\": {\"doc_id\": \"doc_1\", \"chunk\": 1}},\n",
    "        {\"text\": \"Chunk 2 of the document\", \"metadata\": {\"doc_id\": \"doc_1\", \"chunk\": 2}}\n",
    "    ]\n",
    "\n",
    "    add_documents_to_milvus(docs)\n",
    "    \"\"\"\n",
    "    # Initialize the database\n",
    "    vectorstore = init_milvus_db(collection_name, uri, embedding_function)\n",
    "\n",
    "    # Prepare documents\n",
    "    document_list = []\n",
    "    for doc in docs:\n",
    "        text = doc.get(\"text\", \"\")\n",
    "        metadata = doc.get(\"metadata\", {})\n",
    "        document_list.append(Document(page_content=text, metadata=metadata))\n",
    "\n",
    "    # Add documents to the vector store\n",
    "    vectorstore.add_documents(document_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_element_instances_from_chunk(\n",
    "    chunk_text: str,\n",
    "    gleaning_rounds: int = 1,\n",
    "    USE_OPENAI: bool = True,\n",
    "    local_llm: str = \"llama3.1\"\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    (Step 2.2) Use an LLM prompt to identify entity references, relationships, and covariates.\n",
    "    - Identifies entities (name, type, description) and relationships.\n",
    "    - Supports multiple rounds of \"gleanings\" to find any missed entities.\n",
    "    \"\"\"\n",
    "    extracted_elements = []\n",
    "    not_parsed = []\n",
    "\n",
    "    # Select the LLM to use\n",
    "    if USE_OPENAI:\n",
    "        llm = ChatOpenAI(model=openai_model, temperature=0.0)\n",
    "    else:\n",
    "        llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    # Base prompt for extracting entities and relationships\n",
    "    base_prompt = (\n",
    "        \"Extract entities and relationships from the following text. \"\n",
    "        \"For each entity, provide its name, type, and description. \"\n",
    "        \"For each relationship, provide the source entity, target entity, and description. \"\n",
    "        \"Text: \\n{chunk_text}\\n\"\n",
    "        \"Output format: List of dictionaries with keys 'entity_name', 'entity_type', 'entity_description', 'relationship'. \"\n",
    "        \"Follow this format in the example: [{{\\\"entity_name\\\": \\\"Alice\\\", \\\"entity_type\\\": \\\"Person\\\", \\\"entity_description\\\": \\\"A person of interest.\\\", \\\"relationship\\\": {{\\\"source_entity\\\": \\\"Alice\\\", \\\"target_entity\\\": \\\"Bob\\\", \\\"description\\\": \\\"Knows\\\"}}}}]\"\n",
    "        \"Return just the list, so that we can parse it.\"\n",
    "        \"No bullet list or asterisks needed.\"\n",
    "        \"It must be a unique list, do NOT separate entities and relationships in different lists.\"\n",
    "    )\n",
    "\n",
    "    # Loop through gleaning rounds\n",
    "    for round_num in range(gleaning_rounds):\n",
    "        prompt = base_prompt.format(chunk_text=chunk_text)\n",
    "\n",
    "        # Send prompt to the selected LLM\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        # Parse the LLM response\n",
    "        new_elements = response.content\n",
    "        new_elements = new_elements.replace('```json','').replace('```','')\n",
    "\n",
    "        # Assume the response is already in JSON format\n",
    "        try:\n",
    "            new_elements = eval(new_elements)  # Convert string to list of dicts\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing LLM output: {e}\")\n",
    "            not_parsed.extend(new_elements)\n",
    "            new_elements = []\n",
    "\n",
    "        # Add new elements to the result\n",
    "        extracted_elements.extend(new_elements)\n",
    "\n",
    "        # Check if gleaning is needed (e.g., ask LLM if entities were missed)\n",
    "        if round_num < gleaning_rounds - 1:\n",
    "            print(\"Asking for validation...\")\n",
    "            validation_prompt = (\n",
    "                \"Were any entities or relationships missed in the previous extraction? \"\n",
    "                \"Answer 'Yes' or 'No'.\"\n",
    "            )\n",
    "            validation_response = llm.invoke(\n",
    "                [ HumanMessage(content=validation_prompt)], \n",
    "                chat_history=\n",
    "                [HumanMessage(content=prompt), SystemMessage(content=new_elements)]\n",
    "            )\n",
    "\n",
    "            # If LLM says 'No', break early\n",
    "            if 'No' in validation_response.content:\n",
    "                break\n",
    "\n",
    "    # Return all extracted elements\n",
    "    return extracted_elements, not_parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_prompt = \"\"\"\n",
    "You are an expert in text summarization and knowledge graph construction. I will provide you with:\n",
    "\n",
    "1. A list of initial nodes, each containing:\n",
    "   - A unique identifier\n",
    "   - A textual description extracted from the source\n",
    "   - Additional properties (optional)\n",
    "\n",
    "2. A list of relationships between these nodes (e.g., an entity \"Einstein\" related to another entity \"Relativity\").\n",
    "\n",
    "Your task is to:\n",
    "- Identify when multiple nodes actually refer to the same entity or concept.\n",
    "- Generate *summarized nodes* by consolidating their textual descriptions and removing duplicates or near-duplicates.\n",
    "- Maintain references to each node's original ID within your summarized node.\n",
    "- Create new relationships among these summarized nodes that reflect the original relationships, but merged and simplified where appropriate.\n",
    "\n",
    "**Important requirements and format details:**\n",
    "1. Each summarized node should have:\n",
    "   - A `summary` field with the merged description.\n",
    "   - A list of `original_ids` that were merged into this new summary node.\n",
    "   - Any relevant `type` or `label` (e.g., Person, Theory, Location) if it can be inferred from the text.\n",
    "   - (Optional) A short list of `keywords` extracted from the descriptions.\n",
    "\n",
    "2. Each relationship should:\n",
    "   - Include `source` and `target` references to the new summarized nodes.\n",
    "   - Provide a `relation_type` (e.g., \"INVENTED\", \"WORKS_ON\", \"LOCATED_IN\", etc.).\n",
    "   - Have a `weight` or `relevance_score` if it can be inferred (e.g., frequency or importance).\n",
    "   - (Optional) Include an `original_relationships` list indicating which original relationships were merged.\n",
    "\n",
    "3. Return the final data in **JSON** format, containing two top-level keys: `summarized_nodes` and `summarized_relationships`.\n",
    "\n",
    "4. Be concise but ensure the summaries and relationships accurately capture the original meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Here is the initial Nodes and Relationships**:\n",
    "\n",
    "{initial_data}\n",
    "\n",
    "---\n",
    "\n",
    "### **Instructions to the LLM**:\n",
    "1. **Identify duplicates or near-duplicates** (e.g., \"Albert Einstein\" and \"Einstein\" might refer to the same entity).\n",
    "2. **Create a new summarized node** that merges the descriptions of \"N1\" and \"N2\" if they represent the same entity (in this case, Albert Einstein).\n",
    "3. **Consolidate relationships** so that if multiple original relationships lead to the same concept, you unify them into a single relationship with an updated weight (e.g., sum or average of the original).\n",
    "4. Provide your final answer in the following **JSON** structure:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"summarized_nodes\": [\n",
    "    {{\n",
    "      \"title\": \"NewTitle1\",\n",
    "      \"summary\": \"Your merged summary text here...\",\n",
    "      \"original_ids\": [\"ExampleID1\", \"ExampleID2\", ...],\n",
    "      \"type\": \"Person\",\n",
    "      \"keywords\": [\"Einstein\", \"relativity\", \"physics\"]\n",
    "    }},\n",
    "    {{\n",
    "      \"title\": \"NewTitle2\",\n",
    "      \"summary\": \"Your summary text here...\",\n",
    "      \"original_ids\": [\"ExampleID3\", \"ExampleID4\"],\n",
    "      \"type\": \"Theory\",\n",
    "      \"keywords\": [\"relativity\", \"physics\"]\n",
    "    }}\n",
    "  ],\n",
    "  \"summarized_relationships\": [\n",
    "    {{\n",
    "      \"source\": \"NewTitle1\",\n",
    "      \"target\": \"NewTitle2\",\n",
    "      \"relation_type\": \"DEVELOPED_OR_ASSOCIATED_WITH\",\n",
    "      \"weight\": 5,\n",
    "      \"original_relationships\": [\"N1->N3(DEVELOPED)\", \"N2->N3(ASSOCIATED_WITH)\"]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "\n",
    "Please **only** output valid JSON in the format described above, without additional commentary so that we can parse it correctly. \n",
    "Make sure to capture the essence of each original node and relationship in your summarized version.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.3 ELEMENT INSTANCES → ELEMENT SUMMARIES\n",
    "##################################################\n",
    "\n",
    "def summarize_element_instances(\n",
    "    element_instances: List[Dict[str, Any]], \n",
    "    USE_OPENAI: bool = True,\n",
    "    local_llm: str = \"llama3.1\") -> str:\n",
    "    \"\"\"\n",
    "    (Step 2.3) Summarize extracted nodes/relationships into a single descriptive block of text\n",
    "    for each chunk. This is an additional LLM-based summarization step, forming \"element summaries.\"\n",
    "    \"\"\"\n",
    "    # Select the LLM to use\n",
    "    if USE_OPENAI:\n",
    "        llm = ChatOpenAI(model=openai_model, temperature=0.0)\n",
    "    else:\n",
    "        llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    prompt = summarization_prompt.format(initial_data=element_instances)\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    response = response.content\n",
    "\n",
    "    try:\n",
    "        response = response.replace('```json','').replace('```','')\n",
    "        response = eval(response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing LLM output: {e}\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_element_summary_in_graph(tx, data: Dict[str, Any], doc_id: str):\n",
    "    \"\"\"\n",
    "    Load the summarized graph data into Neo4j.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creazione dei nodi\n",
    "    for node in data[\"summarized_nodes\"]:\n",
    "        query_create_node = \"\"\"\n",
    "        CREATE (n:SummarizedNode {\n",
    "            title: $title,\n",
    "            summary: $summary,\n",
    "            original_ids: $original_ids,\n",
    "            type: $type,\n",
    "            keywords: $keywords,\n",
    "            doc_id : $doc_id\n",
    "        })\n",
    "        \"\"\"\n",
    "        tx.run(\n",
    "            query_create_node,\n",
    "            title=node.get(\"title\"),\n",
    "            summary=node.get(\"summary\"),\n",
    "            original_ids=node.get(\"original_ids\"),\n",
    "            type=node.get(\"type\"),\n",
    "            keywords=node.get(\"keywords\"),\n",
    "            doc_id=doc_id\n",
    "        )\n",
    "\n",
    "    # Creazione delle relazioni\n",
    "    for rel in data[\"summarized_relationships\"]:\n",
    "        query_create_rel = f\"\"\"\n",
    "        MATCH (source:SummarizedNode {{title: $source_id}})\n",
    "        MATCH (target:SummarizedNode {{title: $target_id}})\n",
    "        CREATE (source)-[:{rel['relation_type']} {{\n",
    "            weight: $weight,\n",
    "            original_relationships: $original_rels\n",
    "        }}]->(target)\n",
    "        \"\"\"\n",
    "        tx.run(\n",
    "            query_create_rel,\n",
    "            source_id=rel[\"source\"],\n",
    "            target_id=rel[\"target\"],\n",
    "            weight=rel.get(\"weight\", 1),  # default=1 if not provided\n",
    "            original_rels=rel.get(\"original_relationships\", [])\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.4 ELEMENT SUMMARIES → GRAPH COMMUNITIES\n",
    "##################################################\n",
    "\n",
    "def detect_communities():\n",
    "    \"\"\"\n",
    "    (Step 2.4) Perform community detection on the stored nodes/edges in the graph.\n",
    "    For demonstration, we omit the full code for Leiden or other algorithms.\n",
    "    In a real system, you’d gather the graph elements from Neo4j, run community detection,\n",
    "    and store the results (community IDs, hierarchical structure, etc.) back into Neo4j.\n",
    "    \"\"\"\n",
    "    # Placeholder function\n",
    "    print(\"[Community Detection] Placeholder: run Leiden or other community detection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.5 GRAPH COMMUNITIES → COMMUNITY SUMMARIES\n",
    "##################################################\n",
    "\n",
    "def summarize_communities():\n",
    "    \"\"\"\n",
    "    (Step 2.5) Summarize each community (or sub-community in a hierarchical approach).\n",
    "    - Gather all element summaries (nodes, edges, covariates) in that community.\n",
    "    - Summarize them, potentially chunking if they don't fit in an LLM context window.\n",
    "    \"\"\"\n",
    "    # Placeholder logic\n",
    "    print(\"[Community Summaries] Placeholder: gather summaries and do hierarchical summarization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.6 COMMUNITY SUMMARIES → COMMUNITY ANSWERS → GLOBAL ANSWER\n",
    "##################################################\n",
    "\n",
    "def answer_query_from_communities(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    (Step 2.6) Use the hierarchical community summaries to answer user queries globally.\n",
    "    - In an actual implementation, you'd fetch the relevant community summaries, chunk them,\n",
    "      run partial QA on each chunk, rank answers by helpfulness, and then produce a final answer.\n",
    "    - Below is a simplified approach that just returns a single, direct LLM-based QA.\n",
    "    \"\"\"\n",
    "    # Placeholder logic\n",
    "    # If you have multiple community summaries, you'd do partial QA in parallel, rank by\n",
    "    # self-reported \"helpfulness\" (0-100), then combine or reduce them into a global answer.\n",
    "\n",
    "    return f\"Global answer to '{user_query}' (placeholder).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 5) INGEST PDF -> STORE IN GRAPH (Putting Steps 2.1 and 2.2+ in context)\n",
    "##################################################\n",
    "\n",
    "# Define parallelized function for processing a single chunk\n",
    "def process_chunk_wrapper(i, chunk_text_str):\n",
    "    # Step 5: Extract element instances\n",
    "    print(f\"Extracting element instances from chunk {i}...\")\n",
    "    elements_instance, not_parsed_instance = extract_element_instances_from_chunk(chunk_text_str, USE_OPENAI=USE_OPENAI)\n",
    "    print(f\"Extracted {len(elements_instance)} elements from chunk {i}\\n\\n\")\n",
    "    return elements_instance, not_parsed_instance\n",
    "\n",
    "\n",
    "def ingest_pdf_into_graph(pdf_path: str, doc_id: str, embed_opt: bool = False):\n",
    "    \"\"\"\n",
    "    1) Parse PDF into raw text.\n",
    "    2) Chunk it (Step 2.1).\n",
    "    3) Generate embeddings for each chunk.\n",
    "    4) Store chunk nodes in Neo4j.\n",
    "    5) For each chunk, call LLM to extract element instances (Step 2.2).\n",
    "    6) Summarize them into a single descriptive block (Step 2.3).\n",
    "    7) Optionally store the block in Neo4j for further community detection.\n",
    "    \"\"\"\n",
    "    # Step 1: Parse PDF\n",
    "    print(f\"Parsing PDF at {pdf_path}...\")\n",
    "    raw_text = parse_pdf(pdf_path)\n",
    "    print(f\"Extracted {len(raw_text)} characters from {pdf_path} \\n\\n\")\n",
    "\n",
    "    # Step 1.1: Retrieve metadata\n",
    "    print(f\"Retrieving metadata for {doc_id}...\")\n",
    "    metadata = find_metadata(doc_id)\n",
    "    print(f\"Metadata: {metadata}\\n\\n\")\n",
    "\n",
    "    # Step 2: Chunk the text (default chunk_size=600 for improved recall)\n",
    "    chunks = chunk_text(raw_text)\n",
    "    print(f\"Chunked {len(chunks)} segments from {pdf_path} \\n\\n\")\n",
    "\n",
    "    # Step 3: Embeddings\n",
    "    if embed_opt:\n",
    "        print(\"Generating embeddings for each chunk...\")\n",
    "        # if USE_OPENAI:\n",
    "        #     # Implement an OpenAI embedding function if desired\n",
    "        #     raise NotImplementedError(\"OpenAI embeddings not implemented here.\")\n",
    "        # else:\n",
    "        embed_fn = get_hf_embedding_function()\n",
    "\n",
    "        add_documents_to_milvus([{\"text\": chunk, \"metadata\": {\"doc_id\": doc_id, \"chunk\": i, **metadata}} for i, chunk in enumerate(chunks)], embed_fn)\n",
    "        print(f\"Stored {len(chunks)} chunks in Milvus under Document {doc_id} \\n\\n\")\n",
    "\n",
    "    # Step 4: Store chunk nodes in Neo4j\n",
    "    # Parallel execution\n",
    "    extracted_elements = []\n",
    "    not_parsed_elements = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submit tasks\n",
    "        futures = [\n",
    "            executor.submit(process_chunk_wrapper, i, chunk_text_str) \n",
    "            for i, chunk_text_str in enumerate(chunks[:4])\n",
    "        ]\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in tqdm.tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing chunks\"):\n",
    "            elements_instance, not_parsed_instance = future.result()\n",
    "            extracted_elements.extend(elements_instance)\n",
    "            not_parsed_elements.extend(not_parsed_instance)\n",
    "\n",
    "    print(\"Parallel processing completed.\")\n",
    "        \n",
    "    # Step 6: Summarize them (element-level)\n",
    "    print(\"Summarizing element instances...\")\n",
    "    element_summary = summarize_element_instances(extracted_elements, USE_OPENAI=USE_OPENAI)\n",
    "\n",
    "    print(f\"Summarized {len(element_summary['summarized_nodes'])} nodes and {len(element_summary['summarized_relationships'])} relationships\\n\\n\")\n",
    "\n",
    "    print(\"Storing backup files...\")\n",
    "    # Backup element_summary, extracted_elements and not_parsed_elements\n",
    "    with open(f'backup_extraction_nodes/element_summary_{doc_id}.json', 'w') as f:\n",
    "        f.write(str(element_summary))\n",
    "    \n",
    "    with open(f'backup_extraction_nodes/extracted_elements_{doc_id}.json', 'w') as f:\n",
    "        f.write(str(extracted_elements))\n",
    "    \n",
    "    with open(f'backup_extraction_nodes/not_parsed_elements_{doc_id}.json', 'w') as f:\n",
    "        f.write(str(not_parsed_elements))\n",
    "    \n",
    "    print(\"Backup files stored.\\n\\n\")\n",
    "\n",
    "    # Step 7: Store the summary\n",
    "    print(\"Storing element summary in Neo4j...\")\n",
    "    with driver.session() as session:\n",
    "        # Step 7: Store the summary\n",
    "        session.execute_write(store_element_summary_in_graph, element_summary, doc_id)\n",
    "    \n",
    "    print(f\"Ingested {len(chunks)} chunks from {pdf_path} into Neo4j under Document {doc_id}\")\n",
    "    return element_summary, extracted_elements, not_parsed_elements\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# QA / RAG PIPELINE (Simplified)\n",
    "##################################################\n",
    "\n",
    "def perform_qa_with_graph(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    A simplified RAG approach:\n",
    "    1) Retrieve relevant chunks from the Neo4j graph (naive text search).\n",
    "    2) Build a context from those chunks.\n",
    "    3) Use either an open model or an OpenAI model for generative answer.\n",
    "\n",
    "    NOTE: This doesn't incorporate full community-based summarization from 2.6.\n",
    "    For a more complete approach, see `answer_query_from_communities()`.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        candidate_chunks = session.read_transaction(retrieve_relevant_chunks, user_query)\n",
    "\n",
    "    # Build the context\n",
    "    context = \"\\n\\n\".join([c[\"text\"] for c in candidate_chunks])\n",
    "\n",
    "    # Use a HuggingFace or OpenAI model for generation\n",
    "    if USE_OPENAI:\n",
    "        # If using OpenAI ChatCompletion:\n",
    "        # openai.api_key = OPENAI_API_KEY\n",
    "        # response = openai.ChatCompletion.create(\n",
    "        #     model=\"gpt-3.5-turbo\",\n",
    "        #     messages=[\n",
    "        #         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        #         {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {user_query}\"}\n",
    "        #     ]\n",
    "        # )\n",
    "        # answer = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        raise NotImplementedError(\"OpenAI ChatCompletion usage not fully implemented here.\")\n",
    "    else:\n",
    "        # Example with a local HF pipeline\n",
    "        qa_pipeline = pipeline(\"text-generation\", model=\"bigscience/bloom-560m\")\n",
    "        prompt = f\"Context: {context}\\nQuestion: {user_query}\\nAnswer:\"\n",
    "        answer_list = qa_pipeline(prompt, max_new_tokens=100, do_sample=True)\n",
    "        if answer_list:\n",
    "            answer = answer_list[0][\"generated_text\"].split(\"Answer:\")[-1].strip()\n",
    "        else:\n",
    "            answer = \"No answer generated.\"\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# MAIN EXECUTION EXAMPLE\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing PDF at data/docs/0704.2547.pdf...\n",
      "Extracted 148193 characters from data/docs/0704.2547.pdf \n",
      "\n",
      "\n",
      "Retrieving metadata for 0704.2547...\n",
      "Metadata: {'title': 'Inferring DNA sequences from mechanical unzipping data: the large-bandwidth case', 'summary': 'The complementary strands of DNA molecules can be separated when stretched\\napart by a force; the unzipping signal is correlated to the base content of the\\nsequence but is affected by thermal and instrumental noise. We consider here\\nthe ideal case where opening events are known to a very good time resolution\\n(very large bandwidth), and study how the sequence can be reconstructed from\\nthe unzipping data. Our approach relies on the use of statistical Bayesian\\ninference and of Viterbi decoding algorithm. Performances are studied\\nnumerically on Monte Carlo generated data, and analytically. We show how\\nmultiple unzippings of the same molecule may be exploited to improve the\\nquality of the prediction, and calculate analytically the number of required\\nunzippings as a function of the bandwidth, the sequence content, the elasticity\\nparameters of the unzipped strands.', 'url': 'http://arxiv.org/abs/0704.2547v1', 'authors': 'Valentina Baldazzi, Serena Bradde, Simona Cocco, Enzo Marinari, Remi Monasson', 'categories': 'q-bio.BM, cond-mat.stat-mech'}\n",
      "\n",
      "\n",
      "Chunked 284 segments from data/docs/0704.2547.pdf \n",
      "\n",
      "\n",
      "Extracting element instances from chunk 0...\n",
      "Extracting element instances from chunk 1...\n",
      "Extracting element instances from chunk 2...\n",
      "Extracting element instances from chunk 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  25%|██▌       | 1/4 [00:06<00:19,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 7 elements from chunk 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  50%|█████     | 2/4 [00:08<00:08,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 5 elements from chunk 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  75%|███████▌  | 3/4 [00:09<00:02,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 7 elements from chunk 1\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 4/4 [00:15<00:00,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 10 elements from chunk 0\n",
      "\n",
      "\n",
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 11 nodes and 7 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n",
      "Ingested 284 chunks from data/docs/0704.2547.pdf into Neo4j under Document 0704.2547\n"
     ]
    }
   ],
   "source": [
    "# 1) Ingest an arXiv PDF (Steps 2.1–2.3)\n",
    "pdf_path = \"data/docs/0704.2547.pdf\"  # Replace with the path to your local arXiv PDF\n",
    "doc_id = \"0704.2547\"           # Arbitrary doc ID for grouping in Neo4j\n",
    "element_summary, extracted_elements, not_parsed_elements=ingest_pdf_into_graph(pdf_path, doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'Decoding Algorithms',\n",
       "  'target': 'Monte Carlo Generated Data',\n",
       "  'relation_type': 'STUDIED_NUMERICALLY_ON',\n",
       "  'weight': 1,\n",
       "  'original_relationships': ['decoding algorithm->Monte Carlo generated data(Studied numerically on)']},\n",
       " {'source': 'DNA Molecules',\n",
       "  'target': 'Genetic Information',\n",
       "  'relation_type': 'SUPPORTS',\n",
       "  'weight': 1,\n",
       "  'original_relationships': ['DNA molecules->genetic information(Support for)']},\n",
       " {'source': 'DNA Molecules',\n",
       "  'target': 'Quality of Prediction',\n",
       "  'relation_type': 'IMPROVES',\n",
       "  'weight': 1,\n",
       "  'original_relationships': ['unzippings->quality of prediction(Exploited to improve)']},\n",
       " {'source': 'Human Genome',\n",
       "  'target': 'Sanger Method',\n",
       "  'relation_type': 'USED_FOR',\n",
       "  'weight': 1,\n",
       "  'original_relationships': ['human genome->Sanger method(Sequencing method used to read DNA)']},\n",
       " {'source': 'Sanger Method',\n",
       "  'target': 'DNA Molecules',\n",
       "  'relation_type': 'USED_TO_READ',\n",
       "  'weight': 1,\n",
       "  'original_relationships': ['Sanger method->DNA molecule(Used to read DNA molecules)']},\n",
       " {'source': 'PCR',\n",
       "  'target': 'DNA Polymerases',\n",
       "  'relation_type': 'INVOLVES',\n",
       "  'weight': 1,\n",
       "  'original_relationships': ['PCR->DNA polymerases(Involves the action of DNA polymerases)']},\n",
       " {'source': 'Research Paper on DNA Sequences',\n",
       "  'target': 'Institutions',\n",
       "  'relation_type': 'AFFILIATED_WITH',\n",
       "  'weight': 1,\n",
       "  'original_relationships': ['arXiv:0704.2547v1->Dipartimento di Fisica, Università di Roma Tor Vergata(Affiliated with)']}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_summary['summarized_relationships']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Decoding Algorithms',\n",
       "  'summary': 'Methods used to interpret or convert encoded data into a readable format, including the Viterbi decoding algorithm and other numerical techniques.',\n",
       "  'original_ids': ['decoding algorithm', 'Viterbi decoding algorithm'],\n",
       "  'type': 'Algorithm',\n",
       "  'keywords': ['decoding', 'algorithm', 'data interpretation']},\n",
       " {'title': 'Monte Carlo Generated Data',\n",
       "  'summary': 'Data generated using Monte Carlo methods, which rely on random sampling to obtain numerical results and are used for performance studies.',\n",
       "  'original_ids': ['Monte Carlo generated data'],\n",
       "  'type': 'Data',\n",
       "  'keywords': ['Monte Carlo', 'random sampling', 'numerical results']},\n",
       " {'title': 'DNA Molecules',\n",
       "  'summary': 'Molecules that carry genetic information in living organisms, supporting genetic information and related to sequence content.',\n",
       "  'original_ids': ['DNA molecules', 'DNA molecule'],\n",
       "  'type': 'Biological Molecule',\n",
       "  'keywords': ['DNA', 'genetic information', 'molecules']},\n",
       " {'title': 'Genetic Information',\n",
       "  'summary': 'Information encoded in the sequences of DNA that determines the characteristics of an organism.',\n",
       "  'original_ids': ['genetic information'],\n",
       "  'type': 'Information',\n",
       "  'keywords': ['genetics', 'DNA', 'information']},\n",
       " {'title': 'Quality of Prediction',\n",
       "  'summary': 'A measure of how accurately a model predicts outcomes based on input data, influenced by processes like unzipping.',\n",
       "  'original_ids': ['quality of prediction'],\n",
       "  'type': 'Metric',\n",
       "  'keywords': ['prediction', 'accuracy', 'model']},\n",
       " {'title': 'Sanger Method',\n",
       "  'summary': 'A traditional strategy for sequencing DNA, used to read DNA molecules and associated with the human genome.',\n",
       "  'original_ids': ['Sanger method'],\n",
       "  'type': 'Method',\n",
       "  'keywords': ['Sanger', 'sequencing', 'DNA']},\n",
       " {'title': 'PCR',\n",
       "  'summary': 'Polymerase Chain Reaction, a method used to amplify DNA, involving DNA polymerases to synthesize new DNA strands.',\n",
       "  'original_ids': ['PCR'],\n",
       "  'type': 'Technique',\n",
       "  'keywords': ['PCR', 'amplification', 'DNA']},\n",
       " {'title': 'Unzipping Signal',\n",
       "  'summary': 'A signal correlated to the base content of the DNA sequence, affected by thermal and instrumental noise.',\n",
       "  'original_ids': ['unzipping signal'],\n",
       "  'type': 'Signal',\n",
       "  'keywords': ['unzipping', 'signal', 'DNA']},\n",
       " {'title': 'Bayesian Inference',\n",
       "  'summary': \"A method of statistical inference using Bayes' theorem to update the probability for a hypothesis as more evidence becomes available.\",\n",
       "  'original_ids': ['Bayesian inference'],\n",
       "  'type': 'Statistical Method',\n",
       "  'keywords': ['Bayesian', 'inference', 'statistics']},\n",
       " {'title': 'Research Paper on DNA Sequences',\n",
       "  'summary': 'A research paper on inferring DNA sequences from mechanical unzipping data, authored by V. Baldazzi, S. Bradde, S. Cocco, E. Marinari, and R. Monasson.',\n",
       "  'original_ids': ['arXiv:0704.2547v1'],\n",
       "  'type': 'Paper',\n",
       "  'keywords': ['DNA', 'research', 'unzipping']},\n",
       " {'title': 'Institutions',\n",
       "  'summary': 'Academic and research institutions in Italy and France associated with the authors of the research paper.',\n",
       "  'original_ids': ['Dipartimento di Fisica, Università di Roma Tor Vergata',\n",
       "   'CNRS-Laboratoire de Physique Statistique de l’ENS',\n",
       "   'CNRS-Laboratoire de Physique Théórique de l’ENS',\n",
       "   'Dipartimento di Fisica and INFN, Università di Roma La Sapienza'],\n",
       "  'type': 'Institution',\n",
       "  'keywords': ['institution', 'research', 'academic']}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_summary['summarized_nodes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "The following steps are not yet implemented.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following steps are not yet implemented.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The following steps are not yet implemented."
     ]
    }
   ],
   "source": [
    "raise NotImplementedError(\"The following steps are not yet implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Community Detection] Placeholder: run Leiden or other community detection.\n",
      "[Community Summaries] Placeholder: gather summaries and do hierarchical summarization.\n"
     ]
    }
   ],
   "source": [
    "# 2) Community detection & summarization (Steps 2.4–2.5)\n",
    "detect_communities()\n",
    "summarize_communities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/k27_kqbn1yvdcwxq44w79hw80000gn/T/ipykernel_28803/2195154659.py:16: DeprecationWarning: read_transaction has been renamed to execute_read\n",
      "  candidate_chunks = session.read_transaction(retrieve_relevant_chunks, user_query)\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Chunk)} {position: line: 2, column: 14, offset: 14} for query: '\\n    MATCH (c:Chunk)\\n    WHERE c.text CONTAINS $user_query\\n    RETURN c.chunk_id AS chunk_id, c.text AS text\\n    LIMIT $limit\\n    '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: text)} {position: line: 3, column: 13, offset: 33} for query: '\\n    MATCH (c:Chunk)\\n    WHERE c.text CONTAINS $user_query\\n    RETURN c.chunk_id AS chunk_id, c.text AS text\\n    LIMIT $limit\\n    '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: chunk_id)} {position: line: 4, column: 14, offset: 72} for query: '\\n    MATCH (c:Chunk)\\n    WHERE c.text CONTAINS $user_query\\n    RETURN c.chunk_id AS chunk_id, c.text AS text\\n    LIMIT $limit\\n    '\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: text)} {position: line: 4, column: 38, offset: 96} for query: '\\n    MATCH (c:Chunk)\\n    WHERE c.text CONTAINS $user_query\\n    RETURN c.chunk_id AS chunk_id, c.text AS text\\n    LIMIT $limit\\n    '\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "OpenAI ChatCompletion usage not fully implemented here.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m user_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the main contributions of the paper?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Simple QA (naive RAG):\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m answer \u001b[38;5;241m=\u001b[39m perform_qa_with_graph(user_query)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 33\u001b[0m, in \u001b[0;36mperform_qa_with_graph\u001b[0;34m(user_query)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Use a HuggingFace or OpenAI model for generation\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m USE_OPENAI:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# If using OpenAI ChatCompletion:\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# openai.api_key = OPENAI_API_KEY\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# answer = response[\"choices\"][0][\"message\"][\"content\"]\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI ChatCompletion usage not fully implemented here.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Example with a local HF pipeline\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     qa_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigscience/bloom-560m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: OpenAI ChatCompletion usage not fully implemented here."
     ]
    }
   ],
   "source": [
    "# 3) Ask a question (Step 2.6 simplified vs. full approach)\n",
    "user_query = \"What are the main contributions of the paper?\"\n",
    "# Simple QA (naive RAG):\n",
    "answer = perform_qa_with_graph(user_query)\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, full approach using community-based QA:\n",
    "# global_answer = answer_query_from_communities(user_query)\n",
    "# print(f\"Global Answer: {global_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
