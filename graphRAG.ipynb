{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install neo4j\n",
    "# !pip install langchain\n",
    "# !pip install PyPDF2\n",
    "# !pip install tiktoken\n",
    "# !pip install openai  # Only if you want to use the OpenAI API\n",
    "# !pip install transformers  # For open (HF) models\n",
    "# !pip install sentence_transformers\n",
    "# !pip install -U langchain-community\n",
    "# !pip install -qU  langchain_milvus\n",
    "# !pip install -U langchain-ollama\n",
    "# !pip install graphdatascience\n",
    "# For advanced community detection with Leiden, you might need external libraries (e.g., igraph, networkx, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama pull llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "# -----------------------\n",
    "# Neo4j Database imports\n",
    "# -----------------------\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# -----------------------\n",
    "# LLM / Embeddings imports\n",
    "# -----------------------\n",
    "# If using HuggingFace transformers:\n",
    "from transformers import pipeline\n",
    "\n",
    "# If using LangChain for retrieval + QA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "from langchain_milvus import Milvus\n",
    "from uuid import uuid4\n",
    "from pymilvus import MilvusClient\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# If you want to use OpenAI, uncomment:\n",
    "import openai\n",
    "openai_model=\"gpt-4o-mini\"\n",
    "\n",
    "# -----------------------\n",
    "# Load environment variables\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# -----------------------\n",
    "# ArXiv API\n",
    "# -----------------------\n",
    "import arxiv\n",
    "\n",
    "# -----------------------\n",
    "# PDF Parsing library\n",
    "# -----------------------\n",
    "import PyPDF2  # or \"pypdf\" if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 1) CONFIGURATION: toggle open vs. OpenAI\n",
    "#############################################\n",
    "\n",
    "USE_OPENAI = True  # Set to True if you want to switch to OpenAI’s ChatGPT\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# For Neo4j:\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.1 SOURCE DOCUMENTS → TEXT CHUNKS\n",
    "##################################################\n",
    "def parse_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract raw text from a PDF file using PyPDF2.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def find_metadata(doc_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    (Step 2.1) Retrieve metadata for a document from a database or API.\n",
    "    \"\"\"\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        id_list=[doc_id]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        result = next(client.results(search))\n",
    "        return \\\n",
    "            {\"title\": result.title, \n",
    "            \"summary\": result.summary, \n",
    "            \"url\": result.entry_id,\n",
    "            \"authors\": ', '.join([a.name for a in result.authors]),\n",
    "            \"categories\": ', '.join(result.categories)\n",
    "            }\n",
    "    except StopIteration:\n",
    "        return {}\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 600, chunk_overlap: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    (Step 2.1) Split text into chunks. \n",
    "    Following the guidance in 2.1, we use a smaller chunk size (e.g., ~600 tokens).\n",
    "    This can improve entity recall at the cost of more LLM calls.\n",
    "    \"\"\"\n",
    "    # Note: the chunk_size is in characters by default using this splitter;\n",
    "    # you may want to adapt to token-based splitting.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# EMBEDDING UTILITIES\n",
    "##################################################\n",
    "\n",
    "def get_hf_embedding_function(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\", device: str = \"mps\", USE_OPENAI: bool = False):\n",
    "    \"\"\"\n",
    "    Returns a function that can generate embeddings using a HuggingFace model.\n",
    "    \"\"\"\n",
    "    if not USE_OPENAI:\n",
    "        hf_embed = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={'device': device})\n",
    "        return hf_embed\n",
    "    else:\n",
    "        def _embeddings(texts: List[str], model_name: str = \"text-embedding-ada-002\") -> List[List[float]]:\n",
    "            response = openai.Embedding.create(\n",
    "                input=texts,\n",
    "                model=model_name\n",
    "            )\n",
    "            embeddings = [item[\"embedding\"] for item in response[\"data\"]]\n",
    "            return embeddings\n",
    "        return _embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# VECTORSTORE UTILITIES\n",
    "##################################################\n",
    "\n",
    "def init_milvus_db(collection_name: str, uri: str, embedding_function):\n",
    "    \"\"\"\n",
    "    Initialize the Milvus database if it does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(uri):\n",
    "        print(f\"Creating database at {uri}\")\n",
    "\n",
    "    vectorstore = Milvus(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=embedding_function,\n",
    "        connection_args={\"uri\": uri},\n",
    "    )\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "# Function to add multiple documents to Milvus\n",
    "def add_documents_to_milvus(docs: list, embedding_function, collection_name: str = \"rag_milvus\", uri: str = \"./vector_db_graphRAG/milvus_ingest.db\"):\n",
    "    \"\"\"\n",
    "    Add multiple documents to the Milvus vector store.\n",
    "\n",
    "    Args:\n",
    "        docs (list): List of tuples containing text and metadata.\n",
    "        collection_name (str): Name of the Milvus collection.\n",
    "        uri (str): URI for the Milvus database.\n",
    "    \n",
    "    # Example usage\n",
    "    docs = [\n",
    "        {\"text\": \"Chunk 1 of the document\", \"metadata\": {\"doc_id\": \"doc_1\", \"chunk\": 1}},\n",
    "        {\"text\": \"Chunk 2 of the document\", \"metadata\": {\"doc_id\": \"doc_1\", \"chunk\": 2}}\n",
    "    ]\n",
    "\n",
    "    add_documents_to_milvus(docs)\n",
    "    \"\"\"\n",
    "    # Initialize the database\n",
    "    vectorstore = init_milvus_db(collection_name, uri, embedding_function)\n",
    "\n",
    "    # Prepare documents\n",
    "    document_list = []\n",
    "    for doc in docs:\n",
    "        text = doc.get(\"text\", \"\")\n",
    "        metadata = doc.get(\"metadata\", {})\n",
    "        document_list.append(Document(page_content=text, metadata=metadata))\n",
    "    \n",
    "    uuids = [str(uuid4()) for _ in range(len(document_list))]\n",
    "\n",
    "    # Add documents to the vector store\n",
    "    vectorstore.add_documents(document_list, ids=uuids)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "# Function to search for similar documents in Milvus\n",
    "def search_milvus(query: str, vectorstore, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Search for similar documents in the Milvus vector store.\n",
    "\n",
    "    Args:\n",
    "        query (str): Text to search for.\n",
    "        vectorstore: Milvus vector store.\n",
    "        top_k (int): Number of similar documents to return.\n",
    "    \"\"\"\n",
    "    return vectorstore.similarity_search_with_score_by_vector(query, top_k=top_k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.2. Text Chunks → Element Instances\n",
    "##################################################\n",
    "\n",
    "def extract_element_instances_from_chunk(\n",
    "    chunk_text: str,\n",
    "    gleaning_rounds: int = 1,\n",
    "    USE_OPENAI: bool = True,\n",
    "    local_llm: str = \"llama3.1\"\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    (Step 2.2) Use an LLM prompt to identify entity references, relationships, and covariates.\n",
    "    - Identifies entities (name, type, description) and relationships.\n",
    "    - Supports multiple rounds of \"gleanings\" to find any missed entities.\n",
    "    \"\"\"\n",
    "    extracted_elements = []\n",
    "    not_parsed = []\n",
    "\n",
    "    # Select the LLM to use\n",
    "    if USE_OPENAI:\n",
    "        llm = ChatOpenAI(model=openai_model, temperature=0.0)\n",
    "    else:\n",
    "        llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    # Base prompt for extracting entities and relationships\n",
    "    base_prompt = (\n",
    "        \"Extract entities and relationships from the following text. \"\n",
    "        \"For each entity, provide its name, type, and description. \"\n",
    "        \"For each relationship, provide the source entity, target entity, and description. \"\n",
    "        \"Text: \\n{chunk_text}\\n\"\n",
    "        \"Output format: List of dictionaries with keys 'entity_name', 'entity_type', 'entity_description', 'relationship'. \"\n",
    "        \"Follow this format in the example: [{{\\\"entity_name\\\": \\\"Alice\\\", \\\"entity_type\\\": \\\"Person\\\", \\\"entity_description\\\": \\\"A person of interest.\\\", \\\"relationship\\\": {{\\\"source_entity\\\": \\\"Alice\\\", \\\"target_entity\\\": \\\"Bob\\\", \\\"description\\\": \\\"Knows\\\"}}}}]\"\n",
    "        \"Return just the list, so that we can parse it.\"\n",
    "        \"No bullet list or asterisks needed.\"\n",
    "        \"It must be a unique list, do NOT separate entities and relationships in different lists.\"\n",
    "    )\n",
    "\n",
    "    # Loop through gleaning rounds\n",
    "    for round_num in range(gleaning_rounds):\n",
    "        prompt = base_prompt.format(chunk_text=chunk_text)\n",
    "\n",
    "        # Send prompt to the selected LLM\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        # Parse the LLM response\n",
    "        new_elements = response.content\n",
    "        new_elements = new_elements.replace('```json','').replace('```','')\n",
    "\n",
    "        # Assume the response is already in JSON format\n",
    "        try:\n",
    "            new_elements = eval(new_elements)  # Convert string to list of dicts\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing LLM output: {e}\")\n",
    "            not_parsed.extend(new_elements)\n",
    "            new_elements = []\n",
    "\n",
    "        # Add new elements to the result\n",
    "        extracted_elements.extend(new_elements)\n",
    "\n",
    "        # Check if gleaning is needed (e.g., ask LLM if entities were missed)\n",
    "        if round_num < gleaning_rounds - 1:\n",
    "            print(\"Asking for validation...\")\n",
    "            validation_prompt = (\n",
    "                \"Were any entities or relationships missed in the previous extraction? \"\n",
    "                \"Answer 'Yes' or 'No'.\"\n",
    "            )\n",
    "            validation_response = llm.invoke(\n",
    "                [ HumanMessage(content=validation_prompt)], \n",
    "                chat_history=\n",
    "                [HumanMessage(content=prompt), SystemMessage(content=new_elements)]\n",
    "            )\n",
    "\n",
    "            # If LLM says 'No', break early\n",
    "            if 'No' in validation_response.content:\n",
    "                break\n",
    "\n",
    "    # Return all extracted elements\n",
    "    return extracted_elements, not_parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.3 ELEMENT INSTANCES → ELEMENT SUMMARIES\n",
    "##################################################\n",
    "\n",
    "summarization_instances_prompt = \"\"\"\n",
    "You are an expert in text summarization and knowledge graph construction. I will provide you with:\n",
    "\n",
    "1. A list of initial nodes, each containing:\n",
    "   - A unique identifier\n",
    "   - A textual description extracted from the source\n",
    "   - Additional properties (optional)\n",
    "\n",
    "2. A list of relationships between these nodes (e.g., an entity \"Einstein\" related to another entity \"Relativity\").\n",
    "\n",
    "Your task is to:\n",
    "- Identify when multiple nodes actually refer to the same entity or concept.\n",
    "- Generate *summarized nodes* by consolidating their textual descriptions and removing duplicates or near-duplicates.\n",
    "- Maintain references to each node's original ID within your summarized node.\n",
    "- Create new relationships among these summarized nodes that reflect the original relationships, but merged and simplified where appropriate.\n",
    "\n",
    "**Important requirements and format details:**\n",
    "1. Each summarized node should have:\n",
    "   - A `summary` field with the merged description.\n",
    "   - A list of `original_ids` that were merged into this new summary node.\n",
    "   - Any relevant `type` or `label` (e.g., Person, Theory, Location) if it can be inferred from the text.\n",
    "   - (Optional) A short list of `keywords` extracted from the descriptions.\n",
    "\n",
    "2. Each relationship should:\n",
    "   - Include `source` and `target` references to the new summarized nodes.\n",
    "   - Provide a `relation_type` (e.g., \"INVENTED\", \"WORKS_ON\", \"LOCATED_IN\", etc.).\n",
    "   - Have a `weight` or `relevance_score` if it can be inferred (e.g., frequency or importance).\n",
    "   - (Optional) Include an `original_relationships` list indicating which original relationships were merged.\n",
    "\n",
    "3. Return the final data in **JSON** format, containing two top-level keys: `summarized_nodes` and `summarized_relationships`.\n",
    "\n",
    "4. Be concise but ensure the summaries and relationships accurately capture the original meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Here is the initial Nodes and Relationships**:\n",
    "\n",
    "{initial_data}\n",
    "\n",
    "---\n",
    "\n",
    "### **Instructions to the LLM**:\n",
    "1. **Identify duplicates or near-duplicates** (e.g., \"Albert Einstein\" and \"Einstein\" might refer to the same entity).\n",
    "2. **Create a new summarized node** that merges the descriptions of \"N1\" and \"N2\" if they represent the same entity (in this case, Albert Einstein).\n",
    "3. **Consolidate relationships** so that if multiple original relationships lead to the same concept, you unify them into a single relationship with an updated weight (e.g., sum or average of the original).\n",
    "4. Provide your final answer in the following **JSON** structure:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"summarized_nodes\": [\n",
    "    {{\n",
    "      \"title\": \"NewTitle1\",\n",
    "      \"summary\": \"Your merged summary text here...\",\n",
    "      \"original_ids\": [\"ExampleID1\", \"ExampleID2\", ...],\n",
    "      \"type\": \"Person\",\n",
    "      \"keywords\": [\"Einstein\", \"relativity\", \"physics\"]\n",
    "    }},\n",
    "    {{\n",
    "      \"title\": \"NewTitle2\",\n",
    "      \"summary\": \"Your summary text here...\",\n",
    "      \"original_ids\": [\"ExampleID3\", \"ExampleID4\"],\n",
    "      \"type\": \"Theory\",\n",
    "      \"keywords\": [\"relativity\", \"physics\"]\n",
    "    }}\n",
    "  ],\n",
    "  \"summarized_relationships\": [\n",
    "    {{\n",
    "      \"source\": \"NewTitle1\",\n",
    "      \"target\": \"NewTitle2\",\n",
    "      \"relation_type\": \"DEVELOPED_OR_ASSOCIATED_WITH\",\n",
    "      \"weight\": 5,\n",
    "      \"original_relationships\": [\"N1->N3(DEVELOPED)\", \"N2->N3(ASSOCIATED_WITH)\"]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "\n",
    "Please **only** output valid JSON in the format described above, without additional commentary so that we can parse it correctly. \n",
    "Make sure to capture the essence of each original node and relationship in your summarized version.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_element_instances(\n",
    "    element_instances: List[Dict[str, Any]], \n",
    "    USE_OPENAI: bool = True,\n",
    "    local_llm: str = \"llama3.1\") -> str:\n",
    "    \"\"\"\n",
    "    (Step 2.3) Summarize extracted nodes/relationships into a single descriptive block of text\n",
    "    for each chunk. This is an additional LLM-based summarization step, forming \"element summaries.\"\n",
    "    \"\"\"\n",
    "    # Select the LLM to use\n",
    "    if USE_OPENAI:\n",
    "        llm = ChatOpenAI(model=openai_model, temperature=0.0)\n",
    "    else:\n",
    "        llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    prompt = summarization_instances_prompt.format(initial_data=element_instances)\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    response = response.content\n",
    "\n",
    "    try:\n",
    "        response = response.replace('```json','').replace('```','')\n",
    "        response = eval(response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing LLM output: {e}\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_element_summary_in_graph(tx, data: Dict[str, Any], doc_id: str, chunks_bounds: tuple):\n",
    "    \"\"\"\n",
    "    Load the summarized graph data into Neo4j.\n",
    "\n",
    "    Args:\n",
    "        tx: Neo4j transaction object.\n",
    "        data (Dict[str, Any]): Summarized nodes and relationships.\n",
    "        doc_id (str): Document ID.\n",
    "        chunks_bounds (tuple): Tuple containing the start and end positions\n",
    "            of the text chunk in the original document.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creazione dei nodi\n",
    "    for node in data[\"summarized_nodes\"]:\n",
    "        query_create_node = \"\"\"\n",
    "        CREATE (n:SummarizedNode {\n",
    "            title: $title,\n",
    "            summary: $summary,\n",
    "            original_ids: $original_ids,\n",
    "            type: $type,\n",
    "            keywords: $keywords,\n",
    "            doc_id : $doc_id,\n",
    "            chunks_lower_bound: $chunks_lower_bound,\n",
    "            chunks_upper_bound: $chunks_upper_bound\n",
    "        })\n",
    "        \"\"\"\n",
    "        tx.run(\n",
    "            query_create_node,\n",
    "            title=node.get(\"title\"),\n",
    "            summary=node.get(\"summary\"),\n",
    "            original_ids=node.get(\"original_ids\"),\n",
    "            type=node.get(\"type\"),\n",
    "            keywords=node.get(\"keywords\"),\n",
    "            doc_id=doc_id,\n",
    "            chunks_lower_bound=chunks_bounds[0],\n",
    "            chunks_upper_bound=chunks_bounds[1]\n",
    "        )\n",
    "\n",
    "    # Creazione delle relazioni\n",
    "    for rel in data[\"summarized_relationships\"]:\n",
    "        query_create_rel = f\"\"\"\n",
    "        MATCH (source:SummarizedNode {{title: $source_id}})\n",
    "        MATCH (target:SummarizedNode {{title: $target_id}})\n",
    "        CREATE (source)-[:RELATIONSHIP_TYPE {{\n",
    "            type : $relation_type,\n",
    "            weight: $weight,\n",
    "            original_relationships: $original_rels\n",
    "        }}]->(target)\n",
    "        \"\"\"\n",
    "        tx.run(\n",
    "            query_create_rel,\n",
    "            source_id=rel[\"source\"],\n",
    "            target_id=rel[\"target\"],\n",
    "            relation_type=rel[\"relation_type\"],\n",
    "            weight=rel.get(\"weight\", 1),  # default=1 if not provided\n",
    "            original_rels=rel.get(\"original_relationships\", [])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.4 ELEMENT SUMMARIES → GRAPH COMMUNITIES\n",
    "##################################################\n",
    "\n",
    "class CommunityDetection:\n",
    "    def __init__(self, driver: Any = None, uri: str = '', user: str = '', password: str = ''):\n",
    "        \"\"\"\n",
    "        Initialize the CommunityDetection class with a Neo4j driver or connection details.\n",
    "\n",
    "        Args:\n",
    "            driver (Any): An existing Neo4j driver instance. If provided, `uri`, `user`, and `password` are ignored.\n",
    "            uri (str): The URI for the Neo4j database.\n",
    "            user (str): The username for the Neo4j database.\n",
    "            password (str): The password for the Neo4j database.\n",
    "        \"\"\"\n",
    "        # If a driver is provided, use it; otherwise, create a new driver instance\n",
    "        self.driver = driver or GraphDatabase.driver(uri, auth=(user, password))\n",
    "        self.graph_name = 'summarizedGraph'\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def project_graph(self, relationship_weight_property: str = \"weight\") -> None:\n",
    "        \"\"\"\n",
    "        Projects the graph into memory for analysis.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(\n",
    "                f\"\"\"\n",
    "                CALL gds.graph.project(\n",
    "                    '{self.graph_name}',\n",
    "                    'SummarizedNode',\n",
    "                    {{ RELATIONSHIP_TYPE: {{ orientation: 'UNDIRECTED', properties: ['{relationship_weight_property}'] }} }}\n",
    "                )\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "\n",
    "    def set_communities(self, relationship_weight_property: str = \"weight\") -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Sets the community IDs directly into the graph database.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\n",
    "                f\"\"\"\n",
    "                CALL gds.leiden.stream(\n",
    "                    '{self.graph_name}', \n",
    "                    {{ relationshipWeightProperty: '{relationship_weight_property}' }})\n",
    "                YIELD nodeId, communityId\n",
    "                SET gds.util.asNode(nodeId).communityId = communityId\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    def retrieve_communities(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve the community assignments from the graph.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\n",
    "                f\"\"\"\n",
    "                MATCH (n:SummarizedNode)-[r:RELATIONSHIP_TYPE]->(m:SummarizedNode)\n",
    "                RETURN\n",
    "                    n.communityId AS communityId,\n",
    "                    n.title AS nodeTitle,      \n",
    "                    n.summary AS nodeSummary,\n",
    "                    n.keywords AS nodeKeywords,\n",
    "                    r.type AS relationshipType,\n",
    "                    r.weight AS relationshipWeight,\n",
    "                    m.title AS targetNodeTitle\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "            communities = {}\n",
    "            for record in result:\n",
    "                community_id = record[\"communityId\"]\n",
    "                node_title = record[\"nodeTitle\"]\n",
    "                node_summary = record[\"nodeSummary\"]\n",
    "                node_keywords = record[\"nodeKeywords\"]\n",
    "                relationship_type = record[\"relationshipType\"]\n",
    "                relationship_weight = record[\"relationshipWeight\"]\n",
    "                target_node_title = record[\"targetNodeTitle\"]\n",
    "                \n",
    "                if community_id not in communities:\n",
    "                    communities[community_id] = {\"nodes\": [], \"relationships\": []}\n",
    "                \n",
    "                communities[community_id][\"nodes\"].append({\n",
    "                    \"title\": node_title,\n",
    "                    \"summary\": node_summary,\n",
    "                    \"keywords\": node_keywords\n",
    "                })\n",
    "                \n",
    "                communities[community_id][\"relationships\"].append({\n",
    "                    \"source\": node_title,\n",
    "                    \"target\": target_node_title,\n",
    "                    \"type\": relationship_type,\n",
    "                    \"weight\": relationship_weight\n",
    "                })\n",
    "\n",
    "            return communities\n",
    "\n",
    "    def drop_graph(self) -> None:\n",
    "        \"\"\"\n",
    "        Drops the graph from memory.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(f\"CALL gds.graph.drop('{self.graph_name}') YIELD graphName\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2.5 GRAPH COMMUNITIES → COMMUNITY SUMMARIES\n",
    "##################################################\n",
    "\n",
    "summary_community_prompt = \"\"\"\n",
    "You are an expert summarizer helping to create a concise “community report” from a list of related nodes. Each node has a title, a summary, and keywords. All these nodes belong to the same community, meaning they share a common theme, topic, or set of closely related ideas. Moreover, you are provided with a list of relationships between these nodes, indicating how they are connected or interact with each other.\n",
    "\n",
    "Here is the list of nodes for this community:\n",
    "\n",
    "{nodes}\n",
    "\n",
    "And here are the relationships between these nodes:\n",
    "\n",
    "{relationships}\n",
    "\n",
    "Please read through the nodes and relationships and then produce a coherent summary describing:\n",
    "1. The main topics, themes, or domains covered by the nodes in this community.\n",
    "2. Any notable or central nodes and why they are important.\n",
    "3. How the nodes interrelate: highlight significant relationships and mention if there are strong connections (high weight).\n",
    "4. Overall, what makes this community distinct or interesting?\n",
    "\n",
    "- Aim for a concise yet informative text, written in a clear paragraph style.\n",
    "- You may group related nodes together and mention prominent links or patterns.\n",
    "- Use plain English. Avoid overly technical language unless it is necessary to describe the domain.\n",
    "\n",
    "4. Provide your final answer in the following **JSON** structure:\n",
    "\n",
    "```json\n",
    "{{\n",
    "    \"title\": a title capable to summarize the community,\n",
    "    \"community_summary\": a single, cohesive summary that helps a reader quickly understand the core content of this community. You do NOT need to repeat every node’s individual summary verbatim. Instead, synthesize the most relevant information into a unified overview.\n",
    "    \"keywords\": a list of keywords that capture the main topics or themes of this community.\n",
    "}}\n",
    "```\n",
    "\n",
    "Please **only** output valid JSON in the format described above, without additional commentary so that we can parse it correctly. \n",
    "\n",
    "Begin now.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_communities(\n",
    "    communities: Dict[int, List[Dict[str, Any]]],\n",
    "    USE_OPENAI: bool = True,\n",
    "    local_llm: str = \"llama3.1\",\n",
    ") -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    (Step 2.5) Summarize each community (or sub-community in a hierarchical approach).\n",
    "    - Gather all element summaries (nodes, edges, covariates) in that community.\n",
    "    - Summarize them, potentially chunking if they don't fit in an LLM context window.\n",
    "\n",
    "    Args:\n",
    "        communities (dict): A dictionary where keys are community IDs and values are lists of nodes with 'title', 'summary', and 'keywords' and a list of relationships with 'source', 'target', 'type', and 'weight'.\n",
    "        USE_OPENAI (bool): Whether to use OpenAI or a local LLM.\n",
    "        local_llm (str): The name of the local LLM model.\n",
    "\n",
    "    Returns:\n",
    "        dict: Summaries for each community.\n",
    "    \"\"\"\n",
    "    # Select the LLM to use\n",
    "    if USE_OPENAI:\n",
    "        llm = ChatOpenAI(model=openai_model, temperature=0.0)\n",
    "    else:\n",
    "        llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    summaries = {}\n",
    "\n",
    "    for community_id, data in tqdm.tqdm(communities.items()):\n",
    "        # Extract nodes and relationships\n",
    "        nodes = data[\"nodes\"]\n",
    "        relationships = data[\"relationships\"]\n",
    "\n",
    "        # Prepare prompt content\n",
    "        node_descriptions = \"\\n\".join([f\"Title: {node['title']}, Summary: {node['summary']}, Keywords: {', '.join(node['keywords'])}\" for node in nodes])\n",
    "        relationships = \"\\n\".join([f\"Source: {rel['source']}, Target: {rel['target']}, Type: {rel['type']}, Weight: {rel['weight']}\" for rel in relationships])\n",
    "        prompt = summary_community_prompt.format(nodes=node_descriptions, relationships=relationships)\n",
    "\n",
    "        # Generate summary\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        response = response.content.replace('```json','').replace('```','')\n",
    "\n",
    "        try:\n",
    "            summaries[community_id] = eval(response)\n",
    "        except Exception as e:\n",
    "            summaries[community_id] = [f\"Error generating summary: {str(e)}\", response]\n",
    "\n",
    "    return summaries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# MAIN FUNCTION: 2.6 COMMUNITY SUMMARIES → COMMUNITY ANSWERS → GLOBAL ANSWER\n",
    "###################################################\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# UTILITY: Prompt to get partial answer + helpfulness score\n",
    "# -------------------------------------------------------------------------\n",
    "PARTIAL_ANSWER_PROMPT = \"\"\"\\\n",
    "You have a user query:\n",
    "\\\"\\\"\\\"{user_query}\\\"\\\"\\\"\n",
    "\n",
    "Below is a chunk of text from a community summary that may or may not be relevant to the query:\n",
    "\\\"\\\"\\\"{chunk_text}\\\"\\\"\\\"\n",
    "\n",
    "1) Please provide a concise partial answer (if any) relevant to the query, based on the chunk above.\n",
    "   If the chunk is irrelevant, you can say \"No relevant info here.\"\n",
    "2) Provide a helpfulness score from 0 to 100 (integer), indicating how much this chunk helps answer the query. \n",
    "   0 = not relevant at all, 100 = extremely helpful.\n",
    "\n",
    "Output your response **only** in valid JSON format like:\n",
    "{{\n",
    "  \"partial_answer\": \"...\",\n",
    "  \"helpfulness_score\": ...\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_from_communities(\n",
    "    user_query: str,\n",
    "    community_summaries: Dict[int, str],\n",
    "    USE_OPENAI: bool = True,\n",
    "    local_llm: str = \"llama3.1\",\n",
    "    max_context_tokens: int = 1000\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    (Step 2.6) Use the hierarchical community summaries to answer a user query globally.\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): The question asked by the user.\n",
    "        community_summaries (Dict[int, str]): A dict of {community_id: summary_text}.\n",
    "        USE_OPENAI (bool): Whether to use OpenAI or a local LLM.\n",
    "        local_llm (str): The local LLM name if not using OpenAI.\n",
    "        max_context_tokens (int): Approx token limit for each chunk.\n",
    "\n",
    "    Returns:\n",
    "        str: The final global answer.\n",
    "    \n",
    "    High-level process:\n",
    "      1) For each community summary:\n",
    "         - Chunk the text (so we don't exceed context window).\n",
    "      2) For each chunk:\n",
    "         - Ask the LLM for a partial answer + helpfulness score.\n",
    "      3) Sort partial answers by score.\n",
    "      4) Combine top partial answers into a final context.\n",
    "      5) Ask the LLM for a final answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # 0) Select which LLM to use\n",
    "    # -------------------------\n",
    "    if USE_OPENAI:\n",
    "        # Example with OpenAI\n",
    "        llm = ChatOpenAI(model=openai_model, temperature=0.0)\n",
    "    else:\n",
    "        # Example with Ollama or a local Hugging Face model\n",
    "        llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # 1) Chunk each community summary\n",
    "    # ----------------------------------------------\n",
    "    chunked_texts = []\n",
    "    for comm_id, summary_text in community_summaries.items():\n",
    "        # Break down the summary into smaller pieces\n",
    "        chunks = chunk_text(summary_text, chunk_size=max_context_tokens)\n",
    "        for c in chunks:\n",
    "            chunked_texts.append((comm_id, c))\n",
    "        \n",
    "    # Shuffl\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # 2) For each chunk, get partial answer + score\n",
    "    # ----------------------------------------------\n",
    "    partial_answers = []\n",
    "    marks = []\n",
    "    for comm_id, text_chunk in chunked_texts:\n",
    "        # Build the prompt\n",
    "        prompt = PARTIAL_ANSWER_PROMPT.format(\n",
    "            user_query=user_query,\n",
    "            chunk_text=text_chunk\n",
    "        )\n",
    "       \n",
    "        # LLM call\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        raw_content = response.content.strip()\n",
    "\n",
    "        # Attempt to parse JSON response\n",
    "        try:\n",
    "            # Clean up any trailing text\n",
    "            first_brace = raw_content.find('{')\n",
    "            last_brace = raw_content.rfind('}')\n",
    "            json_str = raw_content[first_brace:last_brace+1]\n",
    "\n",
    "            parsed = eval(json_str)  # or use json.loads if strictly valid\n",
    "            partial_answer = parsed.get(\"partial_answer\", \"No relevant info here.\")\n",
    "            score = parsed.get(\"helpfulness_score\", 0)\n",
    "        except Exception as e:\n",
    "            partial_answer = \"Parsing error or no relevant info.\"\n",
    "            score = 0\n",
    "\n",
    "        # Store result\n",
    "        partial_answers.append((partial_answer, score))\n",
    "        marks.append((comm_id, score))\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # 3) Sort partial answers by score (descending)\n",
    "    # ----------------------------------------------\n",
    "    partial_answers.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # 4) Combine top partial answers into a final context\n",
    "    #    We'll do a simple cutoff if we have too many\n",
    "    # ----------------------------------------------\n",
    "    final_context = []\n",
    "    used_chars = 0\n",
    "\n",
    "    for ans, sc in partial_answers:\n",
    "        # Add some label or delimiter if needed\n",
    "        if sc==0:\n",
    "            continue\n",
    "        \n",
    "        snippet = f\"PartialAnswer(Score={sc}): {ans}\\n\"\n",
    "        if used_chars + len(snippet) <= max_context_tokens * 4:\n",
    "            final_context.append(snippet)\n",
    "            used_chars += len(snippet)\n",
    "        else:\n",
    "            break  # no more space in final context\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # 5) Produce a final answer from the LLM\n",
    "    # ----------------------------------------------\n",
    "    final_prompt = f\"\"\"\\\n",
    "We have these partial answers and their helpfulness scores:\n",
    "\n",
    "{''.join(final_context)}\n",
    "\n",
    "User Query: {user_query}\n",
    "\n",
    "Based on these partial answers, please produce a single, coherent, and well-structured final answer.\n",
    "Feel free to synthesize or refine the information. If there's conflicting info, do your best to clarify.\n",
    "\n",
    "Respond in plain text.\n",
    "\"\"\"\n",
    "\n",
    "    # Final LLM call\n",
    "    final_response = llm.invoke([HumanMessage(content=final_prompt)])\n",
    "    global_answer = final_response.content.strip()\n",
    "\n",
    "    return global_answer, partial_answers, marks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 5) INGEST PDF -> STORE IN GRAPH (Putting Steps 2.1 and 2.2+ in context)\n",
    "##################################################\n",
    "\n",
    "def ingest_pdf_into_graph(pdf_path: str, doc_id: str, embed_opt: bool = True, BATCH_SIZE: int = 25):\n",
    "    \"\"\"\n",
    "    1) Parse PDF into raw text.\n",
    "    2) Chunk it (Step 2.1).\n",
    "    3) Generate embeddings for each chunk.\n",
    "    4) Store chunk nodes in Neo4j.\n",
    "    5) For each chunk, call LLM to extract element instances (Step 2.2).\n",
    "    6) Summarize them into a single descriptive block (Step 2.3).\n",
    "    7) Store the block in Neo4j for further community detection.\n",
    "    \"\"\"\n",
    "    # Step 1: Parse PDF\n",
    "    print(f\"Parsing PDF at {pdf_path}...\")\n",
    "    raw_text = parse_pdf(pdf_path)\n",
    "    print(f\"Extracted {len(raw_text)} characters from {pdf_path} \\n\\n\")\n",
    "\n",
    "    # Step 1.1: Retrieve metadata\n",
    "    print(f\"Retrieving metadata for {doc_id}...\")\n",
    "    metadata = find_metadata(doc_id)\n",
    "    print(f\"Metadata: {metadata}\\n\\n\")\n",
    "\n",
    "    # Step 2: Chunk the text (default chunk_size=600 for improved recall)\n",
    "    chunks = chunk_text(raw_text)\n",
    "    print(f\"Chunked {len(chunks)} segments from {pdf_path} \\n\\n\")\n",
    "\n",
    "    # Step 3: Embeddings\n",
    "    if embed_opt:\n",
    "        print(\"Generating embeddings for each chunk...\")\n",
    "        embed_fn = get_hf_embedding_function(USE_OPENAI=False)\n",
    "\n",
    "        print(\"Storing chunks in Milvus...\")\n",
    "        vectorstore = add_documents_to_milvus([\n",
    "            {\n",
    "                \"text\": chunk, \n",
    "                \"metadata\": {\n",
    "                    \"doc_id\": doc_id, \n",
    "                    \"chunk\": i, \n",
    "                    **metadata\n",
    "                    }\n",
    "            } for i, chunk in enumerate(chunks)], embed_fn)\n",
    "        print(f\"Stored {len(chunks)} chunks in Milvus under Document {doc_id} \\n\\n\")\n",
    "\n",
    "\n",
    "    # Step 4: Store chunk nodes in Neo4j\n",
    "    # Process chunks in parallel with batch size of 25\n",
    "    for lim in tqdm.tqdm(range(0, len(chunks), BATCH_SIZE), desc=\"Processing chunks in parallel\"):\n",
    "        extracted_elements = []\n",
    "        not_parsed_elements = []\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            # Submit tasks\n",
    "            futures = [\n",
    "                executor.submit(extract_element_instances_from_chunk, chunk_text_str, USE_OPENAI=USE_OPENAI)\n",
    "                for i, chunk_text_str in enumerate(chunks[lim:lim+BATCH_SIZE])\n",
    "            ]\n",
    "            \n",
    "            # Process results as they complete\n",
    "            for future in tqdm.tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing chunks\"):\n",
    "                elements_instance, not_parsed_instance = future.result()\n",
    "                extracted_elements.extend(elements_instance)\n",
    "                not_parsed_elements.extend(not_parsed_instance)\n",
    "\n",
    "        print(\"Parallel processing completed.\")\n",
    "            \n",
    "        # Step 6: Summarize them (element-level)\n",
    "        print(\"Summarizing element instances...\")\n",
    "        element_summary = summarize_element_instances(extracted_elements, USE_OPENAI=USE_OPENAI)\n",
    "\n",
    "        print(f\"Summarized {len(element_summary['summarized_nodes'])} nodes and {len(element_summary['summarized_relationships'])} relationships\\n\\n\")\n",
    "\n",
    "        print(\"Storing backup files...\")\n",
    "        # Backup element_summary, extracted_elements and not_parsed_elements\n",
    "        os.makedirs('backup_extraction_nodes/'+doc_id+'/element_summary', exist_ok=True)\n",
    "        os.makedirs('backup_extraction_nodes/'+doc_id+'/extracted_elements', exist_ok=True)\n",
    "        os.makedirs('backup_extraction_nodes/'+doc_id+'/not_parsed_elements', exist_ok=True)\n",
    "\n",
    "        with open(f'backup_extraction_nodes/{doc_id}/element_summary/{lim}.json', 'w') as f:\n",
    "            f.write(str(element_summary))\n",
    "        \n",
    "        with open(f'backup_extraction_nodes/{doc_id}/extracted_elements/{lim}.json', 'w') as f:\n",
    "            f.write(str(extracted_elements))\n",
    "        \n",
    "        with open(f'backup_extraction_nodes/{doc_id}/not_parsed_elements/{lim}.json', 'w') as f:\n",
    "            f.write(str(not_parsed_elements))\n",
    "        \n",
    "        print(\"Backup files stored.\\n\\n\")\n",
    "\n",
    "        # Step 7: Store the summary\n",
    "        print(\"Storing element summary in Neo4j...\")\n",
    "        with driver.session() as session:\n",
    "            # Step 7: Store the summary\n",
    "            session.execute_write(store_element_summary_in_graph, element_summary, doc_id, (lim, lim+BATCH_SIZE))\n",
    "    \n",
    "    print(f\"Ingested {len(chunks)} chunks from {pdf_path} into Neo4j under Document {doc_id}\")\n",
    "    return True\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# MAIN EXECUTION EXAMPLE\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = 'data/docs/'\n",
    "\n",
    "# Retrieve all the docs in PDF_PATH\n",
    "docs=sorted([f for f in os.listdir(PDF_PATH) if f.endswith('.pdf')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_docs=docs[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 0710.0845.pdf...\n",
      "Parsing PDF at data/docs/0710.0845.pdf...\n",
      "Extracted 91057 characters from data/docs/0710.0845.pdf \n",
      "\n",
      "\n",
      "Retrieving metadata for 0710.0845...\n",
      "Metadata: {'title': 'The nested Chinese restaurant process and Bayesian nonparametric inference of topic hierarchies', 'summary': 'We present the nested Chinese restaurant process (nCRP), a stochastic process\\nwhich assigns probability distributions to infinitely-deep,\\ninfinitely-branching trees. We show how this stochastic process can be used as\\na prior distribution in a Bayesian nonparametric model of document collections.\\nSpecifically, we present an application to information retrieval in which\\ndocuments are modeled as paths down a random tree, and the preferential\\nattachment dynamics of the nCRP leads to clustering of documents according to\\nsharing of topics at multiple levels of abstraction. Given a corpus of\\ndocuments, a posterior inference algorithm finds an approximation to a\\nposterior distribution over trees, topics and allocations of words to levels of\\nthe tree. We demonstrate this algorithm on collections of scientific abstracts\\nfrom several journals. This model exemplifies a recent trend in statistical\\nmachine learning--the use of Bayesian nonparametric methods to infer\\ndistributions on flexible data structures.', 'url': 'http://arxiv.org/abs/0710.0845v3', 'authors': 'David M. Blei, Thomas L. Griffiths, Michael I. Jordan', 'categories': 'stat.ML'}\n",
      "\n",
      "\n",
      "Chunked 186 segments from data/docs/0710.0845.pdf \n",
      "\n",
      "\n",
      "Generating embeddings for each chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/k27_kqbn1yvdcwxq44w79hw80000gn/T/ipykernel_28366/1450880380.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf_embed = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={'device': device})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing chunks in Milvus...\n",
      "Stored 186 chunks in Milvus under Document 0710.0845 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:35<00:00,  1.42s/it]s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n",
      "Summarized 9 nodes and 7 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel:  12%|█▎        | 1/8 [01:00<07:00, 60.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing LLM output: unterminated string literal (detected at line 1) (<string>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:19<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel:  25%|██▌       | 2/8 [01:34<04:30, 45.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 7 nodes and 6 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:20<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel:  38%|███▊      | 3/8 [02:08<03:20, 40.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 8 nodes and 4 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing LLM output: unmatched '}' (<string>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:22<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel:  50%|█████     | 4/8 [02:57<02:53, 43.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 11 nodes and 6 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing LLM output: source code string cannot contain null bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:35<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel:  62%|██████▎   | 5/8 [03:48<02:18, 46.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 9 nodes and 5 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:59<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel:  75%|███████▌  | 6/8 [05:04<01:52, 56.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 7 nodes and 6 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:36<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel:  88%|████████▊ | 7/8 [05:59<00:55, 55.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 8 nodes and 6 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 11/11 [00:27<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel: 100%|██████████| 8/8 [06:56<00:00, 52.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 20 nodes and 15 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n",
      "Ingested 186 chunks from data/docs/0710.0845.pdf into Neo4j under Document 0710.0845\n",
      "Finished processing document 0710.0845.pdf.\n",
      "Processing document 0801.1223.pdf...\n",
      "Parsing PDF at data/docs/0801.1223.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 46562 characters from data/docs/0801.1223.pdf \n",
      "\n",
      "\n",
      "Retrieving metadata for 0801.1223...\n",
      "Metadata: {'title': 'The c2d Spitzer spectroscopy survey of ices around low-mass young stellar objects, III: CH4', 'summary': 'CH4 is proposed to be the starting point of a rich organic chemistry. Solid\\nCH4 abundances have previously been determined mostly toward high mass star\\nforming regions. Spitzer/IRS now provides a unique opportunity to probe solid\\nCH4 toward low mass star forming regions as well. Infrared spectra from the\\nSpitzer Space Telescope are presented to determine the solid CH4 abundance\\ntoward a large sample of low mass young stellar objects. 25 out of 52 ice\\nsources in the $c2d$ (cores to disks) legacy have an absorption feature at 7.7\\num, attributed to the bending mode of solid CH4. The solid CH4 / H2O abundances\\nare 2-8%, except for three sources with abundances as high as 11-13%. These\\nlatter sources have relatively large uncertainties due to small total ice\\ncolumn densities. Toward sources with H2O column densities above 2E18 cm-2, the\\nCH4 abundances (20 out of 25) are nearly constant at 4.7+/-1.6%. Correlation\\nplots with solid H2O, CH3OH, CO2 and CO column densities and abundances\\nrelative to H2O reveal a closer relationship of solid CH4 with CO2 and H2O than\\nwith solid CO and CH3OH. The inferred solid CH4 abundances are consistent with\\nmodels where CH4 is formed through sequential hydrogenation of C on grain\\nsurfaces. Finally the equal or higher abundances toward low mass young stellar\\nobjects compared with high mass objects and the correlation studies support\\nthis formation pathway as well, but not the two competing theories: formation\\nfrom CH3OH and formation in gas phase with subsequent freeze-out.', 'url': 'http://arxiv.org/abs/0801.1223v1', 'authors': 'Karin I. Oberg, A. C. Adwin Boogert, Klaus M. Pontoppidan, Geoffrey A. Blake, Neal J. Evans, Fred Lahuis, Ewine F. van Dishoeck', 'categories': 'astro-ph'}\n",
      "\n",
      "\n",
      "Chunked 99 segments from data/docs/0801.1223.pdf \n",
      "\n",
      "\n",
      "Generating embeddings for each chunk...\n",
      "Storing chunks in Milvus...\n",
      "Stored 99 chunks in Milvus under Document 0801.1223 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:19<00:00,  1.26it/s]s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel:  25%|██▌       | 1/4 [00:46<02:18, 46.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 9 nodes and 8 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:16<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel:  50%|█████     | 2/4 [01:18<01:16, 38.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 6 nodes and 4 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:27<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel:  75%|███████▌  | 3/4 [02:04<00:41, 41.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 8 nodes and 6 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 24/24 [01:02<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel: 100%|██████████| 4/4 [03:23<00:00, 50.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 8 nodes and 6 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n",
      "Ingested 99 chunks from data/docs/0801.1223.pdf into Neo4j under Document 0801.1223\n",
      "Finished processing document 0801.1223.pdf.\n",
      "Processing document 0804.4409.pdf...\n",
      "Parsing PDF at data/docs/0804.4409.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 34382 characters from data/docs/0804.4409.pdf \n",
      "\n",
      "\n",
      "Retrieving metadata for 0804.4409...\n",
      "Metadata: {'title': 'Thermal conductivity and phase separation of the crust of accreting neutron stars', 'summary': 'Recently, crust cooling times have been measured for neutron stars after\\nextended outbursts. These observations are very sensitive to the thermal\\nconductivity $\\\\kappa$ of the crust and strongly suggest that $\\\\kappa$ is large.\\nWe perform molecular dynamics simulations of the structure of the crust of an\\naccreting neutron star using a complex composition that includes many\\nimpurities. The composition comes from simulations of rapid proton capture\\nnucleosynthesys followed by electron captures. We find that the thermal\\nconductivity is reduced by impurity scattering. In addition, we find phase\\nseparation. Some impurities with low atomic number $Z$ are concentrated in a\\nsubregion of the simulation volume. For our composition, the solid crust must\\nseparate into regions of different compositions. This could lead to an\\nasymmetric star with a quadrupole deformation that radiates gravitational\\nwaves. Observations of crust cooling can constrain impurity concentrations.', 'url': 'http://arxiv.org/abs/0804.4409v3', 'authors': 'C. J. Horowitz, O. L. Caballero, D. K. Berry', 'categories': 'astro-ph, nucl-th'}\n",
      "\n",
      "\n",
      "Chunked 67 segments from data/docs/0804.4409.pdf \n",
      "\n",
      "\n",
      "Generating embeddings for each chunk...\n",
      "Storing chunks in Milvus...\n",
      "Stored 67 chunks in Milvus under Document 0804.4409 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:19<00:00,  1.26it/s]s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel:  33%|███▎      | 1/3 [00:36<01:12, 36.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 6 nodes and 5 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 25/25 [00:17<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel:  67%|██████▋   | 2/3 [01:15<00:38, 38.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 10 nodes and 7 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 17/17 [00:26<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing completed.\n",
      "Summarizing element instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in parallel: 100%|██████████| 3/3 [02:00<00:00, 40.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 8 nodes and 5 relationships\n",
      "\n",
      "\n",
      "Storing backup files...\n",
      "Backup files stored.\n",
      "\n",
      "\n",
      "Storing element summary in Neo4j...\n",
      "Ingested 67 chunks from data/docs/0804.4409.pdf into Neo4j under Document 0804.4409\n",
      "Finished processing document 0804.4409.pdf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in filtered_docs:\n",
    "    print(f\"Processing document {doc}...\")\n",
    "    doc_id = doc.replace('.pdf', '')\n",
    "\n",
    "    if ingest_pdf_into_graph(PDF_PATH+doc, doc_id, embed_opt=True):\n",
    "        print(f\"Finished processing document {doc}.\")\n",
    "        continue\n",
    "    else:\n",
    "        raise Exception(f\"Error processing document {doc}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the community detection class\n",
    "detector = CommunityDetection(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Community detection & summarization (Steps 2.4–2.5)\n",
    "\n",
    "# Project the graph for community detection\n",
    "detector.project_graph()\n",
    "\n",
    "# Set community IDs in the graph\n",
    "detector.set_communities()\n",
    "\n",
    "# Drop the graph from memory\n",
    "detector.drop_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [03:05<00:00,  3.15s/it]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the communities\n",
    "communities = detector.retrieve_communities()\n",
    "community_summaries=summarize_communities(communities, USE_OPENAI=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store json file with community summaries\n",
    "os.makedirs('backup_extraction_nodes/', exist_ok=True)\n",
    "with open(f'backup_extraction_nodes/community_summaries.json', 'w') as f:\n",
    "    f.write(str(community_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for the community summaries and store them in Milvus\n",
    "embed_fn = get_hf_embedding_function(USE_OPENAI=False)\n",
    "vectorstore_community_summaries = add_documents_to_milvus([\n",
    "    {\n",
    "        \"text\": summary[\"community_summary\"], \n",
    "        \"metadata\": {\n",
    "            \"doc_id\": \"community_summaries\",\n",
    "            \"community_id\": id,\n",
    "            \"title\": summary[\"title\"],\n",
    "            \"keywords\": \", \".join(summary[\"keywords\"]),\n",
    "            }\n",
    "    } for id, summary in community_summaries.items()], embed_fn, collection_name=\"community_summaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.05$ for a single document of 21 pages with 89 chunks and 10 nodes and 7 relationships extracted. Morevoer this price include the generation of the community summaries of 2 documents.\n",
    "\n",
    "3 min 22 sec for a single document.\n",
    "2 min 9 sec for summary extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.20$ for a 3 documents \n",
    "\n",
    "12 min 49 sec for a single document.\n",
    "3 min 5 sec for summary extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Stop here",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStop here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Stop here"
     ]
    }
   ],
   "source": [
    "raise Exception(\"Stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all community summaries\n",
    "\n",
    "clientMilvus = MilvusClient(\n",
    "    uri=\"./vector_db_graphRAG/milvus_ingest.db\",\n",
    ")\n",
    "\n",
    "community_summaries_retrieved = clientMilvus.query(\n",
    "    collection_name=\"community_summaries\",\n",
    "    output_fields=[\"community_id\", \"text\"],\n",
    "    limit = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor for generation\n",
    "\n",
    "community_summaries_ingestion = {}\n",
    "for el in community_summaries_retrieved:\n",
    "    community_summaries_ingestion[el['community_id']]=el['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Ask a question (Step 2.6 simplified vs. full approach)\n",
    "user_query = 'What does the graph indicate about the relationship between \\\\( \\\\langle r(t) \\\\rangle \\\\) and time (t) during steady-state conditions? Choose between the following options: ' + options \n",
    "\n",
    "# Simple QA (RAG):\n",
    "global_answer, partial_answers, marks  = answer_query_from_communities(user_query, community_summaries_ingestion, USE_OPENAI=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_community_summaries = init_milvus_db(\"community_summaries\", \"./vector_db_graphRAG/milvus_ingest.db\", embed_fn)\n",
    "embed=embed_fn.embed_query(user_query)\n",
    "search=search_milvus(embed, vectorstore_community_summaries, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 1.274196982383728\n",
      "35 1.2917883396148682\n",
      "37 1.3111110925674438\n",
      "31 1.3140339851379395\n"
     ]
    }
   ],
   "source": [
    "for s in search:\n",
    "    print(s[0].metadata['community_id'],s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0   26  0\n",
       "35  11  0\n",
       "26  21  0\n",
       "27  14  0\n",
       "28  19  0\n",
       "29  35  0\n",
       "30  23  0\n",
       "31  45  0\n",
       "32  12  0\n",
       "33   5  0\n",
       "34   9  0\n",
       "36  15  0\n",
       "24  27  0\n",
       "37  34  0\n",
       "38  30  0\n",
       "39  13  0\n",
       "40  10  0\n",
       "41   4  0\n",
       "42   7  0\n",
       "43  16  0\n",
       "44   8  0\n",
       "45  46  0\n",
       "25  20  0\n",
       "23   6  0\n",
       "1   42  0\n",
       "11  50  0\n",
       "2   44  0\n",
       "3   28  0\n",
       "4   24  0\n",
       "5   36  0\n",
       "6    1  0\n",
       "7   32  0\n",
       "8   33  0\n",
       "9   47  0\n",
       "10   3  0\n",
       "12  18  0\n",
       "22  38  0\n",
       "13  49  0\n",
       "14  31  0\n",
       "15  37  0\n",
       "16  29  0\n",
       "17   2  0\n",
       "18   0  0\n",
       "19  25  0\n",
       "20  40  0\n",
       "21  40  0\n",
       "46  17  0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(marks).sort_values(by=1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open parquet file \n",
    "\n",
    "df= pd.read_parquet('data/test-00000-of-00001.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>image</th>\n",
       "      <th>image_filename</th>\n",
       "      <th>options</th>\n",
       "      <th>answer</th>\n",
       "      <th>page</th>\n",
       "      <th>model</th>\n",
       "      <th>prompt</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>What does the graph indicate about the relatio...</td>\n",
       "      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n",
       "      <td>images/0707.1659_3.jpg</td>\n",
       "      <td>['A. \\\\( \\\\langle r(t) \\\\rangle \\\\) decreases ...</td>\n",
       "      <td>B</td>\n",
       "      <td></td>\n",
       "      <td>gpt4V</td>\n",
       "      <td></td>\n",
       "      <td>arxiv_qa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "68  What does the graph indicate about the relatio...   \n",
       "\n",
       "                                                image          image_filename  \\\n",
       "68  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  images/0707.1659_3.jpg   \n",
       "\n",
       "                                              options answer page  model  \\\n",
       "68  ['A. \\\\( \\\\langle r(t) \\\\rangle \\\\) decreases ...      B       gpt4V   \n",
       "\n",
       "   prompt    source  \n",
       "68         arxiv_qa  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['image_filename']=='images/0707.1659_3.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "options=df[df['image_filename']=='images/0707.1659_3.jpg']['options'].values\n",
    "options = ', '.join(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['A. \\\\\\\\( \\\\\\\\langle r(t) \\\\\\\\rangle \\\\\\\\) decreases as time increases.', 'B. \\\\\\\\( \\\\\\\\langle r(t) \\\\\\\\rangle \\\\\\\\) increases as time increases.', 'C. \\\\\\\\( \\\\\\\\langle r(t) \\\\\\\\rangle \\\\\\\\) remains constant as time increases.', 'D. The relationship between \\\\\\\\( \\\\\\\\langle r(t) \\\\\\\\rangle \\\\\\\\) and time cannot be determined from the graph.']\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"End of the script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's question: What does the graph indicate about the relationship between \\( \\langle r(t) \\rangle \\) and time (t) during steady-state conditions? Choose between the following options: ['A. \\\\( \\\\langle r(t) \\\\rangle \\\\) decreases as time increases.', 'B. \\\\( \\\\langle r(t) \\\\rangle \\\\) increases as time increases.', 'C. \\\\( \\\\langle r(t) \\\\rangle \\\\) remains constant as time increases.', 'D. The relationship between \\\\( \\\\langle r(t) \\\\rangle \\\\) and time cannot be determined from the graph.']\n",
      "Answer: To determine the relationship between \\( \\langle r(t) \\rangle \\) and time (t) during steady-state conditions, we need to analyze the graph provided. The options available are:\n",
      "\n",
      "A. \\( \\langle r(t) \\rangle \\) decreases as time increases.  \n",
      "B. \\( \\langle r(t) \\rangle \\) increases as time increases.  \n",
      "C. \\( \\langle r(t) \\rangle \\) remains constant as time increases.  \n",
      "D. The relationship between \\( \\langle r(t) \\rangle \\) and time cannot be determined from the graph.\n",
      "\n",
      "If the graph shows a trend where \\( \\langle r(t) \\rangle \\) consistently rises over time, then option B would be correct. Conversely, if the graph indicates a decline in \\( \\langle r(t) \\rangle \\) as time progresses, option A would be the appropriate choice. If the graph illustrates a flat line, indicating no change in \\( \\langle r(t) \\rangle \\) over time, then option C would apply. Lastly, if the graph does not provide a clear trend or is ambiguous, option D would be the most suitable.\n",
      "\n",
      "In summary, the correct interpretation of the relationship between \\( \\langle r(t) \\rangle \\) and time during steady-state conditions depends on the specific trends observed in the graph. Please refer to the graph to select the most accurate option based on the observed data.\n"
     ]
    }
   ],
   "source": [
    "# the question asked is the one from the database of vidore at document 0707.1659\n",
    "# The answer is wrong\n",
    "\n",
    "# the marks of the retrieving are all 0 \n",
    "\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {global_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's question: What can be inferred about the frequency of state transitions between n-1, n, and n+1? Choose between the following options: ['A) Transitions occur more frequently from n to n-1.', 'B) Transitions occur more frequently from n to n+1.', 'C) Transitions from n-1 to n and n to n+1 occur with equal frequency.', 'D) The figure does not provide information about the frequency of transitions.']\n",
      "Answer: Based on the provided partial answers, it is clear that they all indicate a lack of specific information regarding the frequency of transitions between the states n-1, n, and n+1. While they discuss probabilistic patterns related to these transitions, none of the answers provide concrete details about how often transitions occur in either direction (from n-1 to n, from n to n+1, or vice versa). Therefore, the most accurate inference we can draw is that the figure does not provide information about the frequency of transitions.\n",
      "\n",
      "Final Answer: D) The figure does not provide information about the frequency of transitions.\n"
     ]
    }
   ],
   "source": [
    "# the question asked is the one from the database of vidore at document 0704.2547\n",
    "# The answer is correct\n",
    "\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {global_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's question: What can be inferred about the frequency of state transitions between n-1, n, and n+1?\n",
      "Answer: The text indicates that the frequency of state transitions between n-1, n, and n+1 can be understood through two primary probabilistic patterns: one for moving forward (from n to n+1) and another for moving backward (from n to n-1). These patterns suggest that the transitions are not random but rather governed by specific probabilistic behaviors, which may vary based on the direction of movement. \n",
      "\n",
      "Additionally, the development of probability functions is mentioned, which assign likelihoods to base values within a sequence. This implies that the frequency of transitions can be analyzed by considering how previous states influence current transitions. While the text does not provide explicit frequencies, it suggests that understanding these probabilistic patterns and functions can offer insights into the dynamics of state transitions and their varying frequencies. \n",
      "\n",
      "In summary, the frequency of state transitions between n-1, n, and n+1 is influenced by probabilistic patterns of movement and the historical context provided by previous states, although specific transition frequencies are not detailed.\n"
     ]
    }
   ],
   "source": [
    "# the question asked is the one from the database of vidore at document 0704.2547\n",
    "#2 documents refined prompt avoiding all 0 scores\n",
    "\n",
    "# the top3 retrieving by embedding and by chatbot is the same\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {global_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's question: What are the main topics of the documents?\n",
      "Answer: The main topics of the documents encompass a range of themes primarily focused on the intersection of statistical modeling, biological systems, and genetic analysis. Key areas of discussion include:\n",
      "\n",
      "1. **Statistical and Computational Methods**: The application of Bayesian inference and Monte Carlo simulations is highlighted, particularly in the context of studying free energy functions and understanding biological processes.\n",
      "\n",
      "2. **DNA and RNA Dynamics**: Several documents explore the role of DNA and RNA in genetic information transfer, gene expression, and the impact of thermal fluctuations on DNA stability. The interaction between RNA and unzipped DNA strands is also a significant focus.\n",
      "\n",
      "3. **Genetic Sequence Analysis**: Theoretical and mathematical frameworks are employed to analyze genetic sequences, including decay constants, error models, and prediction metrics related to mutations. This includes the interconnectedness of these concepts in understanding genetic behavior.\n",
      "\n",
      "4. **Mechanical and Theoretical Aspects of DNA Unzipping**: Theoretical explorations and experimental studies on DNA unzipping are discussed, including models and collaborative research efforts that shed light on the mechanical aspects of this process.\n",
      "\n",
      "5. **Signal Processing in DNA Analysis**: Methodologies for analyzing DNA fluctuations, including the use of a DNA Fluctuation Matrix and the Viterbi Algorithm, are examined to enhance the understanding of unzipping signals.\n",
      "\n",
      "6. **Particle Behavior and System Dynamics**: Theoretical explorations of particle behavior, including the KLS model and its implications for static and dynamic properties, are also addressed, focusing on key parameters influencing system dynamics.\n",
      "\n",
      "7. **Errors in Genetic Processes**: The documents discuss the implications of errors in DNA and RNA processes, the role of genetic bases, and how these errors affect prediction metrics in genetic analysis.\n",
      "\n",
      "Overall, the synthesis of these topics illustrates a comprehensive view of the methodologies and theoretical frameworks used to understand complex biological systems and genetic interactions.\n"
     ]
    }
   ],
   "source": [
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {global_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's question: What can be inferred about the frequency of state transitions between n-1, n, and n+1?\n",
      "Answer: The frequency of state transitions between n-1, n, and n+1 can be inferred from the identification of two primary probabilistic patterns: one for moving forward (from n to n+1) and another for moving backward (from n to n-1). These patterns suggest that the likelihood of transitioning between these states is influenced by the probabilities associated with each movement direction.\n",
      "\n",
      "Specifically, the contrasting probabilities of these patterns indicate that the frequency of transitions is not uniform; rather, it may be affected by the likelihood of remaining in a given state versus moving to adjacent states. Additionally, the development of probability functions that assign likelihoods to base values within a sequence implies that historical data and prior values play a role in shaping these transition frequencies.\n",
      "\n",
      "While the exact frequencies of these transitions are not detailed in the text, the framework established by these patterns allows for an analysis of the dynamics governing state transitions, suggesting that both historical context and probabilistic tendencies are key factors in understanding how often transitions occur between states n-1, n, and n+1.\n"
     ]
    }
   ],
   "source": [
    "# the question asked is the one from the database of vidore at document 0704.2547\n",
    "# 2 documents\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {global_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's question: What can be inferred about the frequency of state transitions between n-1, n, and n+1?\n",
      "Answer: The frequency of state transitions between n-1, n, and n+1 can be influenced by several factors, primarily related to the concepts of entropy and the patterns of movement between states. \n",
      "\n",
      "Firstly, the relationship between entropy and sequences suggests that varying levels of entropy can affect the probabilities of state transitions. Higher entropy may indicate a greater likelihood of transitioning between states, while lower entropy could imply a tendency to remain in a particular state. This understanding is crucial for calculating the probabilities of moving from n-1 to n, n to n+1, and vice versa.\n",
      "\n",
      "Additionally, two primary patterns of state transitions have been identified: Pattern A, which involves moving forward (from n-1 to n and n to n+1), and Pattern B, which involves moving backward (from n+1 to n and n to n-1). The contrasting probabilities of these patterns indicate that the frequency of transitions is not uniform; rather, it is influenced by the likelihood of staying in a state compared to moving to adjacent states.\n",
      "\n",
      "Moreover, the development of probability functions that optimize the assignment of probabilities to base values suggests that prior values in a sequence can also impact the frequency of state transitions. This means that the historical context of the states can play a role in determining how often transitions occur between n-1, n, and n+1.\n",
      "\n",
      "In summary, the frequency of state transitions between n-1, n, and n+1 is influenced by entropy levels, the patterns of movement between states, and the historical context of prior values in the sequence. Understanding these factors can provide insights into the dynamics of state transitions.\n"
     ]
    }
   ],
   "source": [
    "# the question asked is the one from the database of vidore at document 0704.2547\n",
    "# Only right document\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {global_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 min generation\n",
    "< 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's question: What are the main contributions of the paper?\n",
      "Answer: The paper makes several significant contributions to the understanding of biological processes, particularly in the context of DNA dynamics and probabilistic modeling. \n",
      "\n",
      "Firstly, it enhances the accuracy of predictions by developing and applying probability functions that optimize how probabilities are assigned to base values, emphasizing the influence of prior data on current assessments. This is complemented by the introduction of a Binding Energy Matrix, which illustrates the recursive processes affecting binding energies and serves as a foundational tool for analyzing their cumulative distribution within biological systems.\n",
      "\n",
      "The paper also delves into the mechanics of DNA unzipping, particularly in relation to the λ-phage sequence. It focuses on the escape probability and average prediction error, exploring their interrelationship in probabilistic modeling and error analysis. This analysis is further enriched by integrating theoretical models with experimental research, notably through the work of Bockelmann and Heslot, which has validated these theories experimentally.\n",
      "\n",
      "Additionally, the research highlights the significance of DNA fluctuations through methodologies such as the DNA Fluctuation Matrix and the Viterbi Algorithm, which are essential for processing signals and predicting outcomes. The interplay between signals, probabilities, and sequences is examined, particularly how specific signals influence the probabilities defined in predictive models.\n",
      "\n",
      "The paper also addresses the understanding of decay constants and error estimation in statistical models, emphasizing the critical role of parameters like µ, M, τ, and the decay constant Rc in the estimation process. Furthermore, it explores the relationship between entropy regions and sequence probabilities, indicating that a comprehensive understanding of entropy is crucial for calculating these probabilities.\n",
      "\n",
      "Lastly, the application of Bayesian inference and Monte Carlo simulations is discussed, particularly in modeling free energy functions related to biological entities, which enhances the understanding of molecular sequences and their predictive accuracy.\n",
      "\n",
      "In summary, the paper contributes to a multifaceted understanding of DNA dynamics, probabilistic modeling, and the interplay of various biological factors, providing valuable insights for future research in genetics and molecular biology.\n"
     ]
    }
   ],
   "source": [
    "# only paper 0704.2547\n",
    "print(f\"User's question: {user_query}\")\n",
    "print(f\"Answer: {global_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
